Summary,Issue key,Issue id,Parent id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Fix Version/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Description,Environment,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Work Ratio,Security Level,Outward issue link (Blocks),Outward issue link (Problem/Incident),Outward issue link (Relates),Outward issue link (Relates),Outward issue link (Relates),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Business Value),Custom field (Change completion date),Custom field (Change reason),Custom field (Change risk),Custom field (Change start date),Custom field (Change type),Custom field (Customer Request Type),Custom field (Development),Custom field (Epic Color),Custom field (Epic Link),Custom field (Epic Name),Custom field (Epic Status),Custom field (Final Effort Assessment),Custom field (Fugue CLI Version),Custom field (Impact),Custom field (Luminal Team),Custom field (Organizations),Custom field (Parent Link),Custom field (Raised During),Custom field (Rank),Custom field (Request participants),Satisfaction rating,Sprint,Sprint,Sprint,Sprint,Sprint,Custom field (Story Points),Custom field (Team),Custom field (Test Sessions),Custom field (Testing Status),Custom field ([CHART] Date of First Response),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
pull support-tool into Fugue CLI,FUGUE-3386,23840,,Story,In Progress,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,scott,alex,alex,06/Oct/16 11:48 AM,09/May/17 10:47 AM,09/May/17 6:48 PM,,,,,,0,,,,,"There's a lot of overlap between the support tool and the CLI. Additionally, its not a great experience if you're using the cli and run into an issue and have to go get the support tool. We should just make the support commands be part of the CLI.",,alex,rob,scott,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,FUGUE-3040,,,,,,CloudOS Team,,,,2|hzytuu:000000r0000000011zzzzzxi,,,Sprint 29,,,,,5.0,,,Not started,2017-04-03 11:57:37.957,"03/Apr/17 11:57 AM;rob;Documentation work will need to get done, but hopefully not much","05/May/17 9:40 AM;scott;Have to work on higher priority things, so leaving the work here.

All code has been moved over, directory structure flattened,  reorg and consolidation in a few places.

https://github.com/LuminalHQ/fugue-cli-python/compare/feature/FUGUE-3386/support-tools

Remaining things:
- Unit tests work, but coverage is light.
- Two unit tests for the {{report}} command need to be fixed; they are marked 'skip' for now. 
- The {{debug}} command might be fixed but don't have time to test further.
-- Has problems starting Vars on my LT; could be an environmental issue
- The {{reset-secret}} cmd hasn't been tested",,,,,,,,,,,,,,
User has a tool to automatically generate Compositions,FUGUE-3497,24053,,Story,Done,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,robertds,stephen,stephen,14/Oct/16 2:28 PM,01/May/17 9:31 AM,09/May/17 6:48 PM,,,r27,,,0,EndOf28,Sprint26Spillover,Sprint27,Sprint27Spillover,"As a user, I have a tool that returns one Ludwig file that accurately represents the resources currently running in my account.

Acceptance Criteria

1. The tool shall be able to omit resources that are being managed by a Fugue conductor .
2. All resources that can be created and managed by Fugue shall be returned in the auto-generated composition.

",,rob,stephen,timw,,,,,,,,,PDOC-517,,,,,,,,,,,,,,,,,,,{},,FUGUE-3486,,,0.0,,,LRT Team,,,,2|i003kn:3,,,Sprint 25,Sprint 26,Sprint 27,Sprint 28,Sprint 29,,,,Not started,2016-12-23 12:20:06.668,23/Dec/16 12:20 PM;timw;@nate and team to cut subtasks for each AWS service. ,14/Apr/17 5:31 PM;rob;Moved back to ready for QA since Erez is still working on Fugue-4203,,,,,,,,,,,,,,
generate IAM policy for admin and user policies at install-time,FUGUE-3634,24633,,Story,In Progress,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,wayne,alex,alex,25/Oct/16 3:46 PM,03/May/17 3:31 PM,09/May/17 6:48 PM,,2017.01.13,,,,0,Sprint28,,,,"As a user of Fugue I have minimum IAM policies for admin and non admin users so that I don't have to determine what the minimum IAM policies that I will need to use with the RBAC feature.

When fugue is installed, we generate an IAM policy that has ""admin"" access, and one that has ""user"" access and register them with IAM as managed policies.

Acceptance criteria:

* The IAM policy generated for non-admin RBAC users should not allow them elevate their IAM credentials to circumvent the RBAC feature. 
** non-admin RBAC users are users that can't install, uninstall, or upgrade the conductor

We can probably manage these policies in the CFn policy that we use for install/upgrade so that we get ""upgrade"" support for free, uninstall, etc.",,alex,dom,eric,stephen,,,,,,,,PDOC-522,,,,,,,,,,,,,,,,,,,{},,,,,,,,CloudOS Team,,,,2|hzytuu:000000r0000000011zzzzzx9,,,Sprint 23,Sprint 24,Sprint 25,Sprint 26,Sprint 29,5.0,,,Not started,2016-10-25 16:16:57.976,25/Oct/16 4:16 PM;dom;Relevant: https://github.com/LuminalHQ/fugue-user-iam-templates,"15/Dec/16 12:11 PM;eric;Im not sure if it possible to have an ""admin"" role that can install, upgrade, and delete the conductor while not being able to circumvent RBAC at the moment. The examples provided above look like exactly what we need to define and administrator, but it includes the permission to iam:CreateRole. With that could a user create a role with any level of permissions and then allow themselves to assume that role? If so, then the user could give themselves the ability to remove s3 bucket policies which would open up rbac users and policies for editing.","16/Dec/16 10:41 AM;dom;Updated link: https://github.com/fugue/fugue-user-iam-templates

*FYI:* Since 25 Oct, we have manually tested these and published them (customers were asking for them). They're extremely hard to produce and test. I hope you make something better, but it's a lot of log parsing I'll tell you.

- If these meet your needs, please adopt, maintain & test with each release.
- If these don't meet your needs, please let us know ASAP so we can update with whatever you come up with.","20/Feb/17 1:05 PM;eric;My preferred implementation would be to add these policies to the conductors cloudformation template. This should give us a good path for install, updates, and uninstalls. Although I would need to take some time to test out how iam policies work in cloudformation and make sure there is a happy path for making updates during conductor upgrades.

If using cloudformation for managing these iam policies ends up being impractical we'll have to build in managing them directly in to the cli by making iam api calls directly..","03/Apr/17 10:59 AM;dom;[~eric] I agree with making them part of ""batteries included."" That said I would ask folks to consider keeping the policy JSON itself in a public repo so that they can still be inspected and accessed easily.",,,,,,,,,,,
[QA] Validate that the stipulated IAM roles are created upon executing an install,FUGUE-3866,25891,24633,QA-Validation,To Do,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,erez,erez,erez,21/Nov/16 2:28 PM,18/Jan/17 11:44 AM,09/May/17 6:48 PM,,,,,,0,,,,,Try being each one of these roles and make sure the corresponding fugue commands can/can't be executed.,,erez,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,QA Team,,,,2|hzyuw7:,,,Sprint 23,Sprint 24,Sprint 25,Sprint 26,Sprint 29,,,,Not started,,,,,,,,,,,,,,,,,
BUGFIX: update_assume_role_policy instructions are issued every tick,FUGUE-4196,27629,,Task,PR Merged,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,tyler,chrisg,chrisg,23/Dec/16 1:44 PM,05/May/17 12:06 PM,09/May/17 6:48 PM,,,,,,0,,,,,"Conductor: ami-1921360e

Running the following composition will result in an {{update_assume_role_policy}} instruction being issued every tick:

{code}
composition

import Fugue.AWS.IAM as IAM

lambda-role: IAM.Role {
  roleName: 'bug-update-every-tick',
  assumeRolePolicyDocument: IAM.Policy.AssumeRole.lambda,
}
{code}

This appears to be due to IAM returning the assume role policy in a slightly different format than it was created with (it transforms {{Statement}} to a list, and adds an empty {{Sid}}), so when the planner compares the policy in the composition with the policy returned by AWS it thinks they're different and issues the instruction to update it.

This isn't a problem for accounts with few roles, but it would be possible to run into API throttling issues if you had a large number of roles managed by fugue.

Our other policy implementations may be susceptible to a similar problem.",,chrisg,tyler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,LRT Team,,,,2|i003kq:6hv9,,,Sprint 29,,,,,5.0,,,Not started,2017-05-03 11:15:45.748,"27/Feb/17 12:24 PM;chrisg;As an example of this, consider the following composition:

{code}
composition

import Fugue.AWS.IAM as IAM

lambda-role: IAM.Role {
  roleName: 'bug-update-every-tick',
  assumeRolePolicyDocument: IAM.Policy.AssumeRole.lambda,
}
{code}

Here is the assume role policy document defined by {{IAM.Policy.AssumeRole.lambda}}:

{code}
{
  ""Version"": ""2012-10-17"",
  ""Statement"": {
    ""Effect"": ""Allow"",
    ""Principal"": {
      ""Service"": [""lambda.amazonaws.com""]
    },
    ""Action"": ""sts:AssumeRole""
  }
}
{code}

and here is what's returned from the describe-role API call (as executed via {{aws iam get-role}}):

{noformat}
$ aws iam get-role --role-name bug-update-every-tick
{
    ""Role"": {
        ""AssumeRolePolicyDocument"": {
            ""Version"": ""2012-10-17"",
            ""Statement"": [
                {
                    ""Action"": ""sts:AssumeRole"",
                    ""Effect"": ""Allow"",
                    ""Principal"": {
                        ""Service"": ""lambda.amazonaws.com""
                    }
                }
            ]
        },
        ""RoleId"": ""AROAJZD3MON5AJD2Y5CDW"",
        ""CreateDate"": ""2017-02-27T16:34:11Z"",
        ""RoleName"": ""bug-update-every-tick"",
        ""Path"": ""/"",
        ""Arn"": ""arn:aws:iam::235986320834:role/bug-update-every-tick""
    }
}
{noformat}

We can make things a bit easier on ourselves by converting the policies provided in {{Fugue.AWS.IAM.Policy.AssumRole}} to be in the format returned by the describe calls, but to fix this for all cases we'll need to canonicallize the IAM coming out of the Ludwig before comparing it with the results from IAM.

This can be done with a custom {{UnmarshallJSON}} implementation that parses policies in both formats into a common type. This will most likely look something like:

{code}
type Policy struct {
        Version string
        Id string
        Statement []Statement{}
}

type SingleStatementPolicy struct {
        Version string
        Id string
        Statement Statement
}

type Statement struct {
  ...
}

func (p *Policy) UnmarshallJSON(b []byte) error {
        var singleStatementPolicy SingleStatementPolicy
        err := json.UnmarshallJSON(b, &singleStatementPolicy)
        if err == nil {
                p.Version = singleStatementPolicy.Version
                p.Statement = []Statement{singleStatementPolicy.Statement}
                return nil
        }

        var policy Policy
        err := json.UnmarshallJSON(b, p)
        if err == nil {
                return nil
        }
        return err        
}
{code}

Then we unmarshall policies in either format and compare them correctly. There's a bit of trickiness around {{Statement}} parsing, but I think that can be sorted out using the [docs|docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements.html].


Given some of the variants in IAM policy (especially around principals), I'm going to estimate this as a 5. Which should ensure that there is plenty of time for testing the parser and comparison code.
","16/Mar/17 6:32 PM;chrisg;[~danthemyth] reported running into this with {{aws.autoscaling.update_auto_scaling_group}} operations as well:

{quote}
dan [17:40] 
small bug that I’ll put in a ticket for tomorrow morning re aws.autoscaling.update_auto_scaling_group where planner is issuing repeated instructions due to the policy comparision evaluating to false
Planner Instruction:
`{""Statement"": {""Principal"": {""Service"": [""ec2.amazonaws.com""] }, ""Action"": ""sts:AssumeRole"", ""Effect"": ""Allow""}, ""Version"": ""2012-10-17""}`
Reflector state:
`{""Statement"": [{""Principal"": {""Service"": ""ec2.amazonaws.com""}, ""Action"": ""sts:AssumeRole"", ""Effect"": ""Allow""}], ""Version"": ""2012-10-17""}`
{quote}",06/Apr/17 4:10 PM;chrisg;I think the existing estimate of a 5 is still valid here for Sprint 28.,03/May/17 11:15 AM;tyler;https://github.com/LuminalHQ/emitInstructions-go/pull/503,,,,,,,,,,,,
[QA] Validate ludwig auto-gen tool,FUGUE-4203,27636,24053,QA-Validation,Will Not Fix,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,erez,erez,erez,23/Dec/16 2:21 PM,20/Apr/17 9:43 AM,09/May/17 6:48 PM,,,,,,0,,,,,"0) Test that the generated ludwig represents all current assets in the account
1) Run fugue assets and validate that fugue assets are omitted (presumably includes fugue resources related to the operation of the conductor)",,erez,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,QA Team,,,,2|hzzwlj:,,,Sprint 25,Sprint 26,Sprint 27,Sprint 28,Sprint 29,,,,Not started,,,,,,,,,,,,,,,,,
Separate Fugue credentials file.,FUGUE-4390,28402,,Story,PR Merged,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,matt,alex,alex,12/Jan/17 10:29 AM,08/May/17 4:24 PM,09/May/17 6:48 PM,,,,,,0,sprint29,,,,"As a user of Fugue, I have a specific Fugue credentials file separate from my fugue.yaml file so that I can easily share my fugue.yaml, (check into git etc) without worrying about passing around my sensitive fugue credentials. 
",,alex,eric,,,,,,,,,,PDOC-519,,,,,,,,,,,,,,,,,,,{},,,,,,,,CloudOS Team,,,,2|hzytuu:000000r0000000011zzzzzx,,,Sprint 29,,,,,3.0,,,Not started,2017-02-02 12:23:40.918,"02/Feb/17 12:23 PM;eric;Id recommend sticking with aws' strategy by making a credentials file in an app data directory ie: ~/.fugue %APPDATA%\fugue

I think keeping secrets unencrypted in the credential file is ok to start with. Again, following aws' lead here is a safe bet, but in the future we may want to consider adding support for either os specific password/keystores or creating our own to securely encrypt secrets to a users disk.

Estimate: 3",,,,,,,,,,,,,,,
Profiles for fugue credentials,FUGUE-4391,28404,,Story,PR Merged,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,matt,alex,alex,12/Jan/17 10:31 AM,08/May/17 4:24 PM,09/May/17 6:48 PM,,,,,,1,sprint29,,,,"As a user, I might have multiple fugue users that I want to switch between. Rather than comment out creds, or have multiple files, we should have a profile-like system where you can pick which profile to use on a per-command basis (with a default)",,alex,eric,nate,,,,,,,,,PDOC-520,,,,,,,,,,,,,,,,,,,{},,,,,,,,CloudOS Team,,,,2|hzytuu:000000r0000000011zzzzzt,,,Sprint 29,,,,,3.0,,,Not started,2017-01-17 14:32:03.961,"17/Jan/17 2:32 PM;nate;Proposed name: ""fudo""","02/Feb/17 12:20 PM;eric;here's where we ask the question. Is it time to create an app data folder for fugue? ie: ~/.fugue %APPDATA%\fugue 

Being that we have a the case FUGUE-4390 my guess is that the answer is yes. I'd recommend creating an ini file pretty much identical to how the aws creds file works and have sections identified by ""profile"" name. Updating the cli to read from a creds file and selecting a specific profile from that file is a trivial task being that we already keep creds in a different file.

Estimate: 3",,,,,,,,,,,,,,
cli can look up valid AMI IDs,FUGUE-4490,28723,,Story,Done,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,matt,alex,alex,19/Jan/17 10:11 AM,01/May/17 9:47 AM,09/May/17 6:48 PM,,,,,,1,EndOf28,,,,"Right now, the cli cant tell the difference between AMIs that are not whitelisted and AMIs that are invalid. We should have an AMI registry where the CLI can look up valid AMI IDs.",,achintya,alex,erez,evan,matt,rob,wayne,,,,,PDOC-526,,,,,,,,,,,,,,,,,,,{},,FUGUE-5036,,,5.0,,,CloudOS Team,,,,2|hzytuu:000000r000000000vm,,,Sprint 28,Sprint 29,,,,3.0,,,Not started,2017-02-02 13:03:40.189,02/Feb/17 1:03 PM;achintya;Blocked on https://luminal.atlassian.net/browse/WEB-843 ,"20/Feb/17 12:42 PM;erez;should this feature also show which CLI version would be compatible for each conductor?
",24/Feb/17 4:05 PM;wayne;Please follow up with the web team and make a comment wrt status on this ticket.,09/Mar/17 10:18 AM;achintya;This ticket can be marked as TODO since https://luminal.atlassian.net/browse/WEB-843 is done,31/Mar/17 10:37 AM;achintya;We need this list to contain the list of already valid AMIs too. So we'd have to add to this list all the AMIs which we released before this list comes into force.,03/Apr/17 11:16 AM;rob;Need to add an environmental variable for our internal needs to get around this. (or maybe just use what's in FUGUE YAML? That's probably better),01/May/17 9:47 AM;matt;fugue-client: 0.32.1,,,,,,,,,
Have the CLI read and parse the conductor AMI list from S3,FUGUE-4492,28725,28723,Sub-task,Done,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,matt,alex,alex,19/Jan/17 10:14 AM,01/May/17 9:49 AM,09/May/17 6:48 PM,,,,,,0,,,,,,,alex,jonathan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,2|i0003b:,,,Sprint 28,Sprint 29,,,,,,,Not started,2017-01-19 10:26:36.032,19/Jan/17 10:26 AM;jonathan;Just wanted to point out that there is probably already an API that you could consider using as well that's part of the download services and if not maybe something to consider if using a file in s3 makes tracking users more difficult or something. ,,,,,,,,,,,,,,,
Hide the AMI checking behind a env variable,FUGUE-4598,29520,28723,Sub-task,Done,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,matt,achintya,achintya,02/Feb/17 11:53 AM,21/Apr/17 9:13 AM,09/May/17 6:48 PM,,,,,,0,,,,,"We'd need a ""cli_preview"" like flag which would hide this behavior if an env variable is set. This is to make sure that the CLI can be used for those AMIs which have not been annotated through the artifact tool (for example all the non cbfr AMIs) ",,achintya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,CloudOS Team,,,,2|i001in:,,,Sprint 28,Sprint 29,,,,,,,Not started,,,,,,,,,,,,,,,,,
[QA] Validate mechanism to cleanly switch between users (with a default),FUGUE-4757,30530,28404,QA-Validation,Will Not Fix,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,,erez,erez,20/Feb/17 11:56 AM,20/Apr/17 11:16 AM,09/May/17 6:48 PM,,,,,,0,,,,,,,erez,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,QA Team,,,,2|i003bj:,,,Sprint 29,,,,,,,,Not started,,,,,,,,,,,,,,,,,
[QA] Validate separation of fugue.yaml from credential file,FUGUE-4758,30531,28402,QA-Validation,To Do,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,,erez,erez,20/Feb/17 11:58 AM,20/Feb/17 11:58 AM,09/May/17 6:48 PM,,,,,,0,,,,,should probably be tested in conjunction with https://luminal.atlassian.net/browse/FUGUE-4757,,erez,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,CloudOS Team,,,,2|i003br:,,,Sprint 29,,,,,,,,Not started,,,,,,,,,,,,,,,,,
Bugfix: CLI --creds-check option is out of date,FUGUE-4777,30564,,Task,In Progress,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,wayne,wayne,wayne,20/Feb/17 2:59 PM,05/May/17 7:36 AM,09/May/17 6:48 PM,,,,CLI,,0,EndOf28,Sprint28,,,`fugue init --creds-check` has not been updated and will return false positives,,matt,wayne,,,,,,0%,,,,FUGUE-3634,QA-335,,20/Feb/17 2:59 PM;wayne;fuguecli (9).log;https://luminal.atlassian.net/secure/attachment/25713/fuguecli+%289%29.log,,,,,,,,,,,,,,,,{},,,,,2.0,,,CloudOS Team,,,,2|hzytuu:000000r000000000b,,,Sprint 28,Sprint 29,,,,2.0,,,Not started,2017-05-01 09:48:45.746,"28/Feb/17 1:31 PM;wayne;Some unnecessary permissions are checked. For example, DeleteMessageBatch is no longer needed.

{code:java}
± fugue init --profile fugue-installer --creds-check ami-482ae35e
[ fugue init ] Initializing Fugue project with the following configuration:

Fugue Conductor AMI ID: ami-482ae35e
AWS Credentials: Profile (fugue-installer)

Checking your AWS Credentials for Fugue CLI use ...
Not Authorized
Your account is missing the following AWS permissions:
sqs:DeleteMessageBatch
s3:ListAllMyBuckets
s3:GetBucketLocation

Validating Fugue Conductor AMI ID ...
[ OK ] Provided AMI ID is valid.

Creating new fugue.yaml file ...

[ Done ] Project initialized.
{code}
",01/May/17 9:48 AM;matt;fugue-client: 0.32.1,"03/May/17 12:28 PM;wayne;We can incorporate 4777 into 3634. 

Whoever picks this task up should also pick up 3634 to get them both knocked out.

Rather than put all the missing actions into init.py, I think we can address this better when we complete FUGUE-3634. We'll need those up-to-date definitions and we can grab all the actions from what we require to ensure the running user has the right permissions.",03/May/17 12:30 PM;wayne;Moving back into TODO,"05/May/17 7:36 AM;wayne;Working alongside FUGUE-3634
",,,,,,,,,,,
Backend work for changing enforcement level of the conductor at the process level,FUGUE-4786,30611,,Task,In Progress,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,mike,alex,alex,21/Feb/17 11:45 AM,09/May/17 2:23 PM,09/May/17 6:48 PM,,,,,,0,,,,,"As a user, I want to tell the conductor not to do enforcement for a specific -account- process


The will require that the broker is able to tell what kind of job led to the plan being created (subtask of FUGUE-4787)

*EDIT*: this ticket is the LRT backend work. the frontend (CLI/demarc) is in FUGUE-4820",,alex,joshe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,FUGUE-5034,,,,,,LRT Team,,,,2|i003kq:6hvd,,,Sprint 29,,,,,8.0,,,Not started,2017-03-09 15:54:29.985,"09/Mar/17 3:54 PM;joshe;Assuming this is the full end-to-end change, including the CLI work to introduce this command","09/Mar/17 4:00 PM;joshe;If specific to accounts, this will be primarily work in the accounts client/service to store additional metadata about each account (enforcement level). 

The Broker can then use the accounts client (which it already consumes) to determine if it should execute instructions or not.

(If it is enforcement by Process, then the Scheduler will own the enforcement level storage)","07/Apr/17 10:51 AM;joshe;Estimate at 13 due to coordination between teams and components, and need to fully flesh out CLI UX details.",19/Apr/17 3:34 PM;alex;This is not the CLI/demarc stuff. That's in FUGUE-4820. This ticket is for the LRT side. Can you re-estimate with that in mind?,"19/Apr/17 3:40 PM;joshe;New estimate at 8, takes into account long tail testing and integration due to large number of components involved",,,,,,,,,,,
Update Vars Headless Mode to no longer write ddb records for every single version of an item,FUGUE-4811,30728,,Task,To Do,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,,eric,eric,23/Feb/17 10:57 AM,05/May/17 10:33 AM,09/May/17 6:48 PM,,,,,,0,,,,,"The strategy we are currently using for writing a dynamodb record for every version of a vars item breaks down when the set of items gets too large. We run into problems of vars taking too long to warm its cache. Our initial attempts to solve this problem by snapshotting and garbage collecting were successful for a time. But again, once the set of items reached certain sizes >2GB or about >4million ddb records, the full table scans required to create snapshots started to become prohibitively long and memory consuming. Even in cases where snapshotting was succeeding deleting individual records during garbage collection was taking even longer.

With these issues identified we want to create a persistence strategy for dynamodb that better supports its properties while keeping Var's Key/Value model in tact.

Proposals:
1 - create a new ddb table where we run updates on records (pretty easy, will scale for a while)
2 - use ddb as more of a transaction log and build snapshots off of that (more difficult, but could provide room for more horizontal scaling as far as dataset size and request throughput is concerned.)",,eric,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,FUGUE-5038,,,,,,CloudOS Team,,,,2|hzytuu:000000r0000000011zzzzzh,,,Sprint 29,,,,,13.0,,,Not started,,"05/May/17 9:57 AM;eric;do not update daemon/keystore/dynamodb please create a new one. here's the schema

{code}
// CreateTable creates the required table
func (m SingleRecordPerVersion) CreateTableInput(
	tablename string,
	writeCap int64,
) *dynamodb.CreateTableInput {
	readCap := int64(1)

	attributeDefinitions := []*dynamodb.AttributeDefinition{
		&dynamodb.AttributeDefinition{
			AttributeName: aws.String(""key""),
			AttributeType: aws.String(""S""),
		},
		&dynamodb.AttributeDefinition{
			AttributeName: aws.String(""deleted""),
			AttributeType: aws.String(""N""),
		},
		&dynamodb.AttributeDefinition{
			AttributeName: aws.String(""last_update""),
			AttributeType: aws.String(""N""),
		},
	}

	gsiSpec := &dynamodb.GlobalSecondaryIndex{
		IndexName: aws.String(""deleted-last_update-index""),

		KeySchema: []*dynamodb.KeySchemaElement{
			&dynamodb.KeySchemaElement{
				AttributeName: aws.String(""deleted""),
				KeyType:       aws.String(""HASH""),
			},
			&dynamodb.KeySchemaElement{
				AttributeName: aws.String(""last_update""),
				KeyType:       aws.String(""RANGE""),
			},
		},
		Projection: &dynamodb.Projection{
			ProjectionType: aws.String(""KEYS_ONLY""),
		},
		ProvisionedThroughput: &dynamodb.ProvisionedThroughput{
			ReadCapacityUnits:  aws.Int64(readCap),  // Required
			WriteCapacityUnits: aws.Int64(writeCap), // Required
		},
	}

	input := &dynamodb.CreateTableInput{
		TableName: nil,

		KeySchema: []*dynamodb.KeySchemaElement{
			&dynamodb.KeySchemaElement{
				AttributeName: aws.String(""key""),
				KeyType:       aws.String(""HASH""),
			},
		},

		AttributeDefinitions: attributeDefinitions,

		GlobalSecondaryIndexes: []*dynamodb.GlobalSecondaryIndex{
			gsiSpec,
		},

		ProvisionedThroughput: &dynamodb.ProvisionedThroughput{
			ReadCapacityUnits:  aws.Int64(readCap),  // Required
			WriteCapacityUnits: aws.Int64(writeCap), // Required
		},
	}

	input.SetTableName(tablename)
	return input
}
{code}","05/May/17 9:59 AM;eric;instead of a conditional PutItem we will be using a conditional UpdateItem. A gotya will be knowing when to remove columns that were in use in the current version but not in use in the next version the below code is a close shot at that

{code}
func updateItem(
	dynamoDBAPI dynamodbiface.DynamoDBAPI,
	tableName string,
	pkey string,
	skey string,
	currentItem map[string]*dynamodb.AttributeValue,
	nextItem map[string]*dynamodb.AttributeValue,
) {
	key := map[string]*dynamodb.AttributeValue{
		pkey: nextItem[pkey],
	}

	updateExpression := attributeValueToSetExpression(pkey, skey, nextItem)
	expressionAttributeValues := attributeValueToExpressionValue(
		pkey,
		skey,
		nextItem,
	)
	expressionAttributeNames := attributeValueToExpressionNames(nextItem)
	conditionExpression := aws.String(
		""attribute_not_exists(#key)"",
	)

	if currentItem != nil {
		diff := attributeValueDiff(currentItem, nextItem)
		if len(diff) > 0 {
			for i, k := range diff {
				diff[i] = ""#"" + k
				expressionAttributeNames[diff[i]] = aws.String(k)
			}
			updateExpression = aws.String(
				*updateExpression + "" REMOVE "" + strings.Join(diff, "", ""),
			)
		}

		expressionAttributeValues["":currentVersion""] = currentItem[""version""]
		conditionExpression = aws.String(
			""attribute_exists(#key) and #version=:currentVersion"",
		)
	}

	input := &dynamodb.UpdateItemInput{
		TableName: aws.String(tableName),
		Key:       key,

		UpdateExpression:          updateExpression,
		ExpressionAttributeValues: expressionAttributeValues,

		ExpressionAttributeNames: expressionAttributeNames,
		ConditionExpression:      conditionExpression,

		ReturnConsumedCapacity: aws.String(""TOTAL""),
	}

	fmt.Println(input)
	if output, err := dynamoDBAPI.UpdateItem(input); err != nil {
		log.WithError(err).Fatal(""Error Writing Item"")
	} else {
		fmt.Println(""Consumed:"", *output.ConsumedCapacity.CapacityUnits)
	}
}
{code}",,,,,,,,,,,,,,
Update Vars Conductor to no longer write ddb records for every single version of vars item,FUGUE-4812,30729,,Task,In Progress,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,matt,eric,eric,23/Feb/17 10:58 AM,09/May/17 2:25 PM,09/May/17 6:48 PM,,,,,,0,,,,,"The strategy we are currently using for writing a dynamodb record for every version of a vars item breaks down when the set of items gets too large. We run into problems of vars taking too long to warm its cache. Our initial attempts to solve this problem by snapshotting and garbage collecting were successful for a time. But again, once the set of items reached certain sizes >2GB or about >4million ddb records, the full table scans required to create snapshots started to become prohibitively long and memory consuming. Even in cases where snapshotting was succeeding deleting individual records during garbage collection was taking even longer.

With these issues identified we want to create a persistence strategy for dynamodb that better supports its properties while keeping Var's Key/Value model in tact.

Proposals:
1 - create a new ddb table where we run updates on records (pretty easy, will scale for a while)
2 - use ddb as more of a transaction log and build snapshots off of that (more difficult, but could provide room for more horizontal scaling as far as dataset size and request throughput is concerned.)",,eric,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,FUGUE-5038,,,,,,CloudOS Team,,,,2|hzytuu:000000r0000000011zzzzzhi,,,Sprint 29,,,,,5.0,,,Not started,,"05/May/17 9:58 AM;eric;{code}
// CreateTable creates the required table
func (m SingleRecordPerVersion) CreateTableInput(
	tablename string,
	writeCap int64,
) *dynamodb.CreateTableInput {
	readCap := int64(1)

	attributeDefinitions := []*dynamodb.AttributeDefinition{
		&dynamodb.AttributeDefinition{
			AttributeName: aws.String(""key""),
			AttributeType: aws.String(""S""),
		},
		&dynamodb.AttributeDefinition{
			AttributeName: aws.String(""deleted""),
			AttributeType: aws.String(""N""),
		},
		&dynamodb.AttributeDefinition{
			AttributeName: aws.String(""last_update""),
			AttributeType: aws.String(""N""),
		},
	}

	gsiSpec := &dynamodb.GlobalSecondaryIndex{
		IndexName: aws.String(""deleted-last_update-index""),

		KeySchema: []*dynamodb.KeySchemaElement{
			&dynamodb.KeySchemaElement{
				AttributeName: aws.String(""deleted""),
				KeyType:       aws.String(""HASH""),
			},
			&dynamodb.KeySchemaElement{
				AttributeName: aws.String(""last_update""),
				KeyType:       aws.String(""RANGE""),
			},
		},
		Projection: &dynamodb.Projection{
			ProjectionType: aws.String(""KEYS_ONLY""),
		},
		ProvisionedThroughput: &dynamodb.ProvisionedThroughput{
			ReadCapacityUnits:  aws.Int64(readCap),  // Required
			WriteCapacityUnits: aws.Int64(writeCap), // Required
		},
	}

	input := &dynamodb.CreateTableInput{
		TableName: nil,

		KeySchema: []*dynamodb.KeySchemaElement{
			&dynamodb.KeySchemaElement{
				AttributeName: aws.String(""key""),
				KeyType:       aws.String(""HASH""),
			},
		},

		AttributeDefinitions: attributeDefinitions,

		GlobalSecondaryIndexes: []*dynamodb.GlobalSecondaryIndex{
			gsiSpec,
		},

		ProvisionedThroughput: &dynamodb.ProvisionedThroughput{
			ReadCapacityUnits:  aws.Int64(readCap),  // Required
			WriteCapacityUnits: aws.Int64(writeCap), // Required
		},
	}

	input.SetTableName(tablename)
	return input
}
{code}","05/May/17 10:01 AM;eric;instead of a conditional PutItem we will be using a conditional UpdateItem. A gotya will be knowing when to remove columns that were in use in the current version but not in use in the next version the below code is a close shot at that

{code}
func updateItem(
	dynamoDBAPI dynamodbiface.DynamoDBAPI,
	tableName string,
	pkey string,
	skey string,
	currentItem map[string]*dynamodb.AttributeValue,
	nextItem map[string]*dynamodb.AttributeValue,
) {
	key := map[string]*dynamodb.AttributeValue{
		pkey: nextItem[pkey],
	}

	updateExpression := attributeValueToSetExpression(pkey, skey, nextItem)
	expressionAttributeValues := attributeValueToExpressionValue(
		pkey,
		skey,
		nextItem,
	)
	expressionAttributeNames := attributeValueToExpressionNames(nextItem)
	conditionExpression := aws.String(
		""attribute_not_exists(#key)"",
	)

	if currentItem != nil {
		diff := attributeValueDiff(currentItem, nextItem)
		if len(diff) > 0 {
			for i, k := range diff {
				diff[i] = ""#"" + k
				expressionAttributeNames[diff[i]] = aws.String(k)
			}
			updateExpression = aws.String(
				*updateExpression + "" REMOVE "" + strings.Join(diff, "", ""),
			)
		}

		expressionAttributeValues["":currentVersion""] = currentItem[""version""]
		conditionExpression = aws.String(
			""attribute_exists(#key) and #version=:currentVersion"",
		)
	}

	input := &dynamodb.UpdateItemInput{
		TableName: aws.String(tableName),
		Key:       key,

		UpdateExpression:          updateExpression,
		ExpressionAttributeValues: expressionAttributeValues,

		ExpressionAttributeNames: expressionAttributeNames,
		ConditionExpression:      conditionExpression,

		ReturnConsumedCapacity: aws.String(""TOTAL""),
	}

	fmt.Println(input)
	if output, err := dynamoDBAPI.UpdateItem(input); err != nil {
		log.WithError(err).Fatal(""Error Writing Item"")
	} else {
		fmt.Println(""Consumed:"", *output.ConsumedCapacity.CapacityUnits)
	}
}
{code}",,,,,,,,,,,,,,
Implement new component that publishes notifications,FUGUE-4817,30741,,Story,Done,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,scott,alex,alex,23/Feb/17 3:36 PM,03/May/17 4:35 PM,09/May/17 6:48 PM,,,,,,0,EndOf28,,,,"The first and only notification type to be supported will be the Planner instruction stream changes.

Implement a service that:
* listens on the brokers sns inbox topic
* renders the plan into a human-readable form (from protobuf)
* publishes the plan to an SNS topic based on what kind of job created the plan
* -notifications will be a process level attribute, enabled by default. The broker should check this attribute to determine whether to send notifications or not.-

For example, if the plan was the result of a `fugue run`, then publish it to a `run` topic. If it was a system job, then publish it to a `system` topic.

Step 1: pre-baked topics
* fugue-notifications-instruction-stream-run
* fugue-notifications-instruction-stream-update
* fugue-notifications-instruction-stream-resume
* fugue-notifications-instruction-stream-kill
* fugue-notifications-instruction-stream-drift (name?) -- only non-empty System Plans

Step 2: Filters/CRUD operations/etc (TBD in future, not in scope for this ticket)",,alex,joshe,rob,scott,wayne,,,,,,,,,,,,,,,,,,,,,,,,,,{},,FUGUE-5027,,,8.0,,,CloudOS Team,,,,2|hzytuu:000000r00000000ir,,,Sprint 28,Sprint 29,,,,2.0,,,Not started,2017-03-10 15:38:36.436,10/Mar/17 3:38 PM;wayne;What about suspend/resume/release?,10/Mar/17 3:39 PM;wayne;Someone with sns:Subscribe permissions would be able to get information about the Conductor's processes that they may not otherwise have access to via RBAC.,"10/Mar/17 4:07 PM;joshe;I think ""Broker"" is a confusing name for this component -- it's sort of an Event Bus or Event Notification service. The concept of ""Broker"" I thought was reserved for the thing that receives and executes plans (which is only 1 small duty of this new component)

Also, only consuming Instruction Streams won't actually tell us anything about the User Signals (update/resume/etc). The Job Type as display on {{fugue history}} is an approximation. Only the Scheduler Signal Handler actually knows about these.

The way to notify on actual signals would be for the Scheduler to emit events (including suspend/resume/release).","10/Mar/17 4:09 PM;joshe;Also doesn't the translate broker already do the ""renders the plan into a human-readable form (from protobuf)""? Can the translate broker just be extended to emit events for NON dry-run jobs of the human-readable Plan, which are then consumed by this new component and funneled into the proper notification channel (SNS)?",25/Mar/17 2:51 PM;wayne;We can probably take some lessons from the translate broker or make some common code. I'd hesitate to extend the translate broker at this time since we don't know how this is going to evolve in the future. Just my thought.,25/Mar/17 2:59 PM;wayne;Modified the description to include the process notification attribute.,"29/Mar/17 4:21 PM;joshe;Right, maybe the translate broker isn't the right place, but certainly the same code from it should be re-used (ugh, shared python libs!). I was just thinking that the work is already being done once, why not leverage it.","03/Apr/17 11:24 AM;rob;This will probably require the first swing at what the output should look like.
(look at the Dry Run Tickets)

Design Doc is also part of this work. ","18/Apr/17 10:58 AM;joshe;Overriding design principle should be to _not_ couple the implementation of the new fugue-notification-publisher to the existing implementation of existing components. The component is basically a ""router"" that will subscribe to various sources, filter the messages, and forward/re-publish them to new places.

Of course, individual code modules within the component must have intimate knowledge of their respective notification type, e.g. ""instruction stream notifications"" will know about the shape of the Instruction Stream message and how to filter it based on user configuration. These should be specifically de-coupled from the overall design of the entire component.

Work entailed for MVP includes:
* Subscribe to existing Instruction Stream SNS topic (commKit)
* Consume Instruction Stream messages via commKit
* Filter messages and assign event type (instruction stream only)
* Translate message based on event type (instruction stream only)
* Publish messages to appropriate endpoint (SNS) based on type

","18/Apr/17 11:20 AM;joshe;[~alex] It seems to make more sense to publish notifications to individual topics based on the user's configuration, rather than a separate topic per Job Type. 

For instance, the user may configure it to publish only ""drift"" (""system"" jobs) to SNS topic ""my-drift-jobs"" and then all user-related Job Types (""run"", ""kill"", ""update"", ""resume"") to another topic ""my-user-jobs"". In this case, the fugue-notification-publisher could just filter the notifications internally and publish the right things to the right topics.

This approach is simpler, as well as more flexible for when we eventually publish more than just Instruction Streams.
* The various topic names will become overloaded/too complex (e.g. ""fugue-instruction-stream-run-notifications""). 
* With multiple notification types and sub-types (e.g. ""Job.InstructionStream.[Run|Update|Resume|Kill|System]"", ""Process.StateChange.[Halt|Idle|Busy|Kill], etc.""), the number of topics to maintain will quickly grow to an unreasonable number 
* With the large number of topics, the configuration of notifications will become overly complex when the user wants to combine multiple Types to send to a single endpoint. ","18/Apr/17 1:54 PM;joshe;MVP: Pre-baked SNS topics, no user configuration at all. Consumes Instruction Stream, transforms and publishes to the appropriate topic.",18/Apr/17 2:55 PM;joshe;Go code from Planners/Manager can be re-used to translate the instruction stream,27/Apr/17 7:39 PM;scott;Preliminary PR issued. https://github.com/LuminalHQ/fugue-notifications-svc/pull/2,"01/May/17 10:26 AM;scott;Made a bunch of changes after Josh's review, which was really good. New PR for the CLI to add the topics via CFn:

https://github.com/LuminalHQ/fugue-cli-python/pull/423

That and the conductor-image change have only added a few hours of work.

","03/May/17 3:49 PM;scott;Release 0.1.0 is out. It's a POC at this stage.

us-east-1 conductor: {{ami-9596fa83}}
fugue-client: TBD",03/May/17 4:35 PM;scott;Total effort was accurate. Random interrupts slowed the finishing work a bit.
Frontend work to let a user disable enforcement for a process,FUGUE-4820,30744,,Task,To Do,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,,alex,alex,23/Feb/17 3:55 PM,19/Apr/17 3:42 PM,09/May/17 6:48 PM,,,,,,0,,,,,"-provide a cli command to let a user disable enforcement for a fugue-managed account. When executed, this command should set a vars key that will let other components know that enforcement is disabled for that account.-

-This is probably a CLI command that tells demarc to do the setting, and then demarc actually sets the key-

-*{color:red}NOTE{color}*: the vars key that gets written should be an encrypted vars key, so that we get tamper-resistance on the item. We dont want some rando with DDB write-perms to be able to turn off enforcement on prod.-

We decided to make enforcement a process level attribute. This will default to true/yes/on/whatever and users will opt-out by running the {{fugue property}} command. The {{fugue property}} command should be extended to support this attribute. It might be just a matter of updating the help and ensuring the provided information is passed to the conductor and forwarded from there to the scheduler.",,alex,eric,wayne,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,FUGUE-5034,,,,,,CloudOS Team,,,,2|i003kq:6hvi,,,Sprint 29,,,,,8.0,,,Not started,2017-03-23 21:09:51.789,"23/Mar/17 9:09 PM;wayne;Let's also talk about toggling enforcement at the process level. We know some customers aren't set up for multi-account and as a result we are going to build process level permissions in rbac v2. I see enforcement at this level as a process attribute that can be toggled with the existing {{fugue property}} command. I figure it defaults to True and is something customers opt out of. We could still support drift detection and notification by letting the scheduler tick as usual and once the broker receives a message it's check is something like {{if process.enforcing and is not dry_run}}.

Toggling at the account level is also doable, also thinking this is an account level attribute. Maybe we could extend fugue account to include {{fugue account property}} or something. 

I think we could build both of these in tandem or one sprint after the other kind of thing. 

wrt an encrypted key, the account and process records are not currently encrypted. So that's something to consider.

Finally, a global setting for a much larger ""conductor configuration"" component. I think this is farther down the road but worth considering.
",25/Mar/17 3:08 PM;wayne;[~alex] Spitballing here ... For v2 we could provide additional attributes for notification and enforcement during {{fugue run}}. Ex: {{fugue run composition.lw --enforcement off --notification off}} or something to that effect.,"10/Apr/17 9:56 AM;eric;using enforcement as a property seems like the way to go, shouldnt be too much to add that in to the frontend (cli and demarc). After that the brunt of the work will be updating the process model to include this new property and the ticker to honor it. This should be marked as cross team.","10/Apr/17 9:56 AM;eric;[~wayne] is there already work on the LRT side to support this in the scheduler, or is this going to be a new feature?",10/Apr/17 10:29 AM;wayne;Just recording here that it looks like we've got the ball rolling in the right direction in #team-product-mgmt,10/Apr/17 10:35 AM;eric;i missed that this is part of an epic https://luminal.atlassian.net/browse/FUGUE-5034 which includes all of the crossteam work here. This task will be used to split up all the frontend work needed.,,,,,,,,,,
Add RBAC types and support around setting enforcement level,FUGUE-4821,30745,,Task,To Do,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,,alex,alex,23/Feb/17 3:56 PM,01/May/17 11:45 AM,09/May/17 6:48 PM,,,,,,0,,,,,,,alex,rob,wayne,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,FUGUE-5034,,,,,,CloudOS Team,,,,2|i003kq:6hvk,,,Sprint 29,,,,,3.0,,,Not started,2017-03-25 15:02:31.067,25/Mar/17 3:02 PM;wayne;RBAC types will be for process level enforcement.,03/Apr/17 11:33 AM;rob;Adding RBAC to control who can use this function,,,,,,,,,,,,,,
Have the broker ignore jobs for processes where enforcement is disabled,FUGUE-4822,30746,30611,Sub-task,To Do,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,,alex,alex,23/Feb/17 3:59 PM,10/Apr/17 11:34 AM,09/May/17 6:48 PM,,,,,,0,NEEDS_INFO,,,,"The Broker will receive the enforcement level and Job Type as part of the incoming Instruction Stream package from the Manager.

If the enforcement level is ""off"", then it should skip work for SYSTEM Jobs and send a success back to the scheduler. 

On other Job types besides SYSTEM (user-initiated), the Broker _should_ do work, regardless of enforcement being ""off"".",,alex,joshe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,LRT Team,,,,2|hzytuu:000000r00000008xa,,,Sprint 29,,,,,,,,Not started,2017-03-29 15:06:05.882,"29/Mar/17 3:06 PM;joshe;[~alex] So if the enforcement is account-specific, this data will presumably be managed by the accounts service? So the accounts client needs to have an additional method to check on enforcement level (on/off) and the accounts service needs to handle any logic for storing that information.

Is there already a ticket somewhere for the accounts work?",,,,,,,,,,,,,,,
Bugfix: Job Results lost if recording fails,FUGUE-4846,31010,,Task,PR Merged,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,joshe,joshe,joshe,28/Feb/17 3:32 PM,04/May/17 10:51 AM,09/May/17 6:48 PM,,,,Scheduler,,0,NEEDS_INFO,sprint29,,,"When the Scheduler receives a Job Result, it is effectively ""popped"" from the queue (based on commKit's behavior). If the result fails to be recorded in vars, this result is lost (since its not longer in the queue) and the Process will eventually time out since it will not be receiving another result.

A quick partial fix is to ""re-queue"" the Job result on failures, by sending it back to the queue where it came from. Order is not important.

FYI [~alex] [~tyler]",,alex,joshe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,LRT Team,,,,2|hzytuu:000000r000000hzp,,,Sprint 29,,,,,3.0,,,Not started,2017-02-28 16:05:32.227,28/Feb/17 4:05 PM;alex;can we not retry in the process that's trying to put it into vars?,"28/Feb/17 4:10 PM;joshe;We could certainly retry, but there are a multitude of errors that can cause this to happen -- not just CAS failures. Retrying also holds up the entire system which is synchronous. 

Most importantly, if a CAS fails or a lock is contended, it means that the state of the Process has changed in some way, so the entire logic chain needs to be re-executed anyway, which is the effect that would happen if the message was re-queued.","28/Feb/17 4:26 PM;joshe;Alternatively we could build a temporary in-memory queue of all the results available on each tick. If processing one fails, instead of sending the results back to SNS/SQS, any failures are added back into the in-memory queue. It keeps going until the queue is drained. This would eliminate some complexity of using external resources to maintain the queue.

This however has a similar pitfall: if a catastrophic failure (scheduler crash) occurs, all items in the in-memory queue will be lost.","21/Apr/17 5:26 PM;joshe;Use new commKit nack on failure, ack on success.","24/Apr/17 2:02 PM;joshe;The work is the use pocky's new interface `next_message`, which returns both and ack and nack callback. The work will be wrapped in a try/except block -- if the Vars data is stored successfully, the ack will be called, if an exceptions is raised prior to saving the data, the nack should be called. 

There is no practical way of integration testing this other than using a debugger to stop the process before it calls the vars put, stop the Vars daemon, then resume the process (which should cause a write failure).",24/Apr/17 2:03 PM;joshe;Estimate 3 (high for the amount of code) due to difficulty testing.,03/May/17 1:46 PM;joshe;https://github.com/LuminalHQ/fugue-scheduler/pull/246,,,,,,,,,
"Bugfix: An RBAC user with no ""fugue status"" perms will have other authorized commands fail",FUGUE-4926,31714,,Task,In Progress,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,scott,erez,erez,11/Mar/17 2:44 PM,09/May/17 10:46 AM,09/May/17 6:48 PM,,,,,,0,sprint29,,,,"Fugue CLI Version: 1.0.3-2016-506a18a76db7c4377577292e89ccdc6344a72eca
VARs Version: 1.0.0-1983-38519039e173a5683206032d44ae769956fb9f81
LWDoc Version: 0.19.0
LWC Version: 0.19.0
conductor AMI: ami-6118b877

Steps to reproduce:
0) Create an RBAC user that only has {{fugue run}} and {{fugue kill}} permissions
1) Run the composition of your choice
2) Kill it

Expected results:
This should work

Observed results:
Run throws an error at the end however when switching to another account to verify, it actually does work
{code}
$> fugue run _FugueDemo.lw -a _FugueDemo
[ fugue run ] Running _FugueDemo.lw

Run Details:
    Account: default
    Alias: _FugueDemo

Compiling Ludwig file /Users/erezsemaria/Desktop/Work/Technical/zbb_testing_platform/rbac_test/_FugueDemo.lw
[ OK ] Successfully compiled. No errors.


Uploading compiled Ludwig composition to S3...
[ OK ] Successfully uploaded.

Requesting the Conductor to create and run process based on composition ...
[ ERROR ] Signal Error. 401 - 'Unauthorized': Command 'STATUS' is not allowed for principal 'accountkillprocess_accountrunprocess' on account None based on stored policy.
{code}

Kill fails since it seems unable to find the process
{code}
$>fugue kill _FugueDemo -y
[ fugue kill ] Killing  process with Alias: _FugueDemo

[ ERROR ] No process with Alias: _FugueDemo could be found. Run the 'fugue status' command to view available Fugue processes.
{code}",,erez,scott,wayne,,,,,,,,,,,,11/Mar/17 4:41 PM;erez;fugue_report-2017-03-11-19-37-38.zip;https://luminal.atlassian.net/secure/attachment/26913/fugue_report-2017-03-11-19-37-38.zip,11/Mar/17 2:45 PM;erez;rbac_test_policy.lw;https://luminal.atlassian.net/secure/attachment/26912/rbac_test_policy.lw,,,,,,,,,,,,,,,{},,,,,,,,CloudOS Team,,,,2|hzytuu:000000r0000000011zzzzzi,,,Sprint 29,,,,,5.0,,,Not started,2017-03-20 10:50:01.558,"20/Mar/17 10:50 AM;wayne;For fugue run, the CLI fires off the run itself, then a status request to display the follow-up status table according to spec. Unless we change the spec, I think this should stay the same.

For fugue kill, the CLI runs a status request, then uses that information to fire off the kill. I think the conductor can handle a kill message without first running status, but we would need to evaluate the conductor code and test it well.

Estimating 5","27/Mar/17 9:17 AM;scott;If the user is authorized for run/kill, then they're implicitly authorized to run status on the target FID.

Seems like this needs to be a standard policy definition somehow.  ""User Bob can run/kill processes in account X, so also can run status on same.""","09/Apr/17 8:50 PM;scott;When getting process status as part of run or kill, the RBAC principal is passed from the CLI to demarc to the scheduler client. The authZ decision is based on policy for that principal. Getting around in the code that would probably require some ugly gyrations.

Instead, RBAC policy could be modified to allow any principal with `run` or `kill` authorization to also get `status` for free -- i.e. without having to explictly call it out in the policy.  

Will stick with estimate of 5 and revise if necessary on Monday.",08/May/17 9:46 AM;scott;https://github.com/LuminalHQ/fugue-rbac-ludwig/pull/37/files,"09/May/17 10:46 AM;scott;The policy approach was rejected.

New proposal is to fix `kill` in the CLI so that it will try to kill a process even if the user is not authorized to get process info.

https://github.com/LuminalHQ/fugue-cli-python/pull/430",,,,,,,,,,,
[QA] validation,FUGUE-4986,32017,30611,QA-Validation,To Do,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,,chrisc,chrisc,20/Mar/17 9:35 AM,20/Mar/17 9:35 AM,09/May/17 6:48 PM,,,,,,0,,,,,,,chrisc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,QA Team,,,,2|i006ae:,,,Sprint 29,,,,,,,,Not started,,,,,,,,,,,,,,,,,
Support having data files in Ludwig libraries,FUGUE-4999,32062,,Task,To Do,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,,jasper,jasper,20/Mar/17 12:43 PM,01/May/17 9:30 AM,09/May/17 6:48 PM,,,,Ludwig Language & Compiler,,0,EndOf28,ludwig_language_design,Sprint27Spillover,Sprint28,"Assume that we have a library {{fugue-bananas}} which loads a file called {{policy.json}} and uses that to set up some {{IAM}} policy that is used in your composition.  There is currently no way to do this -- where do you put this file?  On your {{LUDWIG_PATH}}?

Now, what happens if a second library {{fugue-coconuts}} also wants to include a file called {{policy.json}}?  If we naively decided that it was a good idea to put the other {{policy.json}} on our {{LUDWIG_PATH}}, then we have another problem!

Further problems arise when we want to create a snapshot of a composition that uses both libraries -- which {{policy.json}} is included in the snapshot?

For now, we've been able to get around this by basically inlining all this data into Ludwig files but that's not a good long-term solution.

The goal of this ticket is to design and implement a working solution.  One solution I could come up with is the following:

1. We document an _idiom_ where we include the necessary files within the module hierarchy.  For example, a {{policy.json}} that is relevant for the {{Fugue/Bananas.lw}} module is placed in {{Fugue/Bananas/policy.json}}.

2. We add an builtin function __Ludwig_Path_filePathForModule` which gives you the full filepath for a module.  E.g. {{__Ludwig_Path_filePathForModule(""Fugue.Bananas"")}} gives you {{/usr/opt/fugue/lib/Fugue/Bananas.lw}}.

3. We add convenience ludwig functions to join module paths and data paths.  For example,
{{Path.getDataFilePath(""Fugue.Bananas"", ""policy.json"")}} would give you the full path to {{/usr/opt/fugue/lib/Fugue/Bananas/policy.json}}.  You can then safely {{readFile}} on the result.

Disadvantage: the {{_Ludwig_Path_filePathForModule}} is now the only effectful that _doesn't_ get saved in {{env.json}} when using {{environment.json}}.  This should _usually_ be fine, but this can lead to problems if you use the path for anything else than reading -- for example if you use it to name a VPC.  Should we use a different call that only allows you to read these files to prevent people from making that mistake?",,jasper,maciej,,,,,,,,,,PDOC-547,,,,,,,,,,,,,,,,,,,{},,,,,,,,Toolchain Team,,,,2|hzytuu:000000r0001qzzzzzzzx0d,,,Sprint 27,Sprint 28,Sprint 29,,,8.0,,,Not started,2017-03-21 00:28:07.328,"21/Mar/17 12:28 AM;maciej;This strikes me as yet another reason for having packages as a distribution unit.

Then we could do the same thing that's done for Haskell in {{.cabal}} files: have a {{data-files}} section which lists data files that get copied to a special directory and an API to access it.

{code}
policy: String.readFileUtf8(Packages.Paths.getDataFile(""policy.json""))
{code}

With respect to the above, the idiom as described is also fine. My vote would be on adding a restricted function that only allows reading those files directly without returning any file paths:

{code}
value1:
    Package.readFileFromModuleDirectory
        { module: ""Fugue.Bananas""
        , file: ""policy.json""
        }
value2:
    Package.readFileUtf8FromModuleDirectory
        { ... }
{code}",,,,,,,,,,,,,,,
Ludwig compiler should be able to return all validation errors,FUGUE-5013,32084,,Task,Done,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,,robertds,robertds,20/Mar/17 5:49 PM,08/May/17 9:31 AM,09/May/17 6:48 PM,,,,Ludwig Language & Compiler,,0,ludwig_error_messages,,,,"Currently, lwc stops after finding the first validation error when compiling a composition. Based on a Slack discussion on March 8th, 2017 we discussed that it seems like a reasonable improvement to the compiler to generate all validation errors found before stopping.

I.e. For a Ludwig composition that has no compile errors, lwc should be able to return all validation errors for that composition, rather than stopping at the first.

It seems reasonable to make this a command line flag if necessary.

The use case for this would be end users who are testing compositions, as well as automated tools  that may want to collect all validation errors for a composition.",,robertds,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,FUGUE-3225,,,5.0,,,Toolchain Team,,,,2|hzytuu:000000r0000000011zy,,,Sprint 29,,,,,5.0,,,Not started,,,,,,,,,,,,,,,,,
Design versioning for Ludwig libraries,FUGUE-5016,32087,,Task,In Progress,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,brett,jasper,jasper,20/Mar/17 8:02 PM,01/May/17 9:30 AM,09/May/17 6:48 PM,,,,Ludwig Language & Compiler,,0,EndOf28,ludwig_language_design,Sprint27Spillover,Sprint28,"It's very tempting to send compositions in between people but there's little metadata attached to that:

* What versions of {{lwc}} does it work with?
* What versions of {{fugue-client}} does it require?
* Does it require any custom libraries?

Most of this is useful for libraries as well:

* What libraries does this library depend on?
* Does this library depend on a specific compiler version?

It would be nice to create some mechanism that users can _opt-in_ to.  Currently, I'm thinking of 3 designs:

# We add another (JSON?) file to the libraries.  See https://luminal.atlassian.net/wiki/display/PE/Ludwig+Packages
# We add some sort of annotation, e.g. {{@requireVersion(""fugue-aws"", "">= 0.2.0"")}}
# We encode the version info in Ludwig, e.g.  {{version: dependencies [""fugue-aws >= 0.2.0""]}}

I'm a proponent of (3) because of dogfooding and the sake of not having to worry about another file, but there's advantages and disadvantages to every approach.

This ticket does not necessarily include a full implementation.",,jasper,maciej,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,Toolchain Team,,,,2|hzytuu:000000r0001qzzzzzzzx0g,,,Sprint 27,Sprint 28,Sprint 29,,,8.0,,,Not started,2017-03-21 05:17:13.582,"21/Mar/17 5:17 AM;maciej;Encoding package information / version numbers in Ludwig would be nice. I think it would also be worthwhile to have {{exposedModules}} and {{otherModules}} sections as in {{Cabal}} files.

We could make this opt-in by only generating warnings. For instance let's say we had a file {{Version.lw}} at the root of library directory:
{code}
package: ""ludwig-bananas""
version: ""1.0.0""
exposedModules:
  [ ""Ludwig.Bananas""
  ]
dependencies:
  [ (""fugue-aws"", "">="", ""1.0.0"")
  ]
{code}
Then, if we had
{code}
❯ tree ludwig-bananas
ludwig-bananas
├── Ludwig
│   ├── Bananas.lw
│   └── Oranges.lw
└── Version.lw
{code}
We could warn:
{code}
Warning: undeclared module Ludwig.Oranges. Consider adding it to exposedModules or otherModules in Version.lw
{code}
Similarly, if {{fugue-aws}} version was incorrect we could warn about it. If {{fugue-aws}} didn't define {{Version.lw}} we'd warn:
{code}
Warning: Can't find Version.lw for dependency: fugue-aws
{code}
And so on. Defining packages and keeping things tidy would be a guidance rather than requirement.",,,,,,,,,,,,,,,
Write ECS Pattern Lib,FUGUE-5029,32204,,Story,In Progress,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,chrisg,alex,alex,22/Mar/17 12:11 PM,03/May/17 4:24 PM,09/May/17 6:48 PM,,,,,,0,EndOf28,,,,"We should start with the Ludwig definitions and a higher-level pattern library to figure out how we want ECS+Fugue to look.

Alex has talked to a customer at VMWare who is happy to look at what we come up with for an ECS Pattern Library to provide feedback so we can iterate. Sync up with Alex when we start getting something that we want to show to people",,alex,chrisg,tyler,,,,,,,FUGUE-5031,,,,,,,,,,,,,,,,,,,,,{},,FUGUE-5028,,,,,,LRT Team,,,,2|hzytuu:000000r00000000i,,,Sprint 28,Sprint 29,,,,8.0,,,Not started,2017-04-07 10:15:45.764,"07/Apr/17 10:15 AM;chrisg;I think there is probably a couple of things that come out of this:

1. High level interface for running tasks on an ECS cluster with {{ELB.LoadBalancer}} integration
2. Pattern library for provisioning an ECS cluster 

The first is probably the more interesting part, in that it covers how tasks are managed in ECS via Fugue. This may be nothing more than fugue-aws support for ECS tasks, but it's hard to say without having done the research.

The second is probably closer to {{Fugue.AWS.Pattern.Network}} in that it provides one convenient way to get an ECS cluster running (using the default AMI), but user's are free to build a cluster using whatever method best suits their needs. Some utility functions for working with things like ECR/docker repos and images may also fall out of this.","07/Apr/17 10:20 AM;chrisg;I'm going to estimate this as an 8, which gives a couple of days for each task. This should account for hand-waviness in what we're building here (based on the outcome of FUGUE-4790) and build in some time to talk with QA about what's coming in FUGUE-5031.",17/Apr/17 7:34 AM;tyler;Understanding that this will not get picked up until week two.,,,,,,,,,,,,,
Write ECS Core Ludwig,FUGUE-5030,32205,,Story,In Progress,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,chrisg,alex,alex,22/Mar/17 12:11 PM,02/May/17 4:59 PM,09/May/17 6:48 PM,,,,,,0,EndOf28,,,,Write the core Ludwig behind the ECS Pattern and whatever else is needed to cover the service.,,alex,chrisg,tyler,,,,,,,FUGUE-5031,,,,,,,,,,,,,,,,,,,,,{},,FUGUE-5028,,,,,,LRT Team,,,,2|hzytuu:000000r000000009,,,Sprint 28,Sprint 29,,,,3.0,,,Not started,2017-04-07 11:14:20.871,"07/Apr/17 11:14 AM;chrisg;It doesn't look like ECS has a ton of data types, but I'm going to estimate this as a 3 to account for integrating with our existing service coverage (like ELB) and to allow time to review the core type with product and engineering (based on how we've done things in the past the review would be with [~alex], [~tyler], and [~chrisg]). ",17/Apr/17 7:34 AM;tyler;Understanding that this will not get picked up until week two.,"01/May/17 2:25 PM;chrisg;Current status:

Core types are generated and appear to be (mostly) okay. I'll open a PR with what I have so others can review. I'm still going through the {{TaskDefinition}} and {{Service}} examples from the AWS docs to make sure that everything looks sane and that there's not some case that can't be expressed. I'll add those here when they're finished.

There are a couple of issues I've run into:

1. To support ECS we need to upgrade the botocore version used to generate the core types. This isn't a show stopper, but will require tweaks to many of our existing services to avoid adding unsupported features (and in a few cases ensuring enum order is maintained in the protobuf definitions).

2. An ECS service has a {{desiredCount}} property that controls how many tasks are launched on the cluster. Much like {{AutoScalingGroup}}s, this value is manipulated by the auto scaling application service in response to CloudWatch metrics. We'll need to figure out a good way to express this in ludwig (and handle it on the planner side).

3. An ECS service can be attached to a LoadBalancer, which will result in the ECS agent dynamically adding cluster instances to the LoadBalancer. Will need to make sure that our {{LoadBalancer}} support can handle this. This is actually handled really well by the core types, but I'm noting it here so I don't forget about it.

Also, ECS Services lean on  the Application LoadBalancer, Application AutoScaling, and CloudWatch Logs services to provide a lot of container functionality, so we may need to bump those up in priority to make full use of ECS support (I'll be adding {{ExternalOnly}} definitions for these as needed).","01/May/17 2:58 PM;chrisg;I'm leaving this as a 3 for sprint 29 to account for the work of updating botocore in fugue-core-aws (there are likely some other bugs from the backlog, like updating instance types and lambda runtimes, that will fall out of this).",01/May/17 4:31 PM;chrisg;Progress on core types: https://github.com/LuminalHQ/fugue-core-aws/pull/364,02/May/17 4:59 PM;chrisg;Updating botocore in fugue-core-aws: https://github.com/LuminalHQ/fugue-core-aws/pull/366,,,,,,,,,,
Runtime ECS Coverage Implementation,FUGUE-5031,32206,,Story,To Do,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,chrisg,alex,alex,22/Mar/17 12:13 PM,01/May/17 7:48 AM,09/May/17 6:48 PM,,,,,,0,,,,,*NB* This is a placeholder task until we have the Pattern Lib and Core Ludwig for ECS done,,alex,chrisg,tyler,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,FUGUE-5028,,,,,,LRT Team,,,,2|i003kq:6hf,,,Sprint 29,,,,,13.0,,,Not started,2017-03-24 15:37:20.563,"24/Mar/17 3:37 PM;tyler;From [~alex] the goal of FUGUE-5030, FUGUE-5029, FUGUE-5031 is to get FUGUE-5030 and FUGUE-5029 finished to show customers and see if that is what they would like.

If FUGUE-5031 falls through to 29 it falls through to 29.","07/Apr/17 11:27 AM;chrisg;A lot of the specifics regarding ECS will emerge as part of FUGUE-4790, so I'm unable to provide a details estimated. However based on past experience I'm going to give this a 13. That's split between roughly 8 points of implementation and developer testing, followed by 5 points for QA.
",,,,,,,,,,,,,,
Bugfix: Autoscaling policy getting incorrect cooldown default,FUGUE-5057,32302,,Task,Done,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,tyler,erez,erez,23/Mar/17 1:22 PM,04/May/17 2:45 PM,09/May/17 6:48 PM,,,,,,0,sprint29,,,,"Fugue CLI Version: 1.2.0-2249-81daade2d63427d3e434b75364a2b92834492d04
VARs Version: 3.0.0-2308-5bbcc0ca4561abc26e46f43d2b79ef12637465a5
LWDoc Version: 0.21.0
LWC Version: 0.21.0
Conductor AMI ami-77b30361 (last RC for 26, TEAM version)

Steps to reproduce:
Run attached composition cw_scaling.lw

Expected results:
According to AWS documentation (http://docs.aws.amazon.com/cli/latest/reference/autoscaling/put-scaling-policy.html) the ""cooldown"" for an autoscaling policy shall default to whatever the DefaultCooldown field is in the ASG the scaling policy belongs to.
In this case, 120
{code}
my-asg: AutoScalingGroup.new(
  AutoScalingGroup.default with {
    minSize: 1,
    maxSize: 2,
    defaultCooldown: 120,
    healthCheckGracePeriod: 120,
    launchConfiguration: my-launch-config,
    subnets: my-network.publicSubnets
  })
my-launch-config: LaunchConfiguration {
  instanceType: EC2.T2_nano,
  image: 'ami-7172b611',  # default Amazon Linux in us-west-2
  securityGroups: [my-sg]
}
my-scaling-policy: ScalingPolicy {
  policyName: ""example-scaling-policy"",
  adjustmentType: ChangeInCapacity,
  autoScalingGroup: my-asg,
  scalingAdjustment: 2
}
{code}

Observed results:
The autoscaling policy cooldown is actually defaulting to 300 which is the default value of an ASG cooldown http://docs.aws.amazon.com/autoscaling/latest/userguide/Cooldown.html, however in this case the ASG doesn't have a default value because we explicitly choose one.
{code}
$ fugue  history cw_scaling.lw

Jobs History Report for Erez/877453989326 - Thu Mar 23 2017 11:14am
cw_scaling.lw (bbf265c9-65ac-44c5-b013-a680b3a394cd)

Row Id      Started    Last Updated    Job Type    Jobs Ran    Job Status    Description
----------  ---------  --------------  ----------  ----------  ------------  -------------
1490289265  11:14am                    SYSTEM      1
1490289235  10:57am    11:14am         SYSTEM      32          SUCCEEDED     JobSucceeded
{code}

{code}
$ fugue status cw_scaling.lw
{
    ""account_id"": ""fugue-1490207615237"",
    ""alias"": ""cw_scaling.lw"",
    ""created"": 1490288267,
    ""current_job_status"": {
        ""current_time"": 1490289731,
        ""dry_run"": false,
        ""job_id"": 1490289715,
        ""start_time"": 1490289715
    },
    ""fid"": ""bbf265c9-65ac-44c5-b013-a680b3a394cd"",
    ""last_job_id"": 1490289685,
    ""last_job_status"": {
        ""description"": ""JobSucceeded"",
        ""dry_run"": false,
        ""finish_time"": 1490289688,
        ""job_id"": 1490289685,
        ""job_status"": ""SUCCEEDED"",
        ""message"": ""SUCCESS"",
        ""start_time"": 1490289685,
        ""status_code"": 200
    },
    ""last_message"": ""SUCCESS"",
    ""resources"": {
        ""autoScalingGroups"": {
            ""bbf265c9-65ac-44c5-b013-a680b3a394cd.55c1b2b4-b2aa-5496-80c1-057c2ed5323d"": {
                ""region"": ""us-west-2"",
                ""resource"": ""autoScalingGroup"",
                ""value"": {
                    ""autoScalingGroup"": {
                        ""AutoScalingGroupARN"": ""arn:aws:autoscaling:us-west-2:877453989326:autoScalingGroup:af1be71a-5ae1-4b3d-9919-392fd5e8ad79:autoScalingGroupName/bbf265c9-65ac-44c5-b013-a680b3a394cd.55c1b2b4-b2aa-5496-80c1-057c2ed5323d"",
                        ""AutoScalingGroupName"": ""bbf265c9-65ac-44c5-b013-a680b3a394cd.55c1b2b4-b2aa-5496-80c1-057c2ed5323d"",
                        ""AvailabilityZones"": [
                            ""us-west-2a"",
                            ""us-west-2b""
                        ],
                        ""CreatedTime"": 1490288308,
                        ""DefaultCooldown"": 120,
                        ""DesiredCapacity"": 1,
                        ""EnabledMetrics"": [],
                        ""HealthCheckGracePeriod"": 120,
                        ""HealthCheckType"": ""EC2"",
                        ""Instances"": [
                            {
                                ""AvailabilityZone"": ""us-west-2b"",
                                ""HealthStatus"": ""Healthy"",
                                ""InstanceId"": ""i-0f37ef73f555862a4"",
                                ""LaunchConfigurationName"": ""bbf265c9-65ac-44c5-b013-a680b3a394cd.4719461b-317a-54d0-a275-d0232dc05a4a"",
                                ""LifecycleState"": ""InService"",
                                ""ProtectedFromScaleIn"": false
                            }
                        ],
                        ""LaunchConfigurationName"": ""bbf265c9-65ac-44c5-b013-a680b3a394cd.4719461b-317a-54d0-a275-d0232dc05a4a"",
                        ""LoadBalancerNames"": [],
                        ""MaxSize"": 2,
                        ""MinSize"": 1,
                        ""NewInstancesProtectedFromScaleIn"": false,
                        ""SuspendedProcesses"": [],
                        ""Tags"": [
                            {
                                ""Key"": ""Fugue ID"",
                                ""PropagateAtLaunch"": true,
                                ""ResourceId"": ""bbf265c9-65ac-44c5-b013-a680b3a394cd.55c1b2b4-b2aa-5496-80c1-057c2ed5323d"",
                                ""ResourceType"": ""auto-scaling-group"",
                                ""Value"": ""bbf265c9-65ac-44c5-b013-a680b3a394cd.55c1b2b4-b2aa-5496-80c1-057c2ed5323d""
                            },
                            {
                                ""Key"": ""Name"",
                                ""PropagateAtLaunch"": true,
                                ""ResourceId"": ""bbf265c9-65ac-44c5-b013-a680b3a394cd.55c1b2b4-b2aa-5496-80c1-057c2ed5323d"",
                                ""ResourceType"": ""auto-scaling-group"",
                                ""Value"": ""my-asg""
                            }
                        ],
                        ""TargetGroupARNs"": [],
                        ""TerminationPolicies"": [
                            ""Default""
                        ],
                        ""VPCZoneIdentifier"": ""subnet-e1aa3186,subnet-2e902d67""
                    },
                    ""notificationConfigurations"": [],
                    ""timestamp"": 1490289721
                }
            }
        },
        ""dhcpOptions"": {
            ""bbf265c9-65ac-44c5-b013-a680b3a394cd.8575f786-35ca-5416-a5c4-105d9843418f"": {
                ""region"": ""us-west-2"",
                ""resource"": ""dhcpOptions"",
                ""value"": {
                    ""DhcpConfigurations"": [
                        {
                            ""Key"": ""domain-name"",
                            ""Values"": [
                                {
                                    ""Value"": ""us-west-2.compute.internal""
                                }
                            ]
                        },
                        {
                            ""Key"": ""domain-name-servers"",
                            ""Values"": [
                                {
                                    ""Value"": ""AmazonProvidedDNS""
                                }
                            ]
                        }
                    ],
                    ""DhcpOptionsId"": ""dopt-c1a706a6"",
                    ""Tags"": [
                        {
                            ""Key"": ""Fugue ID"",
                            ""Value"": ""bbf265c9-65ac-44c5-b013-a680b3a394cd.8575f786-35ca-5416-a5c4-105d9843418f""
                        }
                    ],
                    ""timestamp"": 1490289639
                }
            }
        },
        ""internetGateways"": {
            ""bbf265c9-65ac-44c5-b013-a680b3a394cd.8be3a1fb-6138-5754-ae68-916918c53353"": {
                ""region"": ""us-west-2"",
                ""resource"": ""internetGateway"",
                ""value"": {
                    ""Attachments"": [
                        {
                            ""State"": ""available"",
                            ""VpcId"": ""vpc-5d24693a""
                        }
                    ],
                    ""InternetGatewayId"": ""igw-abdfd1cf"",
                    ""Tags"": [
                        {
                            ""Key"": ""Fugue ID"",
                            ""Value"": ""bbf265c9-65ac-44c5-b013-a680b3a394cd.8be3a1fb-6138-5754-ae68-916918c53353""
                        },
                        {
                            ""Key"": ""Name"",
                            ""Value"": ""Example Network-IGW""
                        },
                        {
                            ""Key"": ""network"",
                            ""Value"": ""public""
                        }
                    ],
                    ""timestamp"": 1490289638
                }
            }
        },
        ""launchConfigurations"": {
            ""bbf265c9-65ac-44c5-b013-a680b3a394cd.4719461b-317a-54d0-a275-d0232dc05a4a"": {
                ""region"": ""us-west-2"",
                ""resource"": ""launchConfiguration"",
                ""value"": {
                    ""BlockDeviceMappings"": [],
                    ""ClassicLinkVPCSecurityGroups"": [],
                    ""CreatedTime"": 1490288303,
                    ""EbsOptimized"": false,
                    ""ImageId"": ""ami-7172b611"",
                    ""InstanceMonitoring"": {
                        ""Enabled"": true
                    },
                    ""InstanceType"": ""t2.nano"",
                    ""KernelId"": """",
                    ""KeyName"": """",
                    ""LaunchConfigurationARN"": ""arn:aws:autoscaling:us-west-2:877453989326:launchConfiguration:c6fcab7b-dd24-48a1-a972-08898bce133f:launchConfigurationName/bbf265c9-65ac-44c5-b013-a680b3a394cd.4719461b-317a-54d0-a275-d0232dc05a4a"",
                    ""LaunchConfigurationName"": ""bbf265c9-65ac-44c5-b013-a680b3a394cd.4719461b-317a-54d0-a275-d0232dc05a4a"",
                    ""RamdiskId"": """",
                    ""SecurityGroups"": [
                        ""sg-e8edf690""
                    ],
                    ""UserData"": """",
                    ""timestamp"": 1490289715
                }
            }
        },
        ""metricAlarms"": {
            ""bbf265c9-65ac-44c5-b013-a680b3a394cd.cabf2592-d212-566f-9d45-278f0f9f4d17"": {
                ""region"": ""us-west-2"",
                ""resource"": ""metricAlarm"",
                ""value"": {
                    ""ActionsEnabled"": true,
                    ""AlarmActions"": [
                        ""arn:aws:autoscaling:us-west-2:877453989326:scalingPolicy:0a0f1c31-f9df-4680-a1e8-9a3e124aff4f:autoScalingGroupName/bbf265c9-65ac-44c5-b013-a680b3a394cd.55c1b2b4-b2aa-5496-80c1-057c2ed5323d:policyName/example-scaling-policy""
                    ],
                    ""AlarmDescription"": ""Scale-up if CPU > 70% for 2 minutes"",
                    ""AlarmName"": ""cpu-utilization-high"",
                    ""ComparisonOperator"": ""GreaterThanThreshold"",
                    ""EvaluationPeriods"": 2,
                    ""MetricName"": ""CPUUtilization"",
                    ""Namespace"": ""AWS/EC2"",
                    ""Period"": 60,
                    ""Statistic"": ""Average"",
                    ""Threshold"": 7,
                    ""timestamp"": 1490288309
                }
            }
        },
        ""routeTableAssociations"": {
            ""bbf265c9-65ac-44c5-b013-a680b3a394cd.307b6cef-418f-51d5-849c-28d4561aed49.6ded3489-9836-535b-8dd3-432d8fa3c0c9.association"": {
                ""region"": ""us-west-2"",
                ""resource"": ""routeTableAssociation"",
                ""value"": {
                    ""Main"": false,
                    ""RouteTableAssociationId"": ""rtbassoc-a46675c2"",
                    ""RouteTableId"": ""rtb-1ab45e7c"",
                    ""SubnetId"": ""subnet-2e902d67"",
                    ""timestamp"": 1490289637
                }
            },
            ""bbf265c9-65ac-44c5-b013-a680b3a394cd.307b6cef-418f-51d5-849c-28d4561aed49.76e508fc-5ccd-5893-8bad-d8faf65fd30a.association"": {
                ""region"": ""us-west-2"",
                ""resource"": ""routeTableAssociation"",
                ""value"": {
                    ""Main"": false,
                    ""RouteTableAssociationId"": ""rtbassoc-07667561"",
                    ""RouteTableId"": ""rtb-1ab45e7c"",
                    ""SubnetId"": ""subnet-e1aa3186"",
                    ""timestamp"": 1490289637
                }
            }
        },
        ""routeTables"": {
            ""bbf265c9-65ac-44c5-b013-a680b3a394cd.307b6cef-418f-51d5-849c-28d4561aed49"": {
                ""region"": ""us-west-2"",
                ""resource"": ""routeTable"",
                ""value"": {
                    ""Associations"": [
                        {
                            ""Main"": false,
                            ""RouteTableAssociationId"": ""rtbassoc-07667561"",
                            ""RouteTableId"": ""rtb-1ab45e7c"",
                            ""SubnetId"": ""subnet-e1aa3186""
                        },
                        {
                            ""Main"": false,
                            ""RouteTableAssociationId"": ""rtbassoc-a46675c2"",
                            ""RouteTableId"": ""rtb-1ab45e7c"",
                            ""SubnetId"": ""subnet-2e902d67""
                        }
                    ],
                    ""PropagatingVgws"": [],
                    ""RouteTableId"": ""rtb-1ab45e7c"",
                    ""Routes"": [
                        {
                            ""DestinationCidrBlock"": ""10.0.0.0/16"",
                            ""GatewayId"": ""local"",
                            ""Origin"": ""CreateRouteTable"",
                            ""State"": ""active""
                        },
                        {
                            ""DestinationCidrBlock"": ""0.0.0.0/0"",
                            ""GatewayId"": ""igw-abdfd1cf"",
                            ""Origin"": ""CreateRoute"",
                            ""State"": ""active""
                        }
                    ],
                    ""Tags"": [
                        {
                            ""Key"": ""Fugue ID"",
                            ""Value"": ""bbf265c9-65ac-44c5-b013-a680b3a394cd.307b6cef-418f-51d5-849c-28d4561aed49""
                        },
                        {
                            ""Key"": ""Name"",
                            ""Value"": ""Example Network-PUBLIC-RT""
                        },
                        {
                            ""Key"": ""network"",
                            ""Value"": ""public""
                        }
                    ],
                    ""VpcId"": ""vpc-5d24693a"",
                    ""timestamp"": 1490289637
                }
            }
        },
        ""scalingPolicies"": {
            ""bbf265c9-65ac-44c5-b013-a680b3a394cd.dcb823b5-5f0f-56e2-8417-a9549a70f69d"": {
                ""region"": ""us-west-2"",
                ""resource"": ""scalingPolicy"",
                ""value"": {
                    ""AdjustmentType"": ""ChangeInCapacity"",
                    ""Alarms"": [
                        {
                            ""AlarmARN"": ""arn:aws:cloudwatch:us-west-2:877453989326:alarm:cpu-utilization-high"",
                            ""AlarmName"": ""cpu-utilization-high""
                        }
                    ],
                    ""AutoScalingGroupName"": ""bbf265c9-65ac-44c5-b013-a680b3a394cd.55c1b2b4-b2aa-5496-80c1-057c2ed5323d"",
                    ""Cooldown"": 300,
                    ""PolicyARN"": ""arn:aws:autoscaling:us-west-2:877453989326:scalingPolicy:0a0f1c31-f9df-4680-a1e8-9a3e124aff4f:autoScalingGroupName/bbf265c9-65ac-44c5-b013-a680b3a394cd.55c1b2b4-b2aa-5496-80c1-057c2ed5323d:policyName/example-scaling-policy"",
                    ""PolicyName"": ""example-scaling-policy"",
                    ""PolicyType"": ""SimpleScaling"",
                    ""ScalingAdjustment"": 2,
                    ""StepAdjustments"": [],
                    ""timestamp"": 1490289715
                }
            }
        },
        ""securityGroups"": {
            ""bbf265c9-65ac-44c5-b013-a680b3a394cd.7cd99e29-bd1f-548d-b31b-331656461a1c"": {
                ""region"": ""us-west-2"",
                ""resource"": ""securityGroup"",
                ""value"": {
                    ""Description"": ""Example Security Group"",
                    ""GroupId"": ""sg-e8edf690"",
                    ""GroupName"": ""bbf265c9-65ac-44c5-b013-a680b3a394cd.7cd99e29-bd1f-548d-b31b-331656461a1c"",
                    ""IpPermissions"": [],
                    ""IpPermissionsEgress"": [],
                    ""OwnerId"": ""877453989326"",
                    ""Tags"": [
                        {
                            ""Key"": ""Fugue ID"",
                            ""Value"": ""bbf265c9-65ac-44c5-b013-a680b3a394cd.7cd99e29-bd1f-548d-b31b-331656461a1c""
                        },
                        {
                            ""Key"": ""Name"",
                            ""Value"": ""my-sg""
                        }
                    ],
                    ""VpcId"": ""vpc-5d24693a"",
                    ""timestamp"": 1490289636
                }
            }
        },
        ""subnets"": {
            ""bbf265c9-65ac-44c5-b013-a680b3a394cd.6ded3489-9836-535b-8dd3-432d8fa3c0c9"": {
                ""region"": ""us-west-2"",
                ""resource"": ""subnet"",
                ""value"": {
                    ""networkAclAssociation"": {
                        ""NetworkAclAssociationId"": ""aclassoc-04e0637e"",
                        ""NetworkAclId"": ""acl-261b2041"",
                        ""SubnetId"": ""subnet-2e902d67""
                    },
                    ""subnet"": {
                        ""AssignIpv6AddressOnCreation"": false,
                        ""AvailabilityZone"": ""us-west-2b"",
                        ""AvailableIpAddressCount"": 250,
                        ""CidrBlock"": ""10.0.2.0/24"",
                        ""DefaultForAz"": false,
                        ""Ipv6CidrBlockAssociationSet"": [],
                        ""MapPublicIpOnLaunch"": true,
                        ""State"": ""available"",
                        ""SubnetId"": ""subnet-2e902d67"",
                        ""Tags"": [
                            {
                                ""Key"": ""network"",
                                ""Value"": ""public""
                            },
                            {
                                ""Key"": ""Fugue ID"",
                                ""Value"": ""bbf265c9-65ac-44c5-b013-a680b3a394cd.6ded3489-9836-535b-8dd3-432d8fa3c0c9""
                            },
                            {
                                ""Key"": ""Name"",
                                ""Value"": ""Example Network-PUBLIC-SN-B""
                            }
                        ],
                        ""VpcId"": ""vpc-5d24693a""
                    },
                    ""timestamp"": 1490289636
                }
            },
            ""bbf265c9-65ac-44c5-b013-a680b3a394cd.76e508fc-5ccd-5893-8bad-d8faf65fd30a"": {
                ""region"": ""us-west-2"",
                ""resource"": ""subnet"",
                ""value"": {
                    ""networkAclAssociation"": {
                        ""NetworkAclAssociationId"": ""aclassoc-18e66562"",
                        ""NetworkAclId"": ""acl-261b2041"",
                        ""SubnetId"": ""subnet-e1aa3186""
                    },
                    ""subnet"": {
                        ""AssignIpv6AddressOnCreation"": false,
                        ""AvailabilityZone"": ""us-west-2a"",
                        ""AvailableIpAddressCount"": 251,
                        ""CidrBlock"": ""10.0.1.0/24"",
                        ""DefaultForAz"": false,
                        ""Ipv6CidrBlockAssociationSet"": [],
                        ""MapPublicIpOnLaunch"": true,
                        ""State"": ""available"",
                        ""SubnetId"": ""subnet-e1aa3186"",
                        ""Tags"": [
                            {
                                ""Key"": ""Name"",
                                ""Value"": ""Example Network-PUBLIC-SN-A""
                            },
                            {
                                ""Key"": ""network"",
                                ""Value"": ""public""
                            },
                            {
                                ""Key"": ""Fugue ID"",
                                ""Value"": ""bbf265c9-65ac-44c5-b013-a680b3a394cd.76e508fc-5ccd-5893-8bad-d8faf65fd30a""
                            }
                        ],
                        ""VpcId"": ""vpc-5d24693a""
                    },
                    ""timestamp"": 1490289636
                }
            }
        },
        ""vpcs"": {
            ""bbf265c9-65ac-44c5-b013-a680b3a394cd.a4c7c637-fa6d-5eaa-bc7b-44d6a999d9b4"": {
                ""region"": ""us-west-2"",
                ""resource"": ""vpc"",
                ""value"": {
                    ""defaultNetworkAcl"": ""acl-261b2041"",
                    ""defaultRouteTable"": ""rtb-2db75d4b"",
                    ""defaultSecurityGroup"": ""sg-29e1fa51"",
                    ""dnsAttributes"": {
                        ""EnableDnsHostnames"": {
                            ""Value"": false
                        },
                        ""EnableDnsSupport"": {
                            ""Value"": true
                        },
                        ""VpcId"": ""vpc-5d24693a""
                    },
                    ""timestamp"": 1490289636,
                    ""vpc"": {
                        ""CidrBlock"": ""10.0.0.0/16"",
                        ""DhcpOptionsId"": ""dopt-c1a706a6"",
                        ""InstanceTenancy"": ""default"",
                        ""IsDefault"": false,
                        ""State"": ""available"",
                        ""Tags"": [
                            {
                                ""Key"": ""Fugue ID"",
                                ""Value"": ""bbf265c9-65ac-44c5-b013-a680b3a394cd.a4c7c637-fa6d-5eaa-bc7b-44d6a999d9b4""
                            },
                            {
                                ""Key"": ""Name"",
                                ""Value"": ""Example Network""
                            }
                        ],
                        ""VpcId"": ""vpc-5d24693a""
                    }
                }
            }
        }
    },
    ""state"": ""BUSY"",
    ""updated"": 1490288267
}
{code}",,chrisg,erez,tyler,,,,,,,,,,,,23/Mar/17 1:23 PM;erez;cw_scaling.lw;https://luminal.atlassian.net/secure/attachment/27400/cw_scaling.lw,23/Mar/17 1:28 PM;erez;fugue_report-2017-03-23-17-15-11.zip;https://luminal.atlassian.net/secure/attachment/27401/fugue_report-2017-03-23-17-15-11.zip,,,,,,,,,,,,,,,{},,,,,2.0,,,LRT Team,,,,2|hzytuu:000000r0000000011zzzzzzi,,,Sprint 29,,,,,2.0,,,Not started,2017-04-07 09:52:42.542,23/Mar/17 1:24 PM;erez;I should note that if you give the autoscaling policy an explicit cooldown value it works fine,07/Apr/17 9:52 AM;chrisg;On an unrelated note: the use of the {{Foo.default with}} pattern is deprecated and should be removed from our tests.,"07/Apr/17 9:56 AM;chrisg;I'm estimating this as a 2.

We have logs and way to reproduce it, so it should be a matter of confirming the documented AWS behavior is correct and if so handling it in the planner. Historically these things can be knocked out in less than a day and since we have already have a test for this I don't think there's a ton of QA time that would need to go into this (feel free to bump up the estimate if that's incorrect).",02/May/17 12:19 PM;tyler;https://github.com/LuminalHQ/emitInstructions-go/pull/500,,,,,,,,,,,,
Make region a required flag for fugue init,FUGUE-5085,32406,,Task,Done,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,matt,wayne,wayne,28/Mar/17 10:47 AM,01/May/17 12:05 PM,09/May/17 6:48 PM,,,,CLI,,0,EndOf28,,,,"If region is not specified: 

{code:java}
± fugue init ami-77b30361
[ ERROR ] Missing required option: region.

Usage: fugue init [options] <ami_id>

  Initialize a Fugue Project with the <ami_id> of the Conductor to install.

Options:
  -p, --profile name  Specify the AWS Credential Profile name to use for Fugue
                      commands.  [default: default]
  --region region     Specify a region to run Fugue.  [required]
  --creds-check       Check that the configured credentials have sufficient
                      permissions.
  -h, --help          Show this message and exit.
{code}

If region is specified:

{code:java}
± fugue init ami-77b30361 --region us-west-2
[ fugue init ] Initializing Fugue project with the following configuration:

Fugue Conductor AMI ID: ami-77b30361
AWS Credentials: Profile (default)
Region: us-west-2

Validating Fugue Conductor AMI ID ...
[ OK ] Provided AMI ID is valid.

Creating new fugue.yaml file ...

[ Done ] Project initialized.
{code}
",,alex,matt,wayne,,,,,,,,,QA-328,PDOC-559,FUGUE-4490,,,,,,,,,,,,,,,,,{},,FUGUE-2114,,,2.0,,,CloudOS Team,,,,2|hzytuu:000000r000000000v,,,Sprint 28,Sprint 29,,,,2.0,,,Not started,2017-03-29 12:28:28.869,29/Mar/17 12:28 PM;alex;we might be able to take either a region _or_ an AMI once we have the CLI automatically detecting AMIs,17/Apr/17 5:02 PM;wayne;Sorry for all the back and forth on the history. I mistakenly thought this was for the fugue-init component on the conductor.,01/May/17 9:46 AM;matt;fugue-client: 0.32.1,,,,,,,,,,,,,
Add lwserver hovering for non-bindings,FUGUE-5087,32408,,Task,To Do,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,,jasper,jasper,28/Mar/17 11:32 AM,01/May/17 9:30 AM,09/May/17 6:48 PM,,,,Ludwig Language & Compiler,,0,EndOf28,ludwig_editor_support,Sprint27Spillover,,"We should support hovering for things that are not bindings:

In these cases I would just show a simple message e.g. ""this is what is exported from the module...""

* Export lists
* Import lists
* The composition keyword

In this case I would try to show at least the type the validation validates:

* Validations

Here we can include more info, e.g. if I hover over a subtype I can see its definition:

* Types",,jasper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,FUGUE-5064,,,,,,Toolchain Team,,,,2|i00792:,,,Sprint 27,Sprint 28,Sprint 29,,,5.0,,,Not started,,,,,,,,,,,,,,,,,
Bugfix: Can't validate the Name tag on DHCPOptions while using the Network Pattern.,FUGUE-5102,32611,,Task,Done,FUGUE,Fugue Product,software,nate,Fugue Product Project,,Critical,,tyler,erez,erez,02/Apr/17 9:16 PM,04/May/17 2:45 PM,09/May/17 6:48 PM,,,,,,0,sprint29,,,,"Marked as critical because this is a bug on a POC in development for an important potential client.
The project is here
https://github.com/LuminalHQ/poc-fidelity/tree/master


{code}
$ ENVIRONMENT=SANDBOX make
lwc --warnings --werror compositions/CreateArtifactRepo.lw > /dev/null
lwc --warnings --werror compositions/CreateDeveloperEnvironment.lw > /dev/null
ludwig (evaluation error):
  ""/opt/fugue/lib/Fugue/AWS/EC2/DhcpOptions.lw"" (line 60, column 3):
  error:

    60|   EC2.DhcpOptions{
    61|     tags: tags,
    62|     domainNameServers: domainNameServers,
    63|     domainName: domainName,
    64|     ntpServers: ntpServers,
    65|     netBiosNameServers: netBiosNameServers,
    66|     netBiosNodeType: netBiosNodeType,
    67|     region: region
    68|   }

  Failed validations:
    - All taggable resources must have the following tags:
      - Name
   (from Fidelity.InfoSecStandards.requireStandardTagsOnEC2DhcpOptions)

  Stack trace:
    In call to new at ""/opt/fugue/lib/Fugue/AWS/EC2/DhcpOptions.lw"" (line 157, column 3)
    In call to defaultForRegion at ""/opt/fugue/lib/Fugue/AWS/EC2/Vpc.lw"" (line 94, column 34)
    In call to new at ""/opt/fugue/lib/Fugue/AWS/Pattern/Network.lw"" (line 83, column 12)
    In call to new at ""compositions/CreateDeveloperEnvironment.lw"" (line 9, column 13)
make: *** [compositions/CreateDeveloperEnvironment.lw] Error 1
{code}",,alex,chrisg,erez,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,1.0,,,LRT Team,,,,2|hzytuu:000000r0000000011zzzzzxv,,,Sprint 29,,,,,0.5,,,Not started,2017-04-03 13:16:12.064,"03/Apr/17 1:16 PM;chrisg;There are a couple of problems here:

1. If {{Fugue.AWS.EC2.Vpc.new}} is not passed a {{DhcpOptions}} value it will create a ""default"" value using {{Fugue.AWS.EC2.DhcpOptions.defaultForRegion()}}. The resulting {{DhcpOptions}} value is created with the proper defaults for the region, but is not assigned any tags. A potential fix here is to pass the {{tags}} supplied to {{Vpc.new}} along with the {{region}} so that the defaults are tagged. This sounds reasonable, but may need a bit more thought to make sure this is a sane default (we also need to make sure that we have proper tag enforcement for {{DhcpOptions}} to ensure this doesn't cause destructive updates).

2. Explicitly passing a {{DhcpOptions}} value with the proper tags to {{EC2.Vpc.new}} should work around the issue, but due to strict evaluation of {{Optional.unpack}} it still generates the untagged ""default"" {{DhcpOptions}} value, which fails the validaiton (even though the resulting defaul value is *not* included in the NodeStream). This can be fixed by lazily creating the default {{DhcpOptions}}, which boils down to replacing this line:

{code}
    dhcpOptions: DhcpOptions.defaultForRegion(region), dhcpOptions),
{code}

with something like this:

{code}
    dhcpOptions: case dhcpOptions of
                   | None -> Optional(DhcpOptions.defaultForRegion(region))
                   | _    -> dhcpOptions
{code}

Which only evaluates the {{DhcpOptions.defaultForRegion(region)}} when {{dhcpOptions}} is {{None}}.
",03/Apr/17 5:15 PM;chrisg;Fix for #2: https://github.com/LuminalHQ/fugue-aws/pull/367,04/Apr/17 10:22 AM;chrisg;[~alex] do you have any thoughts on #1?,"18/Apr/17 12:12 PM;alex;#1 sounds reasonable. If you use Vpc.new and pass in a set of tags, then the child resources inherit them. Is that what you mean? If so, that makes sense to me.","24/Apr/17 2:34 PM;chrisg;Yep, that's it exactly.

BTW the fix for #2 is included in the fugue-aws artifacts for sprint 28.",,,,,,,,,,,
Bugfix: update_auto_scaling_group instruction issued every job,FUGUE-5104,32709,,Task,Done,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,tyler,tyler,tyler,03/Apr/17 12:12 PM,04/May/17 2:45 PM,09/May/17 6:48 PM,,,,,,0,sprint29,,,,"It looks like the autoscaling group planner is issuing an {{update_auto_scaling_group}} instruction every job in FugueDemo.lw.

The resulting job is:
{noformat}
{
    ""timestamp"": ""2017-03-31T22:32:55.483"",
    ""pid"": 1414,
    ""thread"": ""MainThread"",
    ""component"": ""run_worker"",
    ""log_level"": ""DEBUG"",
    ""message"": ""Processing request of Type [aws.autoscaling.update_auto_scaling_group]"",
    ""params"": {
        ""AutoScalingGroupName"": ""0c08a73f-f60c-4ca7-b766-9f5919aa49e3.237b3b54-2332-51e9-bd32-e9d9c1077f4c"",
        ""VPCZoneIdentifier"": ""subnet-57214c30,subnet-5b199d12""
    },
    ""fid"": ""0c08a73f-f60c-4ca7-b766-9f5919aa49e3"",
    ""request_type"": [
        ""aws"",
        ""autoscaling"",
        ""update_auto_scaling_group""
    ],
    ""guid_map"": ""{}"",
    ""guid"": ""0c08a73f-f60c-4ca7-b766-9f5919aa49e3.237b3b54-2332-51e9-bd32-e9d9c1077f4c.update"",
    ""account_id"": ""fugue-1490996106052"",
    ""job_id"": 1490999570,
    ""region"": ""us-west-2""
}
{noformat}

This has the unfortunate side effect of causing errors to appear in {{fugue status}} when an ASG has been externally deleted.

{noformat}
(3.5.1/envs/cli)  ~/conductor/personal ⮀ fugue history demo

Jobs History Report for tyler/102611952903 - Mon Apr 3 2017 12:00pm
demo (0c08a73f-f60c-4ca7-b766-9f5919aa49e3)

Row Id      Started     Last Updated    Job Type    Jobs Ran    Job Status    Description
----------  ----------  --------------  ----------  ----------  ------------  ---------------
1491235241  12:00pm                     SYSTEM      1
1491235211  11:59am     12:00pm         SYSTEM      3           FAILED        ValidationError
1491235121  11:56am     11:58am         SYSTEM      6           SUCCEEDED     JobSucceeded
1491234941  11:55am     11:55am         RESUME      1           SUCCEEDED     JobSucceeded
1491034525  Sat 4:15am  Sat 4:25am      SYSTEM      1           HALTED        JobTimedOut
1491034495  Sat 3:59am  Sat 4:14am      SYSTEM      32          SUCCEEDED     JobSucceeded
1491033534  Fri 5:46pm  Sat 3:59am      SYSTEM      1199        SUCCEEDED     JobSucceeded
1490996717  Fri 5:45pm  Fri 5:46pm      RUN         1           SUCCEEDED     JobSucceeded

(3.5.1/envs/cli)  ~/conductor/personal ⮀ fugue status demo | jq .last_message
""An error occurred (ValidationError) when calling the UpdateAutoScalingGroup operation: AutoScalingGroup name not found - null""
(3.5.1/envs/cli)  ~/conductor/personal ⮀ fugue history demo

Jobs History Report for tyler/102611952903 - Mon Apr 3 2017 12:13pm
demo (0c08a73f-f60c-4ca7-b766-9f5919aa49e3)

Row Id      Started     Last Updated    Job Type    Jobs Ran    Job Status    Description
----------  ----------  --------------  ----------  ----------  ------------  ---------------
1491235992  12:13pm                     SYSTEM      1
1491235962  12:10pm     12:12pm         SYSTEM      5           SUCCEEDED     JobSucceeded
1491235812  12:00pm     12:10pm         SYSTEM      20          FAILED        ValidationError
1491235211  11:59am     12:00pm         SYSTEM      3           FAILED        ValidationError
1491235121  11:56am     11:58am         SYSTEM      6           SUCCEEDED     JobSucceeded
1491234941  11:55am     11:55am         RESUME      1           SUCCEEDED     JobSucceeded
1491034525  Sat 4:15am  Sat 4:25am      SYSTEM      1           HALTED        JobTimedOut
1491034495  Sat 3:59am  Sat 4:14am      SYSTEM      32          SUCCEEDED     JobSucceeded
1491033534  Fri 5:46pm  Sat 3:59am      SYSTEM      1199        SUCCEEDED     JobSucceeded
1490996717  Fri 5:45pm  Fri 5:46pm      RUN         1           SUCCEEDED     JobSucceeded

(3.5.1/envs/cli)  ~/conductor/personal ⮀
{noformat}

Can reproduce locally:
{noformat}
⮀ ./emitInstructions-go --cli --plan-in emit-instructions.pd --plan-out out.pd --fid 0c08a73f-f60c-4ca7-b766-9f5919aa49e3 && cat out.pd | protoc --decode_raw
{""component"":""vars"",""log_level"":""info"",""message"":""Processing planner document /Users/tyler/Downloads/emit-instructions.pd"",""timestamp"":""2017-04-03T16:29:43.716823""}
{""component"":""vars"",""fid"":""0c08a73f-f60c-4ca7-b766-9f5919aa49e3"",""job_id"":""defaultjobid"",""log_level"":""info"",""message"":""START: Processing plan"",""timestamp"":""2017-04-03T16:29:43.716888""}
{""component"":""vars"",""fid"":""0c08a73f-f60c-4ca7-b766-9f5919aa49e3"",""job_id"":""defaultjobid"",""log_level"":""info"",""message"":""SUCCESS: Processing plan"",""timestamp"":""2017-04-03T16:29:43.730745""}
{""component"":""vars"",""log_level"":""info"",""message"":""Executed planner"",""timestamp"":""2017-04-03T16:29:43.730778""}
{""component"":""vars"",""log_level"":""info"",""message"":""Wrote plan to out.pd"",""timestamp"":""2017-04-03T16:29:43.730948""}
1: ""0c08a73f-f60c-4ca7-b766-9f5919aa49e3""
2: ""1490999570""
3 {
  1: ""0c08a73f-f60c-4ca7-b766-9f5919aa49e3.237b3b54-2332-51e9-bd32-e9d9c1077f4c.update""
  3: ""us-west-2""
  4: ""aws.autoscaling.update_auto_scaling_group""
  5: ""{\""AutoScalingGroupName\"":\""0c08a73f-f60c-4ca7-b766-9f5919aa49e3.237b3b54-2332-51e9-bd32-e9d9c1077f4c\"",\""VPCZoneIdentifier\"":\""subnet-57214c30,subnet-5b199d12\""}""
}
4: ""fugue-1490996106052""
{noformat}
Be sure to include the matching FID, otherwise a bunch of create and delete instructions will come up for tags and various other resources.",,alex,tyler,,,,,,,,,,,,,03/Apr/17 12:31 PM;tyler;emit-instructions.pd;https://luminal.atlassian.net/secure/attachment/27808/emit-instructions.pd,,,,,,,,,,,,,,,,{},,,,,2.0,,,LRT Team,,,,2|hzytuu:000000r0000000011zzzzzzy,,,Sprint 29,,,,,3.0,,,Not started,2017-04-20 11:34:35.626,20/Apr/17 11:34 AM;alex;This will cause notification spam once notifications ships.,02/May/17 2:36 PM;tyler;https://github.com/LuminalHQ/emitInstructions-go/pull/501,,,,,,,,,,,,,,
Validator gets lists of running processes/compositions,FUGUE-5127,32904,,Task,In Progress,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,joshe,tyler,tyler,04/Apr/17 3:54 PM,04/May/17 2:51 PM,09/May/17 6:48 PM,,,r28,,,0,,,,,"When new validations are uploaded, the validator must compare it against all compositions for existing processes.

We will need a scheduler client that returns process information for all existing processes (in any state). This can be a generic client, but the most important part that is needed is the snapshot location.

The list of snapshots just needs to be provided in the validator and everything else should be good to go!

Validator: https://github.com/LuminalHQ/fugue-ludwig-validator/blob/4896f60706d6681a710ad959847b9f110c55cf64/validation/post.go#L76",,tyler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,FUGUE-3225,,,,,,LRT Team,,,,2|hzytuu:000000r000000000000m,,,Sprint 29,,,,,5.0,,,Not started,,,,,,,,,,,,,,,,,
Test and integrate validator CLI components,FUGUE-5128,32905,,Task,In Progress,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,tyler,tyler,tyler,04/Apr/17 3:57 PM,04/May/17 4:15 PM,09/May/17 6:48 PM,,,r28,,,0,,,,,"PRs for the validator CLI components exist:
https://github.com/LuminalHQ/fugue-cli-python/pull/357
https://github.com/LuminalHQ/fugue-cli-python/pull/358
https://github.com/LuminalHQ/fugue-cli-python/pull/359
https://github.com/LuminalHQ/fugue-cli-python/pull/360
https://github.com/LuminalHQ/fugue-cli-python/pull/363

Need to confirm that these work and integrate them.

Summary of commands and their overall action are below. Please refer to the library reference: https://github.com/LuminalHQ/fugue-ludwig-validator-client-python

h2. Create

Takes a Ludwig file that is a validation and compiles it using the {{-s snapshot}} backend, writes the file to S3, and sends a message to demarc to create the validation.

FUGUE-3269

h2. Get

Get takes a validation name and local location to store the validation (a {{.tar.gz}}). The CLI should also generate a random S3 URI where a copy of the snapshot should be stored. The CLI should download the file from S3 to the desired location. Upon finishing the download, the CLI should delete the validation copy from S3.

FUGUE-4714

h2. List

List takes in a prefix to filter validations. The CLI should display a list of validations. The metadata contained in the list is pretty old and is worth revisiting from a UI/UX perspective.

FUGUE-3383

h2. Delete

Delete takes a validation name and deletes it.

FUGUE-3270

h2. Validate Compositions

This is already handled through {{fugue run}}.

h2. Existing commands

Be sure to check the user stories for {{fugue run}} and {{fugue update}}.

FUGUE-3275
FUGUE-3277",,tyler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,FUGUE-3225,,,,,,LRT Team,,,,2|i003kq:6hzg,,,Sprint 29,,,,,8.0,,,Not started,,,,,,,,,,,,,,,,,
Create scheduler client for getting processes in Go,FUGUE-5139,32917,32904,Sub-task,In Progress,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,joshe,tyler,tyler,04/Apr/17 4:54 PM,04/May/17 2:16 PM,09/May/17 6:48 PM,,,,,,0,,,,,,,tyler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,LRT Team,,,,2|i008fa:,,,Sprint 29,,,,,,,,Not started,,,,,,,,,,,,,,,,,
Integrate scheduler client into the validator,FUGUE-5140,32918,32904,Sub-task,To Do,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,,tyler,tyler,04/Apr/17 4:55 PM,04/Apr/17 4:55 PM,09/May/17 6:48 PM,,,,,,0,,,,,,,tyler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,2|i008fi:,,,Sprint 29,,,,,,,,Not started,,,,,,,,,,,,,,,,,
Implement the remaining parts of RFC-0003,FUGUE-5285,33123,,Task,To Do,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,,jasper,jasper,06/Apr/17 12:13 PM,28/Apr/17 1:12 PM,09/May/17 6:48 PM,,,,Ludwig Language & Compiler,,0,ludwig_language_design,,,,"We already represent tuples as records in the compiler, but we don't yet allow polymorphic accessors such as {{_2}}.",,jasper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,Toolchain Team,,,,2|hzytuu:000000r0000000011zzzzzzzv,,,Sprint 29,,,,,5.0,,,Not started,,,,,,,,,,,,,,,,,
Fix pretty-printing of values,FUGUE-5288,33200,,Task,To Do,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,brett,jasper,jasper,07/Apr/17 7:12 AM,08/May/17 11:19 AM,09/May/17 6:48 PM,,,,Ludwig Language & Compiler,,0,ludwig_error_messages,,,,"Right now, we are sometimes leaking internal info by printing values using {{Show}}, e.g.:

{noformat}
  Couldn't match: Val {unVal = ListV [Val {unVal = SumConV (SumCon {scStableName = Just (Name {nScope = ToplevelScope, nModuleName = ModuleName {unModuleName = []}, nIsGenerated = True, nOccName = OccName {onNamespace = VarNamespace, onModuleAlias = ModuleAlias {unModuleAlias = ModuleName {unModuleName = []}}, onKind = VarName, onUnqualified = ""$heap$suite$AppCookieStickiness$0""}}), scMetaName = Nothing, scConstructor = Name {nScope = ToplevelScope, nModuleName = ModuleName {unModuleName = [""Fugue"",""Core"",""AWS"",""ELB""]}, nIsGenerated = False, nOccName = OccName {onNamespace = VarNamespace, onModuleAlias = ModuleAlias {unModuleAlias = ModuleName {unModuleName = []}}, onKind = ConName, onUnqualified = ""AppCookieStickiness""}}, scField = Just (Val {unVal = StructV Nothing (fromList [(Key {unKey = ""appCookieName""},Val {unVal = StringV ""sticky-business""})])}), scTypeName = Name {nScope = ToplevelScope, nModuleName = ModuleName {unModuleName = [""Fugue"",""Core"",""AWS"",""ELB""]}, nIsGenerated = False, nOccName = OccName {onNamespace = TypeNamespace, onModuleAlias = ModuleAlias {unModuleAlias = ModuleName {unModuleName = []}}, onKind = TypeName, onUnqualified = ""LoadBalancerPolicy""}}, scStackTrace = [PrimitiveFrame (Name {nScope = ToplevelScope, nModuleName = ModuleName {unModuleName = [""Fugue"",""Core"",""AWS"",""ELB""]}, nIsGenerated = False, nOccName = OccName {onNamespace = VarNamespace, onModuleAlias = ModuleAlias {unModuleAlias = ModuleName {unModuleName = []}}, onKind = ConName, onUnqualified = ""AppCookieStickiness""}}) (Metadata {metaSnippet = Snippet {sModulePointer = ModulePointer {mpName_ = ModuleName {unModuleName = []}, mpPrettyPath_ = ""tests/unit/TestListener.lw"", mpCanonicalPath_ = ""/Users/chris/src/fugue/fugue-aws/tests/unit/TestListener.lw""}, sStart = Position {pLine = 21, pColumn = 7}, sEnd = Position {pLine = 21, pColumn = 64}}, metaAnnotations = []}),FunctionCallFrame (Var {unVar = Name {nScope = ToplevelScope, nModuleName = ModuleName {unModuleName = []}, nIsGenerated = False, nOccName = OccName {onNamespace = VarNamespace, onModuleAlias = ModuleAlias {unModuleAlias = ModuleName {unModuleName = []}}, onKind = VarName, onUnqualified = ""testDefaultSecurityPolicy""}}}) (Metadata {metaSnippet = Snippet {sModulePointer = ModulePointer {mpName_ = ModuleName {unModuleName = []}, mpPrettyPath_ = ""tests/unit/TestListener.lw"", mpCanonicalPath_ = ""/Users/chris/src/fugue/fugue-aws/tests/unit/TestListener.lw""}, sStart = Position {pLine = 29, pColumn = 3}, sEnd = Position {pLine = 29, pColumn = 29}}, metaAnnotations = []})]})},Val {unVal = RefV (Name {nScope = ToplevelScope, nModuleName = ModuleName {unModuleName = []}, nIsGenerated = True, nOccName = OccName {onNamespace = VarNamespace, onModuleAlias = ModuleAlias {unModuleAlias = ModuleName {unModuleName = []}}, onKind = VarName, onUnqualified = ""$heap$178""}})}]}
{noformat}

We should avoid this by removing the {{Show}} instance and consistently using {{Pretty}} everywhere.

This ticket also includes fixing pretty-printing of {{Val}} so that the output is valid (and readable) Ludwig.  That will allow us to have a {{-s normalize}} backend which prints the evaluated, normalized Ludwig.",,jasper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,Toolchain Team,,,,2|hzytuu:000000r0000000011zzzzv,,,Sprint 29,,,,,3.0,,,Not started,,,,,,,,,,,,,,,,,
Bugfix: Vars System Not In Conductor Stats,FUGUE-5291,33203,,Task,To Do,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,,eric,eric,07/Apr/17 9:51 AM,01/May/17 9:43 AM,09/May/17 6:48 PM,,,,,,0,sprint29,,,,"When we started running multiple vars daemons on the conductor this broke how process metrics were collected for Vars. Conductor stats knows to look for a process named vars, but only selects one to report on (lowest pid). Which means that one of the two daemons aren't being reported. We need to update conductor stats to be able to handle this situation so we can get the metrics we need.",,eric,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,CloudOS Team,,,,2|hzytuu:000000r0000000011zzzzzzzx,,,Sprint 29,,,,,5.0,,,Not started,,"10/Apr/17 10:01 AM;eric;to get this to work properly, we may need to do a bit of work to conductor stats, going to conservatively estimate this at a 5",,,,,,,,,,,,,,,
High level interface for managing ECS tasks on a cluster,FUGUE-5292,33204,32204,Sub-task,In Progress,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,,chrisg,chrisg,07/Apr/17 10:22 AM,03/May/17 4:24 PM,09/May/17 6:48 PM,,,,,,0,,,,,Build a fugue-aws style interface to managing ECS tasks on a cluster (managing clusters themselves will be handled on another issue). This should include support for integration with ELB and ECR (if possible).,,chrisg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,2|i009im:,,,Sprint 28,Sprint 29,,,,,,,Not started,,,,,,,,,,,,,,,,,
Pattern library for provisioning ECS clusters,FUGUE-5294,33206,32204,Sub-task,To Do,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,,chrisg,chrisg,07/Apr/17 10:24 AM,25/Apr/17 11:02 AM,09/May/17 6:48 PM,,,,,,0,,,,,"Write a {{Fugue.AWS.Pattern.Network}} style library for provisioning ECS clusters. Should be flexible enough to be generally useful, but is also free to be opiniotated where it simplifies things.

Note that use of ECS support should not require the use of this library, it's more a convenience for use with our own testing and in examples. User's should be free to provision a cluster in whatever manner best suits their needs.",,chrisg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,2|i009iu:,,,Sprint 28,Sprint 29,,,,,,,Not started,,25/Apr/17 11:02 AM;chrisg;Include a pre-defined versions of the minimal ECS container instance role and policy for convienence: http://docs.aws.amazon.com/AmazonECS/latest/developerguide/instance_IAM_role.html,,,,,,,,,,,,,,,
Help integrate Scheduler client changes into demarc,FUGUE-5297,33209,30611,Sub-task,To Do,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,,joshe,joshe,07/Apr/17 10:48 AM,10/Apr/17 11:33 AM,09/May/17 6:48 PM,,,,,,0,,,,,Demarc will need to pass the new enforcement configuration command from the CLI to the Scheduler via the Scheduler client.,,joshe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,LRT Team,,,,2|i009ji:,,,Sprint 29,,,,,,,,Not started,,,,,,,,,,,,,,,,,
Scheduler to pass Job Type and Enforcement Level in outgoing Jobs,FUGUE-5299,33212,30611,Sub-task,To Do,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,,joshe,joshe,07/Apr/17 10:59 AM,10/Apr/17 11:35 AM,09/May/17 6:48 PM,,,,Scheduler,,0,,,,,"The Job Type as maintained by the Scheduler should be included in outgoing Jobs (for eventual consumption by the Broker), in addition to the new field ""enforcement level""",,joshe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,LRT Team,,,,2|i009k6:,,,Sprint 29,,,,,,,,Not started,,"07/Apr/17 11:05 AM;joshe;After the manager client is updated, add {{data.job_type}} to [this method call|https://github.com/LuminalHQ/fugue-scheduler/blob/develop/scheduler/jobs.py#L221-L222]",,,,,,,,,,,,,,,
Manager to accept and pass Job Type and Enforcement Level,FUGUE-5300,33213,30611,Sub-task,To Do,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,,joshe,joshe,07/Apr/17 11:00 AM,10/Apr/17 11:36 AM,09/May/17 6:48 PM,,,,Planners,,0,,,,,"Job Type and Enforcement Level will be added to all Scheduler Jobs, and needs to be read and included in outgoing Plans to the Broker.

This includes updating the python manager client, the manager's incoming Job struct, and the outgoing Plan struct.",,joshe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,LRT Team,,,,2|i009ke:,,,Sprint 29,,,,,,,,Not started,,,,,,,,,,,,,,,,,
Bugfix: Fugue kill errors,FUGUE-5312,33228,,Task,Cannot Reproduce,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,danthemyth,ankush,ankush,07/Apr/17 7:23 PM,03/May/17 9:58 AM,09/May/17 6:48 PM,,,,,,0,sprint29,,,,"Running into 2 issues with fugue kill for 2 processes that have run into error state.  webapp1 error was due to a bug reported by Chris C in https://luminal.atlassian.net/browse/FUGUE-5310

fugue kill for webapp1 does not kill the process. It has been in that state since 1 PM EST. 

fugue kill PAN1 does not find the process even though `fugue status` returns the process name. I also cannot create another process with that name.

ami: ami-fe9b5ce8
Fugue CLI Version: 1.0.3-2016-506a18a76db7c4377577292e89ccdc6344a72eca
VARs Version: 1.0.0-1983-38519039e173a5683206032d44ae769956fb9f81
LWDoc Version: 0.19.0
LWC Version: 0.19.0 

897926356418/fugue_report-2017-04-07-23-12-00.zip",,ankush,danthemyth,wayne,,,,,,,,,,,,10/Apr/17 10:22 AM;ankush;2017-04-10_1022.png;https://luminal.atlassian.net/secure/attachment/28200/2017-04-10_1022.png,07/Apr/17 7:12 PM;ankush;Fugue Kill Error.png;https://luminal.atlassian.net/secure/attachment/28105/Fugue+Kill+Error.png,02/May/17 3:04 PM;ankush;MyWebApp.lw;https://luminal.atlassian.net/secure/attachment/28815/MyWebApp.lw,02/May/17 2:40 PM;ankush;PAN_Plain.lw;https://luminal.atlassian.net/secure/attachment/28814/PAN_Plain.lw,,,,,,,,,,,,,{},,,,,1.0,1.0.3-2016-506a18a76db7c4377577292e89ccdc6344a72eca,,LRT Team,,,,2|hzytuu:000000r00000000009,,,Sprint 29,,,,,5.0,,,Not started,2017-04-10 10:48:25.478,07/Apr/17 7:36 PM;ankush;PAN1 process finally ended itself after an hour,"10/Apr/17 10:37 AM;ankush;Created a new process Friday evening and then executed the Kill command for it Friday night at 10:41. Checked the status and the process was still in ""Killing"" state. 2017-04-10_1022.png is the error.

I had to delete the SG for the VPC manually and then the Conductor went in and deleted the VPC and finally the process.  

Updated Log file from past 1 hour 897926356418/fugue_report-2017-04-10-14-24-51.zip ","10/Apr/17 10:48 AM;wayne;First thing that came to mind when I saw DependencyViolation and ""had to manually delete security group"" was this ticket FUGUE-4740 where I had to manually terminate an instance in order for the conductor to finish tearing down a process because the auto scaling group was stuck in ""(Deleting...)"". We may have another situation in which the conductor issues a delete command for a piece of infrastructure but AWS does not successfully perform a delete and the conductor does not try again. I called this a ""blind spot"" issue. ","10/Apr/17 10:49 AM;wayne;The Fugue Kill error png from a couple days ago points to invalid parameters for an aurora call.

Ankush is using the current public release.

Assigning to LRT.","02/May/17 4:25 PM;danthemyth;Just some notes:

PAN1 from screenshots, FID dc18d3ef-7747-4a06-8768-f461c9c4a5f does not appear in logs. The first screen shot shows an error regarding invalid parameter combination which is expected. The second screenshot shows failure to delete vpc because of a lingering security group. Without logs - it's difficult to pin down what exactly is blocking the SG. Ankush uploaded PAN_Plain.lw for the alias PAN1. Can't repo on current artifacts. As Wayne mentioned, there's no ASG so it's not an exact fit for FUGUE-4740.

webapp1, FID 36332fb2-8cb7-45f4-8eca-e9eab8b02eec - note from description links it to FUGUE-5310 but Ankush's MyWebApp1.lw composition doesn't have print statements currently. I was able to run & kill using current artifacts without issue.

","02/May/17 4:31 PM;ankush;Dan, are there more log files needed besides the one I uploaded (897926356418/fugue_report-2017-04-07-23-12-00.zip)? ",02/May/17 4:48 PM;danthemyth;Ankush pointed out there's a second set of logs in a comment which does contain dc18d3ef-7747-4a06-8768-f461cf9c4a5f (PAN1).,"02/May/17 5:22 PM;danthemyth;Unfortunately that second set of logs only covers a period of time where the dependencyviolation error is currently happening and not calls leading up to it. Logs that began at the composition start time, 1491608399 (Fri Apr  7 19:39:59 EDT 2017), might reveal more (almost 3 days of logs - wowee).",,,,,,,,
CLI command to let a user disable enforcement for a process,FUGUE-5315,33301,30744,Sub-task,To Do,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,,eric,eric,10/Apr/17 10:37 AM,10/Apr/17 10:37 AM,09/May/17 6:48 PM,,,,,,0,,,,,"add a new valid process property called ""enforcement"" to the cli. It should be settable, gettable, and displayed in the process status if it is set.",,eric,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,CloudOS Team,,,,2|i009p2:,,,Sprint 29,,,,,,,,Not started,,,,,,,,,,,,,,,,,
"Ensure demarc core will support setting, getting, and status display of ""enforcement"" property of a process",FUGUE-5316,33302,30744,Sub-task,To Do,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,,eric,eric,10/Apr/17 10:38 AM,10/Apr/17 10:38 AM,09/May/17 6:48 PM,,,,,,0,,,,,,,eric,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,CloudOS Team,,,,2|i009pa:,,,Sprint 29,,,,,,,,Not started,,,,,,,,,,,,,,,,,
"Handle set, get, and status in demarc with new ""enforcement"" property",FUGUE-5317,33303,30744,Sub-task,To Do,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,,eric,eric,10/Apr/17 10:39 AM,10/Apr/17 10:39 AM,09/May/17 6:48 PM,,,,,,0,,,,,mock out calling the scheduler if work on this starts before scheduler/backend work has completed.,,eric,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,CloudOS Team,,,,2|i009pi:,,,Sprint 29,,,,,,,,Not started,,,,,,,,,,,,,,,,,
Coordinate with LRT and replace any mocking with real calls to the backend.,FUGUE-5318,33304,30744,Sub-task,To Do,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,,eric,eric,10/Apr/17 10:39 AM,10/Apr/17 10:39 AM,09/May/17 6:48 PM,,,,,,0,,,,,,,eric,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,CloudOS Team,,,,2|i009pq:,,,Sprint 29,,,,,,,,Not started,,,,,,,,,,,,,,,,,
Update Scheduler to accept and store Enforcement Level per process,FUGUE-5319,33305,30611,Sub-task,To Do,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,,joshe,joshe,10/Apr/17 11:42 AM,10/Apr/17 11:42 AM,09/May/17 6:48 PM,,,,Scheduler,,0,,,,,"1. Add set enforcement level to {{set_property}} Signal Handler
2. Update data definition for {{ProcessData}} to include new enforcement level field
3. Enforcement Level set to default level of ""on"" when a Process is created",,joshe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,LRT Team,,,,2|i009py:,,,Sprint 29,,,,,,,,Not started,,,,,,,,,,,,,,,,,
Update Scheduler client to accept new property,FUGUE-5320,33306,30611,Sub-task,To Do,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,,joshe,joshe,10/Apr/17 11:52 AM,10/Apr/17 11:52 AM,09/May/17 6:48 PM,,,,,,0,,,,,"Validation for {{Command.set_property}} should accept new Property for ""Enforcement Level""

Needs to be updated in Scheduler client and Scheduler server",,joshe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,LRT Team,,,,2|i009q6:,,,Sprint 29,,,,,,,,Not started,,,,,,,,,,,,,,,,,
Enable client side encryption for VARS daemon LVS-pro,FUGUE-5324,33310,,Task,In Progress,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,achintya,wayne,wayne,10/Apr/17 2:17 PM,09/May/17 4:53 PM,09/May/17 6:48 PM,,,,VARS,,0,EndOf28,,,,"In VARS v3, large value support for items that will be stored in s3 are encrypted with SSE and not with client side (VARS) encryption as in previous versions. This task is to:

- Re-enable client side encryption in VARS. This means ALL values sent to VARS with the encryption flag on will be encrypted by VARS
- If a value is going to be placed in S3 (meaning it's a large value) and has the encryption flag on, it should ALSO be encrypted by S3 with SSE
- Assess if any pyvars updates are required.

The goal here is to ensure all values where encryption is desired to be encrypted with VARS. Additionally, we want to enable the provider-supported encryption for data storage which gives more security around sensitive data. 
",,achintya,wayne,,,,,,,,FUGUE-4923,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,CloudOS Team,,,,2|hzytuu:000000r0000000005,,,Sprint 28,Sprint 29,,,,8.0,,,Not started,2017-04-26 10:30:24.609,10/Apr/17 2:20 PM;wayne;This also means we won't need to decrypt all existing LVS-lite values during any VARS migration scripts.,"26/Apr/17 10:30 AM;achintya;Status Update:
It is impossible to calculate the HMAC for a large value while streaming it. This is because we would have to calculate the HMAC over the whole file value and we'll have to hold the transmission of the large value to the caller while we verify the HMAC. All of this means that we have to switch the encryption scheme from CTR to GCM mode. We'd also have to switch out to the GCM mode for small values too. 
Once the switch of the mode is complete we'd have to update the migration script to decrypt using the old scheme and encrypt using the new scheme.","26/Apr/17 1:50 PM;achintya;It turns out that GCM mode is also not suitable for streaming encryption. http://stackoverflow.com/a/39380108
Since this is where we are the only viable recourse is that we split up the large value into smaller chunks which can be verified and streamed. This would work as follows:
1. The large value will be stored in parts. There would be a `HEADER` object. 
   a. This header object would be encrypted by vars and it's HMAC would be in vars' DDB table
   b. This header object would contain the list of parts of the large object and their HMACs as key-value pair. For ex:
       1:SDJFJSDHFKJHDFKJSDH
       2:OPOIPOIPOIPOIPOIQIL
       ...
   c. This header would be created during `vars put` and used to authenticate each chunk during `vars get`
   
2. Instead of sticking the whole file at the calculated hash we start writing the file in chunks (need to figure out the right size of this chunk but I'm thinking something around 20 MB).
   Once a chunk has been created the HEADER file is updated with the chunk number and HMAC
3. When a `vars get` request is received then the daemon goes to the folder and decrypts the HEADER file to get the info on chunks
4. The daemon then gets each chunk, authenticates, decrypts and streams it to the client in order. 
5. All of this will remain transparent to the client but would mean considerable bookkeeping at the daemon's end.","27/Apr/17 6:35 PM;achintya;Status update:
Got the basic chunking of large values working for upload. Will start on download tomorrow.","28/Apr/17 4:15 PM;achintya;[~nate], [~tyler], [~wayne] and [~achintya] met to discuss the sharding in S3 and decided on the following large value scheme:
https://luminal.atlassian.net/wiki/display/PE/Vars+Large+Value+Support+Scheme",01/May/17 2:43 PM;wayne;There's an open PR that will need to be updated for migration https://github.com/LuminalHQ/fugue-vars-cleaner/pull/5,"03/May/17 1:02 AM;achintya;Status update:
Finished with the deletion utility for S3. We now have rollback for failed uploads. Also wrote tests for the same. The deletion part is needed for when uploads fail due to size or when there's a consistency issue (CAS updates). We'll also need the deletion for vacuuming. 
Things pending for `vars put` for large uploads are:
1. Checking for version updates during CAS updates for large value.
2. Writing tests for encryption pipeline. I spent some time figuring out how to do this but couldn't finish it. Might have to refactor code for mocking encryption pipeline (wrapped keys etc)","03/May/17 1:06 AM;achintya;Also, [~chrisg] [~eric] and [~achintya] met to discuss the consistency model for large value. The confluence document https://luminal.atlassian.net/wiki/display/PE/Vars+Large+Value+Support+Scheme has been updated with the outcome of the meeting. This information is under (4) (d) in the document mentioned above.","06/May/17 12:39 AM;achintya;Status update:
Finished with the download and upload. Wrote tests for each. Ran into a minor issue. I was storing the IV at the beginning of the stream so that would go into the first chunk. This would need to change since the IV being in the encrypted data would cause the HMAC to fail. I'm going to store the IV in the header instead.  ","09/May/17 4:53 PM;achintya;Status update:
I found a bug in the design of the backend so fixing that right now. The problem is that we were updating the status of large value operation and would dump stuff like ""uploading"" and ""hashing"" in the DDB table while we were uploading the large value to S3. This caused the encryption to fail since this would throw the version of the item in DDB out of sync (since the publishing of the status would change the versioning). Took me whole day to figure this out. Was trying to upload and download an ubuntu disk image with encryption and that's where this occured. Would be removing the update-status thing now.
PS: This update-status thing was to be deleted anyway with the new design. So this does not impact the final outcome. We could have kept it if weren't doing encryption on our end too but with the new encryption scheme coming in we have to remove the status updater.",,,,,,
Create a job to perform a nightly develop artifacts,FUGUE-5325,33312,,Story,Done,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,danthemyth,tyler,tyler,10/Apr/17 5:03 PM,03/May/17 11:11 AM,09/May/17 6:48 PM,,,,,,0,EndOf28,,,,"h2. Conductor

We need a job that will automatically create an annotate a develop AMI using all develop components. The final AMI should be annotated as a develop artifact when it is created.

h2. Client

We need code that is able to retrieve the latest develop client artifact (I believe this is just an artifactory URL).
",,tyler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,FUGUE-5322,,,3.0,,,LRT Team,,,,2|hzytuu:000000r00000000006i,,,Sprint 28,Sprint 29,,,,2.0,,,Not started,,,,,,,,,,,,,,,,,
Generate worker functions to speed up named parameters,FUGUE-5349,33442,,Task,In Progress,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,jasper,jasper,jasper,12/Apr/17 7:40 AM,01/May/17 9:30 AM,09/May/17 6:48 PM,,,,Ludwig Language & Compiler,,0,EndOf28,ludwig_performance,,,"If you have a named function:

{noformat}
fun foo{k: Int, l: String} -> String:
    BODY
{noformat}

And you call this function:

{noformat}
bar: foo {k: 1, l: ""Hello""}
{noformat}

Let's step through what happens.

# Create an empty object  {{args: \{\}}} .
# Set the field  {{k}}  to  {{1}}  and then the field  {{l}}  to  {{""Hello""}} 
# Enter the function  {{foo}} . Inside,  {{foo}} :
# Take the field  {{args.k}}  and assign it to  {{k}} . Same for  {{l}} .
# Evaluate the function body.

This ends up creating a lot of temporary object which we don't _really_ need. We should generate worker functions, e.g.:

{noformat}
fun foo$worker(k: Int, l: String) -> String:
    BODY

fun foo{k: Int, l: String} -> String:
    foo$worker(k, l)
{noformat}

Then, we detect a call to with named arguments to  {{foo}} , e.g.:

{noformat}
bar: foo {k: 1, l: ""Hello""}
{noformat}

We inline the worker:

{noformat}
bar: foo$worker(1, ""Hello"")
{noformat}

This avoids creating any sort of temporary object. This optimisation of course only works when:

* It's a call to a _known_ function (no lambdas)
* All the arguments are known, e.g. we don't have  {{foo\(args with \{k: 2\}\)}} 

This applies to most of code though, so should be a good improvement.

I am estimating this rather high because there are some things we need to be careful about, such as not messing up free variables, naming, stack traces...
",,jasper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,Toolchain Team,,,,2|i009yu:,,,Sprint 28,Sprint 29,,,,13.0,,,Not started,,,,,,,,,,,,,,,,,
Bugfix: Upgrader fails on 409 - Conflicting Resource,FUGUE-5388,33549,,Task,In Progress,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,tyler,tyler,tyler,17/Apr/17 8:51 AM,05/May/17 12:10 PM,08/May/17 9:10 AM,,,,,,0,EndOf28,sprint28,,,"During testing of upgrade, the upgrader encountered a 409 - Conflicting Resource error and exited with a stack trace on {{fugue install}} ({{ami-6a9e0b7c}} Sprint 26 AMI). The stack trace caused result in {{versions.json}} not being written on to S3.

A subsequent {{fugue upgrade}} lost track of processes because the account migration script was run when it shouldn't have been run.

There are two things that need done:
# The upgraded code the [interacts with S3|https://github.com/LuminalHQ/fugue-upgrader/blob/74b80c0d730857071603ff0b97c16b88a1fcc411/upgrader/run_startup.py#L139] should have retries around it to deal with transient errors.
# The account migration (2017-01-20a) should be made to be (more) idempotent.",,scott,tyler,wayne,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,3.0,,,Cross Team,,,,2|i003kq:6hyw,,,Sprint 28,Sprint 29,,,,2.0,,,Not started,2017-04-25 11:17:24.205,25/Apr/17 11:17 AM;wayne;This may be as simple as adding the 409 error code to the object store retry error list.,25/Apr/17 11:25 AM;tyler;+1,"28/Apr/17 8:03 PM;wayne;Estimating 2 because there are a handful of components that will need this update.

https://github.com/search?utf8=%E2%9C%93&q=org%3ALuminalHQ+filename%3Arequirements.txt+object-store&type=
https://github.com/search?utf8=%E2%9C%93&q=org%3ALuminalHQ+filename%3Arequirements.txt+object_store&type=
",01/May/17 11:25 AM;scott;Same thing happened while testing the notification service component too.,03/May/17 3:19 PM;wayne;Work has been done in https://github.com/LuminalHQ/objectStore-python/pull/39. We just need to push the new object store out to its dependencies.,"04/May/17 3:54 PM;tyler;Broker:               v1.4.0
Demarc:               v2.10.0
Notification Service: v0.1.1
Policy Service:       v1.4.0
Scheduler:            v1.4.0
Translate Broker:     v0.10.0
Upgrader:             v1.7.0",,,,,,,,,,
"On fugue upgrade, query DDB for VARS table size and warn appropriately (GT 3g)",FUGUE-5391,33561,,Story,Done,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,matt,rob,rob,17/Apr/17 11:45 AM,03/May/17 1:03 PM,09/May/17 6:48 PM,,,,,,0,EndOf28,,,,,,evan,nate,rob,wayne,,,,,,,,PDOC-558,,,,,,,,,,,,,,,,,,,{},,FUGUE-5038,,,3.0,,,CloudOS Team,,,,2|hzytuu:000000r0000000004,,,Sprint 28,Sprint 29,,,,1.0,,,Not started,2017-04-17 12:25:28.921,"17/Apr/17 11:50 AM;rob;Make sure to update IAM templates to be sure installed version has DDI permissions (it might already, just confirm)","17/Apr/17 12:25 PM;wayne;^ Confirmed, it's there.",17/Apr/17 12:25 PM;wayne;We also have a question on whether this needs to go into sprint 26 cli [~nate] [~rob] [~alex] [~wayne] [~stephen],"17/Apr/17 12:36 PM;nate;[~wayne] I think the answer is ""No"".  We can manually eyeball the customer's tables for 26. I don't want to move the goalposts for the release again.",17/Apr/17 1:18 PM;wayne;I'm good with that as long as we are sure customers will reach out to us before attempting to upgrade.,"17/Apr/17 3:07 PM;wayne;Proposing the following for prompting on fugue upgrade:

Less than 3gb table:

{code:java}
± fugue upgrade ami-30e16551 --yes
[ fugue upgrade ] Upgrading Conductor

Upgrade Details:

   Conductor AMI ID: ami-30e16551
   AWS Account: wayne/326008628691

Checking availability and validity of ami-30e16551 ...
[ OK ] AMI ID is valid and available.

Checking system VARS table size ...
[ OK ] System VARS table confirmed (1 GB).
...
{code}

Greater than 3gb table:

{code:java}
± fugue upgrade ami-30e16551 --yes
[ fugue upgrade ] Upgrading Conductor

Upgrade Details:

   Conductor AMI ID: ami-30e16551
   AWS Account: wayne/326008628691

Checking availability and validity of ami-30e16551 ...
[ OK ] AMI ID is valid and available.

Checking system VARS table size ...
[ WARN ] System VARS table size is too large to guarantee a smooth migration (50 GB). Please reach out to support@fugue.co before proceeding.

[ WARN ] Are you sure you want to proceed with upgrading? [y/N]: y
...
{code}

Note that a user needs to double-opt in even if the --yes flag is specified. I think if both --yes and --force is specified, we still perform the check and output a message but we don't require confirmation.","17/Apr/17 5:06 PM;evan;[~wayne] Love the approach. Yes, the WARN tag is appropriate here absolutely. And I agree that should it be the case that the VARS table is too large, *AND the User has used JUST THE -yes flag* we should still require confirmation. If the force flag is used, then we just proceed. But the YES flag is a convenience option...this is a case where the --yes flag shouldn't take priority over explicitly confirming the situation. 

That is my recommendation. Feel free to ping me if you disagree or have additional thoughts.",26/Apr/17 9:44 AM;wayne;Current WIP in https://github.com/LuminalHQ/fugue-cli-python/tree/enhancement/FUGUE-5391/check-vars-table-size,,,,,,,,
Split resmon-client from resmon,FUGUE-5395,33565,,Task,Done,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,matt,wayne,wayne,17/Apr/17 12:22 PM,01/May/17 9:52 AM,09/May/17 6:48 PM,,,,,,0,EndOf28,,,,,,wayne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,3.0,,,CloudOS Team,,,,2|hzytuu:000000r0000000008,,,Sprint 28,Sprint 29,,,,2.0,,,Not started,,,,,,,,,,,,,,,,,
Bugfix: Attaching small RBAC policy after big RBAC policy takes 6 minutes,FUGUE-5400,33572,,Task,To Do,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,,chrisc,chrisc,17/Apr/17 4:56 PM,01/May/17 9:43 AM,09/May/17 6:48 PM,,,,,,0,sprint29,,,,"h2. Environment Info
Fugue CLI:
- artifact version: 0.30.8
{code}
$ fugue --version
Fugue CLI Version: 1.2.0-2249-81daade2d63427d3e434b75364a2b92834492d04
VARs Version: 2.0.5-2425-0115a8ea3db23c4ae326ecaef73ceff79b0c1def
LWDoc Version: 0.21.0
LWC Version: 0.21.0
{code}

Conductor AMI: ami-6a9e0b7c (Sprint 26 RC)
{code}
$ fdiag ami
ami-6a9e0b7c
{code}

Reproducible? *Yes*

h2. Problem Description

After attaching a big RBAC policy, attaching a new policy takes a really long time.

# {{fugue rbac-attach Policy.lw}} <- works
# {{fugue rbac-attach BigPolicy.lw}} <- works
# {{fugue rbac-attach Policy.lw}} <- TimeOut error, takes 6 minutes to take effect

h3. fugue policy rbac-attach Policy.lw
{code}
$ fugue policy rbac-attach Policy.lw
Compiling Ludwig file Policy.lw ...
[ OK ] Successfully compiled. No errors.

Uploading policy to S3 ...
[ OK ] Successfully uploaded.

Requesting the Conductor set new policy ...
[ ERROR ] Fugue has timed out waiting for a response from the server.
{code}

h2. Logs

Uploading report to Fugue Support as 683892460708/fugue_report-2017-04-17-20-44-42.zip",,chrisc,wayne,,,,,,,,,,,,,17/Apr/17 4:59 PM;chrisc;BigPolicy.lw;https://luminal.atlassian.net/secure/attachment/28445/BigPolicy.lw,17/Apr/17 4:59 PM;chrisc;Policy.lw;https://luminal.atlassian.net/secure/attachment/28446/Policy.lw,,,,,,,,,,,,,,,{},,,,,,,,CloudOS Team,,,,2|hzytuu:000000r0000000011zzzzzzzy,,,Sprint 29,,,,,3.0,,,Not started,2017-04-27 11:53:47.319,"27/Apr/17 11:53 AM;wayne;It sounded like the issue is we need a bulk user deletion utility in the users service/client. When the new policy is attached, all the unreferenced users are deleted. There are hundreds of them, so doing this one at a time takes a long time to complete.",,,,,,,,,,,,,,,
bug fix: fugue <subcommand> --help commands fail if AWS credentials are invalid,FUGUE-5411,33591,,Task,Done,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,matt,tyler,tyler,19/Apr/17 4:26 PM,01/May/17 11:54 AM,09/May/17 6:48 PM,,,,,,0,EndOf28,sprint28,,,"{noformat}
⮀ fugue run --help
[ ERROR ] There was a problem executing this command.
  Reason: An error occurred (InvalidClientTokenId) when calling the GetCallerIdentity operation: The security token included in the request is invalid.
{noformat}

From the CLI log:
{noformat}
2017-04-19T20:24:48.04 [ fugue ] DEBUG - -------------------------------Configuration Info-------------------------------
2017-04-19T20:24:48.04 [ fugue ] DEBUG - Fugue CLI Version: 1.5.4-2388-e37ed14a7fce01a0fff0ccc04333cb16091a5ae2
2017-04-19T20:24:48.04 [ fugue ] DEBUG - Configuration File Path (symbolic links followed): /Users/tyler/conductor/personal/us-east-2/fugue.yaml
2017-04-19T20:24:48.04 [ fugue ] DEBUG - Conductor Region: us-east-2
2017-04-19T20:24:48.04 [ fugue.screen ] ERROR - There was a problem executing this command.
  Reason: An error occurred (InvalidClientTokenId) when calling the GetCallerIdentity operation: The security token included in the request is invalid.
Traceback (most recent call last):
  File ""site-packages/fugue_cli/cli.py"", line 156, in invoke
  File ""site-packages/click/core.py"", line 1057, in invoke
  File ""site-packages/click/core.py"", line 889, in invoke
  File ""site-packages/click/core.py"", line 534, in invoke
  File ""site-packages/click/decorators.py"", line 17, in new_func
  File ""site-packages/fugue_cli/cli.py"", line 203, in cli
  File ""site-packages/fugue_cli/cli.py"", line 96, in _print_config_info
  File ""site-packages/fugue_cli/project.py"", line 123, in composition_bucket
  File ""site-packages/fugue_cli/project.py"", line 232, in _get_property
  File ""site-packages/fugue_cli/project.py"", line 252, in _get_default_property
  File ""site-packages/fugue_cli/project.py"", line 48, in get_compositions_bucket_name
  File ""site-packages/fugue_cli/utils.py"", line 150, in get_user_identity
  File ""site-packages/botocore/client.py"", line 251, in _api_call
  File ""site-packages/botocore/client.py"", line 537, in _make_api_call
botocore.exceptions.ClientError: An error occurred (InvalidClientTokenId) when calling the GetCallerIdentity operation: The security token included in the request is invalid.
{noformat}",,matt,tyler,,,,,,,,,,QA-314,,,,,,,,,,,,,,,,,,,{},,,,,2.0,,,CloudOS Team,,,,2|hzytuu:000000r00000000006,,,Sprint 28,Sprint 29,,,,,,,Not started,2017-05-01 09:49:18.957,01/May/17 9:49 AM;matt;fugue-client: 0.32.1,,,,,,,,,,,,,,,
Bugfix: Error modifying network interfaces,FUGUE-5413,33593,,Task,In Progress,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,danthemyth,ben,ben,20/Apr/17 9:19 AM,09/May/17 9:10 AM,09/May/17 6:48 PM,,,,,,0,sprint29,,,,"AMI: ami-709e0b66
CLI: 0.31.5
FID: 9d8d3652-7982-4e3f-9164-9503ecbfbd06

Started with composition simple_ni.lw, then updated to two_nis.lw, then updated to modified_ni.lw.  When I tried to update back to two_nis.lw I got the following error:

{noformat}
[
    {
        ""account_id"": ""fugue-1492446008073"",
        ""alias"": ""ni"",
        ""created"": 1492654158,
        ""current_job_status"": {
            ""current_time"": 1492656188,
            ""dry_run"": false,
            ""job_id"": 1492656142,
            ""start_time"": 1492656142
        },
        ""fid"": ""9d8d3652-7982-4e3f-9164-9503ecbfbd06"",
        ""last_job_id"": 1492655571,
        ""last_job_status"": {
            ""description"": ""ResultsWriteError"",
            ""dry_run"": false,
            ""finish_time"": 1492656120,
            ""job_id"": 1492655571,
            ""job_status"": ""FAILED"",
            ""message"": ""Retries Exhausted recording results [aws.ec2.modify_network_interface_attribute] with [{'params': {'NetworkInterfaceId': 'eni-4e2c1835', 'Description': {'Value': ''}}}]: ['Value']"",
            ""start_time"": 1492655571,
            ""status_code"": 400
        },
        ""last_message"": ""Retries Exhausted recording results [aws.ec2.modify_network_interface_attribute] with [{'params': {'NetworkInterfaceId': 'eni-4e2c1835', 'Description': {'Value': ''}}}]: ['Value']"",
        ""resources"": null,
        ""state"": ""BUSY"",
        ""updated"": 1492655553
    }
]
{noformat}

On the next system job, that then shifted into the following error:
{noformat}
[
    {
        ""account_id"": ""fugue-1492446008073"",
        ""alias"": ""ni"",
        ""created"": 1492654158,
        ""current_job_status"": {
            ""current_time"": 1492657717,
            ""dry_run"": false,
            ""job_id"": 1492657673,
            ""start_time"": 1492657673
        },
        ""fid"": ""9d8d3652-7982-4e3f-9164-9503ecbfbd06"",
        ""last_job_id"": 1492657163,
        ""last_job_status"": {
            ""description"": ""InvalidIPAddress.InUse"",
            ""dry_run"": false,
            ""finish_time"": 1492657668,
            ""job_id"": 1492657163,
            ""job_status"": ""FAILED"",
            ""message"": ""Retries Exhausted issuing instruction [aws.ec2.create_network_interface] with [{'PrivateIpAddress': '10.0.2.11', 'SubnetId': 'subnet-989a50e3', 'Groups': ['sg-8d55d7e4', 'sg-a85ad8c1'], 'Description': 'A network interface'}]: [An error occurred (InvalidIPAddress.InUse) when calling the CreateNetworkInterface operation: The specified address is already in use.]"",
            ""start_time"": 1492657163,
            ""status_code"": 400
        },
        ""last_message"": ""Retries Exhausted issuing instruction [aws.ec2.create_network_interface] with [{'PrivateIpAddress': '10.0.2.11', 'SubnetId': 'subnet-989a50e3', 'Groups': ['sg-8d55d7e4', 'sg-a85ad8c1'], 'Description': 'A network interface'}]: [An error occurred (InvalidIPAddress.InUse) when calling the CreateNetworkInterface operation: The specified address is already in use.]"",
        ""resources"": null,
        ""state"": ""BUSY"",
        ""updated"": 1492655553
    }
]
{noformat}

From there I was unable to kill the composition.
Fugue History
{noformat}
Jobs History Report for oktaAdmin/434196457675 - Wed Apr 19 2017 11:31pm
ni (9d8d3652-7982-4e3f-9164-9503ecbfbd06)

Row Id      Started    Last Updated    Job Type    Jobs Ran    Job Status    Description
----------  ---------  --------------  ----------  ----------  ------------  ----------------------
1492658755  11:25pm                    KILL        1
1492658184  11:16pm    11:25pm         KILL        1           FAILED        InvalidIPAddress.InUse
1492657673  11:07pm    11:16pm         SYSTEM      1           FAILED        InvalidIPAddress.InUse
1492657163  10:50pm    11:07pm         SYSTEM      2           FAILED        InvalidIPAddress.InUse
1492656142  10:42pm    10:50pm         SYSTEM      1           FAILED        InvalidIPAddress.InUse
1492655571  10:32pm    10:42pm         UPDATE      1           FAILED        ResultsWriteError
1492655541  10:26pm    10:32pm         SYSTEM      12          SUCCEEDED     JobSucceeded
1492655031  10:23pm    10:26pm         UPDATE      1           SUCCEEDED     JobSucceeded
1492655001  10:20pm    10:23pm         SYSTEM      7           SUCCEEDED     JobSucceeded
1492654760  10:19pm    10:20pm         UPDATE      1           SUCCEEDED     JobSucceeded
1492654730  10:15pm    10:18pm         SYSTEM      7           SUCCEEDED     JobSucceeded
1492654160  10:09pm    10:15pm         RUN         1           SUCCEEDED     JobSucceeded
{noformat}",,ben,danthemyth,tyler,,,,,,,,,,,,20/Apr/17 9:18 AM;ben;Infra.lw;https://luminal.atlassian.net/secure/attachment/28462/Infra.lw,20/Apr/17 9:19 AM;ben;fugue_report-2017-04-20-03-32-38.zip;https://luminal.atlassian.net/secure/attachment/28463/fugue_report-2017-04-20-03-32-38.zip,20/Apr/17 9:18 AM;ben;modify_ni.lw;https://luminal.atlassian.net/secure/attachment/28461/modify_ni.lw,20/Apr/17 9:19 AM;ben;simple_ni.lw;https://luminal.atlassian.net/secure/attachment/28460/simple_ni.lw,20/Apr/17 9:19 AM;ben;two_nis.lw;https://luminal.atlassian.net/secure/attachment/28459/two_nis.lw,09/May/17 9:09 AM;danthemyth;two_nis_reduced.lw;https://luminal.atlassian.net/secure/attachment/29001/two_nis_reduced.lw,,,,,,,,,,,{},,,,,,,,LRT Team,,,,2|hzytuu:000000r000000hzo,,,Sprint 29,,,,,3.0,,,Not started,2017-04-27 11:26:04.626,"27/Apr/17 11:26 AM;tyler;This may be ""Will Not Fix"".

If you go from a composition with random IP addresses to a composition with new specified IP addresses, there is a chance of a collision.

If this is the case, please let [~tyler] and [~alex]. We can talk about designing a solution but the bug will likely move to ""Will Not Fix"".

If this is not the case, please investigate and fix if possible.","09/May/17 9:08 AM;danthemyth;In this case, the NI's index record was being overwritten by and update intended to add the ENI association to the NI. Updates don't need to rewrite the index so the fix here is to simply disable index an index write on record update.

This fix in particular should eliminate this particular class of error where a guid is inappropriately re-used during update.

Also added two_nis_reduced.lw which minimally reproduces this error.",,,,,,,,,,,,,,
Bugfix: Irrefutable pattern failure when compiling with profiling,FUGUE-5414,33598,,Task,To Do,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,,maciej,maciej,20/Apr/17 12:27 PM,01/May/17 9:45 AM,09/May/17 6:48 PM,,,,Ludwig Language & Compiler,,0,sprint29,,,,"To reproduce: in {{fugue-aws-pattern-network}} apply this patch:
{code}
diff --git a/Makefile b/Makefile
index 0f9cb2a..0af7860 100644
--- a/Makefile
+++ b/Makefile
@@ -1,6 +1,6 @@
 
 LWINCLUDE = $(shell find ./deps -maxdepth 1 -mindepth 1 -type d | paste -sd : - | sed -e 's/^\(.\)/-i \1/')
-LWCFLAGS = --warnings --werror $(LWINCLUDE)
+LWCFLAGS = --no-warnings -dprofiling --timeout 30 $(LWINCLUDE)
 LWDOCFLAGS = $(LWINCLUDE)
 
 default: check
@@ -21,4 +21,7 @@ lwdoc: docs
 docs:
     mkdir docs
 
-.PHONY: $(LUDWIG) ludwig check lwdoc
+quick-check:
+    find Fugue deps/*/Fugue -name '*.lw' | sed 's/.*\(Fugue.*\)/\1/g' | sed 's/\//./g' | sed 's/\.lw//g'| sed 's/\(.*\)/import \1/g' | lwc $(LWCFLAGS) -
+
+.PHONY: $(LUDWIG) ludwig check quick-check lwdoc
diff --git a/deps/fugue-aws b/deps/fugue-aws
index 766e895..92596b4 160000
--- a/deps/fugue-aws
+++ b/deps/fugue-aws
@@ -1 +1 @@
-Subproject commit 766e895836fd01b157bf259db12bf016df07f0c4
+Subproject commit 92596b4aca662f09f8adef9ee25ddd5f8efcc875
diff --git a/deps/fugue-core-aws b/deps/fugue-core-aws
index 05f16a1..f3521eb 160000
--- a/deps/fugue-core-aws
+++ b/deps/fugue-core-aws
@@ -1 +1 @@
-Subproject commit 05f16a185962574d1e7ae0a5e86085c8a0f592b2
+Subproject commit f3521eb79633b0c83b5781ec3bad9689efb6da4c
diff --git a/deps/fugue-netaddr b/deps/fugue-netaddr
index 7d7759b..09a3056 160000
--- a/deps/fugue-netaddr
+++ b/deps/fugue-netaddr
@@ -1 +1 @@
-Subproject commit 7d7759b73384e4af1fa5bcf293eae53b4da32373
+Subproject commit 09a3056db4ac47db3313ae29fba9115afa036445
{code}
Then run
{code}
make quick-check
{code}",,jasper,maciej,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,Toolchain Team,,,,2|i00anq:,,,Sprint 29,,,,,1.0,,,Not started,2017-04-28 13:11:00.66,28/Apr/17 1:11 PM;jasper;This should be a very easy fix.,,,,,,,,,,,,,,,
Bugfix: lwc doesn't to compile compositions with DOS-style newline in string values,FUGUE-5432,33708,,Task,PR Merged,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,brett,chrisc,chrisc,21/Apr/17 12:49 PM,08/May/17 5:22 AM,09/May/17 6:48 PM,,,,Ludwig Language & Compiler,,0,sprint29,,,,"h2. Environment Info

{code}
$ fugue --version
Fugue CLI Version: 1.5.4-2388-e37ed14a7fce01a0fff0ccc04333cb16091a5ae2
VARs Version: 2.0.5-2425-0115a8ea3db23c4ae326ecaef73ceff79b0c1def
LWDoc Version: 0.24.2
LWC Version: 0.24.2
{code}

h2. Problem Description

lwc fails to compile compositions that contain DOS style line endings in inline strings.

{code}
role: IAM.Role.new {
  roleName: ""Application-User"",    <- ok if this is DOS-style line ending
  assumeRolePolicyDocument: '{   <- NOT ok if this is DOS-style line ending
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Sid"": """",
      ""Effect"": ""Allow"",
      ""Principal"": {
        ""AWS"": ""arn:aws:iam::183840590373:root""
      },
      ""Action"": ""sts:AssumeRole""
    }
  ]
}',
  managedPolicies: [
    fugue-user,
  ],
}
{code}

For example, I've attached 2 versions of the same compositions (and screenshots showing the line endings).

The composition can have DOS-style line endings in it, just not in string values for a type.

h3. Note

lwc correctly compiles compositions that read files with DOS-style line endings using {{String.readFileUtf8}}.",,chrisc,,,,,,,,,,,,,,21/Apr/17 12:50 PM;chrisc;DoesNotWork.lw;https://luminal.atlassian.net/secure/attachment/28506/DoesNotWork.lw,21/Apr/17 12:52 PM;chrisc;DoesNotWork.png;https://luminal.atlassian.net/secure/attachment/28507/DoesNotWork.png,21/Apr/17 12:50 PM;chrisc;Works.lw;https://luminal.atlassian.net/secure/attachment/28505/Works.lw,21/Apr/17 12:52 PM;chrisc;Works.png;https://luminal.atlassian.net/secure/attachment/28508/Works.png,,,,,,,,,,,,,{},,,,,,,,Toolchain Team,,,,2|i00au6:,,,Sprint 29,,,,,3.0,,,Not started,,,,,,,,,,,,,,,,,
have fugue-support reset-secret guess the bucket name if one isnt provided or in fugue.yaml,FUGUE-5433,33709,,Task,To Do,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,,alex,alex,21/Apr/17 1:08 PM,28/Apr/17 12:15 PM,09/May/17 6:48 PM,,,,,,0,NEEDS_INFO,,,,"now that fugue.yaml has had bucketname removed, its not obvious what I should be giving to fugue-support reset-secret",,achintya,alex,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,CloudOS Team,,,,2|i003kq:649,,,Sprint 29,,,,,2.0,,,Not started,2017-04-27 16:01:16.241,"27/Apr/17 4:01 PM;achintya;The fugue support CLI can calculate the bucket name in the same way as the main CLI does:
https://github.com/LuminalHQ/fugue-cli-python/blob/develop/fugue_cli/project.py#L49",,,,,,,,,,,,,,,
Write vars schema migration to move from multi-item to single-item mode,FUGUE-5435,33716,,Task,In Progress,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,eric,alex,alex,21/Apr/17 2:43 PM,09/May/17 1:41 PM,09/May/17 6:48 PM,,,,,,0,,,,,,,alex,eric,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,FUGUE-5038,,,,,,CloudOS Team,,,,2|hzytuu:000000r0000000011zzzzzg,,,Sprint 29,,,,,5.0,,,Not started,2017-04-25 10:09:19.788,"25/Apr/17 10:09 AM;eric;we already have tools to export, transform and upload ddb information. in the case here the needed transform step would be a full cleaning of multiple versions, which we already have (vars-cleaner utility), then we would create a new ddb table without a Sort key, then run the same steps we have from the FUGUE-5077 scripts. We turn the cleaned data into batches and run dynamodb-import to the new table.

The bulk of the work here will be the orchestration of how/when this process happens. This will need some design work, so i say 5","09/May/17 1:41 PM;eric;PR: https://github.com/LuminalHQ/vars/pull/451

creates {{varsctl migrate}} tool

Next step is to get it installed and automated on conductor-image",,,,,,,,,,,,,,
Bugfix: Uncaught pocky exception,FUGUE-5440,33721,,Task,PR Merged,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,matt,tyler,tyler,21/Apr/17 5:54 PM,03/May/17 9:57 AM,09/May/17 6:48 PM,,,,,,0,sprint29,,,,"Derived from FUGUE-5350

There is an uncaught exception in the pocky keymanager that can surface and bring down the broker:
{noformat}
Traceback (most recent call last):
  File ""/opt/fugue/.pex/install/pocky-2.0.2-py3-none-any.whl.c29cdb955a66fc367b551371381eaec44e697731/pocky-2.0.2-py3-none-any.whl/pocky/base.py"", line 9, in read
    payload, ack, _ = self.next_message(**kwargs)
  File ""/opt/fugue/.pex/install/pocky-2.0.2-py3-none-any.whl.c29cdb955a66fc367b551371381eaec44e697731/pocky-2.0.2-py3-none-any.whl/pocky/pocky_auth.py"", line 135, in next_message
    secret = self._get_signing_key(key_name, ack, nack)
  File ""/opt/fugue/.pex/install/pocky-2.0.2-py3-none-any.whl.c29cdb955a66fc367b551371381eaec44e697731/pocky-2.0.2-py3-none-any.whl/pocky/pocky_auth.py"", line 171, in _get_signing_key
    raise AcknowledgeOnError(e, ack)
pocky.exceptions.AcknowledgeOnError: (KVNotFoundException(), <function PockyReader.next_message.<locals>.ack at 0x7fbe7ca0cc80>)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "".bootstrap/_pex/pex.py"", line 360, in execute
  File "".bootstrap/_pex/pex.py"", line 288, in _wrap_coverage
  File "".bootstrap/_pex/pex.py"", line 320, in _wrap_profiling
  File "".bootstrap/_pex/pex.py"", line 401, in _execute
  File "".bootstrap/_pex/pex.py"", line 432, in execute_script
  File "".bootstrap/_pex/pex.py"", line 446, in execute_content
  File "".bootstrap/_pex/compatibility.py"", line 72, in exec_function
  File ""/opt/fugue/.pex/install/fugue_broker-0.19.5-py3-none-any.whl.16dfcd97eb8b53bd3cfb25a604afd44bc3f85845/fugue_broker-0.19.5-py3-none-any.whl/fugue_broker-0.19.5.dist-info/fugue_broker-0.19.5.data/scripts/fugue_broker"", line 16, in <module>
  File ""/opt/fugue/.pex/install/fugue_broker-0.19.5-py3-none-any.whl.16dfcd97eb8b53bd3cfb25a604afd44bc3f85845/fugue_broker-0.19.5-py3-none-any.whl/fugue_broker-0.19.5.dist-info/fugue_broker-0.19.5.data/scripts/fugue_broker"", line 10, in main
  File ""/opt/fugue/.pex/install/fugue_broker-0.19.5-py3-none-any.whl.16dfcd97eb8b53bd3cfb25a604afd44bc3f85845/fugue_broker-0.19.5-py3-none-any.whl/fugue_broker/broker_supervisor.py"", line 28, in cli
    supe.run()
  File ""/opt/fugue/.pex/install/scaffold-2.1.0-py3-none-any.whl.c41356528a56f809f0ec41ba48a520e168dbbfc9/scaffold-2.1.0-py3-none-any.whl/scaffold/supervisor.py"", line 86, in run
    input_msg = self._get_input()
  File ""/opt/fugue/.pex/install/scaffold-2.1.0-py3-none-any.whl.c41356528a56f809f0ec41ba48a520e168dbbfc9/scaffold-2.1.0-py3-none-any.whl/scaffold/supervisor.py"", line 185, in _get_input
    msg = self._inbox.read()
  File ""/opt/fugue/.pex/install/pocky-2.0.2-py3-none-any.whl.c29cdb955a66fc367b551371381eaec44e697731/pocky-2.0.2-py3-none-any.whl/pocky/base.py"", line 19, in read
    e.raise_error()
  File ""/opt/fugue/.pex/install/pocky-2.0.2-py3-none-any.whl.c29cdb955a66fc367b551371381eaec44e697731/pocky-2.0.2-py3-none-any.whl/pocky/exceptions.py"", line 56, in raise_error
    raise self.original_exc
  File ""/opt/fugue/.pex/install/pocky-2.0.2-py3-none-any.whl.c29cdb955a66fc367b551371381eaec44e697731/pocky-2.0.2-py3-none-any.whl/pocky/pocky_auth.py"", line 159, in _get_signing_key
    key = self.km_client.get_key(key_name)
  File ""/opt/fugue/.pex/install/keymanager_client_python-1.0.0-py3-none-any.whl.f4ee9e66c47e290bbe79eb665c2487779c41b756/keymanager_client_python-1.0.0-py3-none-any.whl/keymanager_client/client.py"", line 50, in get_key
    kv_item = _vars_get(kv=self.kv_service, key_name=key_name)
  File ""/opt/fugue/.pex/install/keymanager_client_python-1.0.0-py3-none-any.whl.f4ee9e66c47e290bbe79eb665c2487779c41b756/keymanager_client_python-1.0.0-py3-none-any.whl/keymanager_client/client.py"", line 137, in _vars_get
    return retry_vars_not_found_or_expired(kv, key_name)
  File ""/opt/fugue/.pex/install/keymanager_client_python-1.0.0-py3-none-any.whl.f4ee9e66c47e290bbe79eb665c2487779c41b756/keymanager_client_python-1.0.0-py3-none-any.whl/keymanager_client/client.py"", line 130, in retry_vars_not_found_or_expired
    raise e
  File ""/opt/fugue/.pex/install/keymanager_client_python-1.0.0-py3-none-any.whl.f4ee9e66c47e290bbe79eb665c2487779c41b756/keymanager_client_python-1.0.0-py3-none-any.whl/keymanager_client/client.py"", line 121, in retry_vars_not_found_or_expired
    ret_value = kv.get(key=key_name)
  File ""/opt/fugue/.pex/install/pyvars-1.2.1-py3-none-any.whl.dca8617cf9db6cfa03461ed01168db3788698a17/pyvars-1.2.1-py3-none-any.whl/pyvars/kv_service.py"", line 80, in get
    raise KVNotFoundException()
pyvars.kv_service.KVNotFoundException
{noformat}

See old ticket's [comment|https://luminal.atlassian.net/browse/FUGUE-5350?focusedCommentId=32217&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-32217] for more details.",,eric,tyler,,,,,,,,,FUGUE-5350,,,,,,,,,,,,,,,,,,,,{},,,,,,,,CloudOS Team,,,,2|hzytuu:000000r0000000011zzzzz8,,,Sprint 29,,,,,2.0,,,Not started,2017-04-27 12:04:57.398,"27/Apr/17 12:04 PM;eric;https://github.com/LuminalHQ/pocky/blob/develop/pocky/pocky_auth.py#L163

needs to be catching KV errors not VarsClient errors. the km_client uses a KV

https://github.com/LuminalHQ/keymanager-client-python/blob/develop/keymanager_client/client.py#L36",,,,,,,,,,,,,,,
"Lenient option for LWC, errors in proto/simple output",FUGUE-5443,33733,,Task,PR Merged,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,jasper,jasper,jasper,24/Apr/17 10:46 AM,08/May/17 9:31 AM,09/May/17 6:48 PM,,,,Ludwig Language & Compiler,,0,EndOf28,ludwig_editor_support,ludwig_error_messages,,"This is mostly to help out the GUI / Visualizer project, but will have other beneficial side effects.

We would like to add a {{--lenient}} flag that causes {{lwc}} to _always_ produce output, even though there are some errors.

Some notes:

# If a Node contains errors, we need to ""bubble these up"" until we reach the top of the node.  We can't encode errors in Strings.
# We must add a new Node type which is just an Error.  This does not need to contain any additional data, since the error message will be on {{stderr}}.
# Errors can now optionally contain the UUID of the offending note.  This will require a bunch of trickery to make these UUIDs consistent through the renaming phases.
# Stack traces should also be added to the JSON form, that way the consumer can select the part of the stack trace that applies to them.",,evan,jasper,leslie,,,,,,,GUI-82,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,Toolchain Team,,,,2|i00aye:,,,Sprint 28,Sprint 29,,,,13.0,,,Not started,2017-05-03 11:45:27.233,"03/May/17 11:45 AM;evan;[~jasper] [~alex] - this ticket blocks the GUI team from implementing validations. So this is high priority work from a GUI standpoint. 
cc: [~leslie] [~henry]","03/May/17 1:23 PM;jasper;[~evan] yes, we should have a working version out on Friday.",,,,,,,,,,,,,,
Bugfix: Error creating SNS topic policy when policy is not valid,FUGUE-5444,33734,,Task,PR Merged,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,joshe,ben,ben,24/Apr/17 10:53 AM,02/May/17 2:06 PM,09/May/17 6:48 PM,,,,,,0,sprint29,,,,"AMI: ami-709e0b66
CLI: 0.31.5
FID: b581111b-41bd-4647-beb1-46366fd144fb

I created a set of topics where the topic_policy was invalid because SNS:* is not allowed as a permission in the policy.  Fugue initially failed on run with the InvalidParameter error. However, 2 system jobs later, the process was marked as SUCCEEDED even though the topic policy had not been set to the correct policy.

{noformat}
Jobs History Report for oktaAdmin/434196457675 - Mon Apr 24 2017 10:27am
sns (b581111b-41bd-4647-beb1-46366fd144fb)

Row Id      Started    Last Updated    Job Type    Jobs Ran    Job Status    Description
----------  ---------  --------------  ----------  ----------  ------------  -----------------
1493043973  10:26am                    KILL        1
1493043463  10:17am    10:26am         KILL        1           FAILED        ResultsWriteError
1493042922  10:08am    10:17am         KILL        1           FAILED        ResultsWriteError
1493042892  10:07am    10:08am         SYSTEM      2           SUCCEEDED     JobSucceeded
1493042772  10:06am    10:07am         SYSTEM      1           FAILED        InvalidParameter
1493042712  10:05am    10:05am         RUN         1           FAILED        InvalidParameter
[
    {
        ""account_id"": ""fugue-1492446008073"",
        ""alias"": ""sns"",
        ""created"": 1493042684,
        ""current_job_status"": {
            ""current_time"": 1493042825,
            ""dry_run"": false,
            ""job_id"": 1493042772,
            ""start_time"": 1493042772
        },
        ""fid"": ""b581111b-41bd-4647-beb1-46366fd144fb"",
        ""last_job_id"": 1493042712,
        ""last_job_status"": {
            ""description"": ""InvalidParameter"",
            ""dry_run"": false,
            ""finish_time"": 1493042757,
            ""job_id"": 1493042712,
            ""job_status"": ""FAILED"",
            ""message"": ""An error occurred (InvalidParameter) when calling the SetTopicAttributes operation: Invalid parameter: Policy statement action out of service scope!"",
            ""start_time"": 1493042712,
            ""status_code"": 400
        },
        ""last_message"": ""An error occurred (InvalidParameter) when calling the SetTopicAttributes operation: Invalid parameter: Policy statement action out of service scope!"",
        ""resources"": null,
        ""state"": ""BUSY"",
        ""updated"": 1493042684
    }
]

{noformat}

",,ben,joshe,,,,,,,,QA-160,,,,,24/Apr/17 10:52 AM;ben;Infra.lw;https://luminal.atlassian.net/secure/attachment/28523/Infra.lw,24/Apr/17 10:52 AM;ben;fugue_report-2017-04-24-14-40-51.zip;https://luminal.atlassian.net/secure/attachment/28527/fugue_report-2017-04-24-14-40-51.zip,24/Apr/17 10:53 AM;ben;lambdaHandler.js.zip;https://luminal.atlassian.net/secure/attachment/28520/lambdaHandler.js.zip,24/Apr/17 10:53 AM;ben;lambda_policy.json;https://luminal.atlassian.net/secure/attachment/28521/lambda_policy.json,24/Apr/17 10:52 AM;ben;sns_policy1.json;https://luminal.atlassian.net/secure/attachment/28526/sns_policy1.json,24/Apr/17 10:52 AM;ben;sns_policy2.json;https://luminal.atlassian.net/secure/attachment/28525/sns_policy2.json,24/Apr/17 10:52 AM;ben;topic_policy.json;https://luminal.atlassian.net/secure/attachment/28522/topic_policy.json,24/Apr/17 10:52 AM;ben;two_sns.lw;https://luminal.atlassian.net/secure/attachment/28524/two_sns.lw,,,,,,,,,{},,,,,,,,LRT Team,,,,2|hzytuu:000000r00000000008i,,,Sprint 29,,,,,5.0,,,Not started,2017-05-01 11:54:25.472,"01/May/17 11:54 AM;joshe;SNS Topic Enforcement was never completed:

https://github.com/LuminalHQ/emitInstructions-go/blob/develop/services/sns/topicPlanner/topicPlanner.go#L112-L118

This task is to complete the SNS Topic type which includes makes the Policy mutable and completing the Planner (including unit tests).","01/May/17 5:29 PM;joshe;We need to have a default value for the SNS Topic Policy. An empty value will not suffice here, it needs to be a valid policy document.",,,,,,,,,,,,,,
have the CLI write lwc output to a temp file,FUGUE-5445,33735,,Task,Will Not Fix,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,,alex,alex,24/Apr/17 11:10 AM,26/Apr/17 6:02 PM,09/May/17 6:48 PM,,,,,,0,NEEDS_INFO,,,,"`String.print` can cause `run`s to fail. See FUGUE-5310. If we have LWC write to a temp (using `mktemp` or something similar), then this problem should just go away.

If moving from a streaming read to file IO is problematic, we can also look at using a named pipe or something else. ",,alex,wayne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,CloudOS Team,,,,2|i00an1:,,,Sprint 29,,,,,,,,Not started,2017-04-26 17:32:33.932,26/Apr/17 5:32 PM;wayne;Do we want to show the output or just write it to a file and ignore it?,26/Apr/17 6:01 PM;alex;just as opposed to reading the output directly to send to S3,26/Apr/17 6:02 PM;alex;looks like we dont need to do this,,,,,,,,,,,,,
Bugfix: Error when creating SNS Subscription with invalid Subscription Attributes,FUGUE-5446,33736,,Task,PR Merged,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,joshe,ben,ben,24/Apr/17 12:14 PM,01/May/17 4:33 PM,09/May/17 6:48 PM,,,,,,0,sprint29,,,,"AMI: ami-709e0b66
CLI: 0.31.5 
FID: d5a00f2b-94b5-43a5-87cf-3f499332bddc

Ran the composition provided below.  The sns_policy1.json config is malformed as min and max delay target are not specified.  On run, that InvalidParameter error is displayed. But on subsequent system jobs, the job is marked as succeeded.

{noformat}
(fdiag) Fugue-107:fugue ben$ fugue history sns --json
[
    {
        ""correlation_id"": ""54fbdd59-e3ef-4440-a3cd-12191fa06f21"",
        ""count"": 1,
        ""description"": ""InvalidParameter"",
        ""dry_run"": false,
        ""fid"": ""d5a00f2b-94b5-43a5-87cf-3f499332bddc"",
        ""id"": 1493049528,
        ""job_status"": ""RUN"",
        ""job_type"": ""RUN"",
        ""last_updated"": 1493049647,
        ""message"": ""An error occurred (InvalidParameter) when calling the SetSubscriptionAttributes operation: Invalid parameter: DeliveryPolicy: healthyRetryPolicy.minDelayTarget must be specified"",
        ""start_time"": 1493049528,
        ""status_code"": 400
    },
    {
        ""correlation_id"": null,
        ""count"": 4,
        ""description"": ""JobSucceeded"",
        ""dry_run"": false,
        ""fid"": ""d5a00f2b-94b5-43a5-87cf-3f499332bddc"",
        ""id"": 1493049739,
        ""job_status"": ""SYSTEM"",
        ""job_type"": ""SYSTEM"",
        ""last_updated"": 1493049744,
        ""message"": ""SUCCESS"",
        ""start_time"": 1493049649,
        ""status_code"": 200
    },
    {
        ""correlation_id"": null,
        ""count"": 1,
        ""description"": null,
        ""dry_run"": false,
        ""fid"": ""d5a00f2b-94b5-43a5-87cf-3f499332bddc"",
        ""id"": 1493049769,
        ""job_status"": ""SYSTEM"",
        ""job_type"": ""SYSTEM"",
        ""last_updated"": null,
        ""message"": null,
        ""start_time"": 1493049769,
        ""status_code"": null
    }
]
(fdiag) Fugue-107:fugue ben$ fugue history sns

Jobs History Report for oktaAdmin/434196457675 - Mon Apr 24 2017 12:04pm
sns (d5a00f2b-94b5-43a5-87cf-3f499332bddc)

Row Id      Started    Last Updated    Job Type    Jobs Ran    Job Status    Description
----------  ---------  --------------  ----------  ----------  ------------  ----------------
1493049829  12:03pm                    SYSTEM      1
1493049799  12:00pm    12:03pm         SYSTEM      6           SUCCEEDED     JobSucceeded
1493049528  11:58am    12:00pm         RUN         1           FAILED        InvalidParameter

{noformat}",,ben,joshe,tyler,,,,,,,QA-160,,,,,24/Apr/17 12:13 PM;ben;Infra.lw;https://luminal.atlassian.net/secure/attachment/28535/Infra.lw,24/Apr/17 12:13 PM;ben;fugue_report-2017-04-24-16-04-28.zip;https://luminal.atlassian.net/secure/attachment/28536/fugue_report-2017-04-24-16-04-28.zip,24/Apr/17 12:13 PM;ben;lambdaHandler.js.zip;https://luminal.atlassian.net/secure/attachment/28533/lambdaHandler.js.zip,24/Apr/17 12:13 PM;ben;lambda_policy.json;https://luminal.atlassian.net/secure/attachment/28534/lambda_policy.json,24/Apr/17 12:13 PM;ben;sns_policy1.json;https://luminal.atlassian.net/secure/attachment/28532/sns_policy1.json,24/Apr/17 12:13 PM;ben;sns_policy2.json;https://luminal.atlassian.net/secure/attachment/28531/sns_policy2.json,24/Apr/17 12:13 PM;ben;topic_policy.json;https://luminal.atlassian.net/secure/attachment/28530/topic_policy.json,24/Apr/17 12:13 PM;ben;two_sns.lw;https://luminal.atlassian.net/secure/attachment/28529/two_sns.lw,,,,,,,,,{},,,,,,,,LRT Team,,,,2|hzytuu:000000r00000000008,,,Sprint 29,,,,,1.0,,,Not started,2017-04-27 11:22:45.695,"27/Apr/17 11:22 AM;tyler;[~ben] and I talked about this, looks like the we are not enforcing/modify subscription attributes. This causes subsequent system jobs to (incorrectly) be reported as successful.",01/May/17 11:56 AM;joshe;This is the same root cause as FUGUE-5447 -- we're comparing old to new nodestream instead of nodestream to reflector results. Changing it to compare to reflector results should resolve this.,,,,,,,,,,,,,,
Bugfix: HALT when modifying attributes on SNS Subscriptions,FUGUE-5447,33743,,Task,PR Merged,FUGUE,Fugue Product,software,nate,Fugue Product Project,,Critical,,joshe,ben,ben,24/Apr/17 2:02 PM,01/May/17 4:33 PM,09/May/17 6:48 PM,,,,,,0,sprint29,,,,"AMI: ami-709e0b66
CLI 0.31.5
FID: 5e89ab5f-aa74-45e0-81e2-1f25f331b604

Updated from two_sns.lw to modify_sns.lw which only updates the SNS attributes on the subscriptions and got the following error.

{noformat}
Jobs History Report for oktaAdmin/434196457675 - Mon Apr 24 2017 1:52pm
sns (5e89ab5f-aa74-45e0-81e2-1f25f331b604)

Row Id      Started    Last Updated    Job Type      Jobs Ran    Job Status    Description
----------  ---------  --------------  ------------  ----------  ------------  -------------
1493056195  1:49pm     1:50pm          UPDATE        1           HALTED        ValueError
1493056165  1:47pm     1:49pm          SYSTEM        4           SUCCEEDED     JobSucceeded
1493056045  1:47pm     1:47pm          UPDATE (dry)  1           SUCCEEDED     JobSucceeded
1493056015  1:46pm     1:47pm          SYSTEM        2           SUCCEEDED     JobSucceeded
1493055955  1:45pm     1:45pm          UPDATE (dry)  1           SUCCEEDED     JobSucceeded
1493055925  1:43pm     1:45pm          SYSTEM        4           SUCCEEDED     JobSucceeded
1493055805  1:43pm     1:43pm          UPDATE (dry)  1           SUCCEEDED     JobSucceeded
1493055774  1:29pm     1:42pm          SYSTEM        28          SUCCEEDED     JobSucceeded
1493054783  1:26pm     1:28pm          RUN           1           SUCCEEDED     JobSucceeded

(fdiag) Fugue-107:fugue ben$ fugue status --json
[
    {
        ""account_id"": ""fugue-1492446008073"",
        ""alias"": ""sns"",
        ""created"": 1493054766,
        ""current_job_status"": {
            ""current_time"": 1493056330,
            ""dry_run"": false,
            ""job_id"": null,
            ""start_time"": null
        },
        ""fid"": ""5e89ab5f-aa74-45e0-81e2-1f25f331b604"",
        ""last_job_id"": 1493056195,
        ""last_job_status"": {
            ""description"": ""ValueError"",
            ""dry_run"": false,
            ""finish_time"": 1493056254,
            ""job_id"": 1493056195,
            ""job_status"": ""HALTED"",
            ""message"": ""An unhandled error occurred running a Job: ValueError - Invalid endpoint: https://sns..amazonaws.com."",
            ""start_time"": 1493056195,
            ""status_code"": 500
        },
        ""last_message"": ""An unhandled error occurred running a Job: ValueError - Invalid endpoint: https://sns..amazonaws.com."",
        ""resources"": null,
        ""state"": ""HALT"",
        ""updated"": 1493056254
    }
]
{noformat}",,ben,joshe,tyler,,,,,,,QA-160,,,,,24/Apr/17 2:01 PM;ben;Infra.lw;https://luminal.atlassian.net/secure/attachment/28546/Infra.lw,24/Apr/17 2:02 PM;ben;fugue_report-2017-04-24-17-52-34.zip;https://luminal.atlassian.net/secure/attachment/28547/fugue_report-2017-04-24-17-52-34.zip,24/Apr/17 2:01 PM;ben;lambdaHandler.js.zip;https://luminal.atlassian.net/secure/attachment/28544/lambdaHandler.js.zip,24/Apr/17 2:01 PM;ben;lambda_policy.json;https://luminal.atlassian.net/secure/attachment/28545/lambda_policy.json,24/Apr/17 2:01 PM;ben;modify_sns.lw;https://luminal.atlassian.net/secure/attachment/28543/modify_sns.lw,24/Apr/17 2:02 PM;ben;sns_policy1.json;https://luminal.atlassian.net/secure/attachment/28542/sns_policy1.json,24/Apr/17 2:02 PM;ben;sns_policy2.json;https://luminal.atlassian.net/secure/attachment/28541/sns_policy2.json,24/Apr/17 2:02 PM;ben;topic_policy.json;https://luminal.atlassian.net/secure/attachment/28540/topic_policy.json,24/Apr/17 2:02 PM;ben;two_sns.lw;https://luminal.atlassian.net/secure/attachment/28539/two_sns.lw,,,,,,,,{},,,,,,,,LRT Team,,,,2|hzytuu:000000r00000000007,,,Sprint 29,,,,,3.0,,,Not started,2017-04-27 11:21:29.745,27/Apr/17 11:21 AM;tyler;Looks like the region is getting dropped somewhere?,"28/Apr/17 4:54 PM;joshe;This is a planner bug:
{code}
Command aws.sns.set_subscription_attributes for resource 5e89ab5f-aa74-4
5e0-81e2-1f25f331b604.1fe40cca-4e87-5714-ba1a-10f7af571ec9 in account ID  region  added by go-planner on layer emit-instructions
{code}
This excerpt from the below log shows that the Planner added an instruction with no region
{code}
{""account_id"":"""",""component"":""manager"",""fid"":""5e89ab5f-aa74-45e0-81e2-1f25f331b604"",""guid"":""5e89ab5f-aa74-45e0-81e2-1f25f331b604.1fe40cca-4e87-5714-ba1a-10f7af
571ec9"",""job_id"":1493056195,""layer"":""emit-instructions"",""log_level"":""debug"",""message"":""Command aws.sns.set_subscription_attributes for resource 5e89ab5f-aa74-4
5e0-81e2-1f25f331b604.1fe40cca-4e87-5714-ba1a-10f7af571ec9 in account ID  region  added by go-planner on layer emit-instructions"",""params"":""{\""AttributeName\"":
\""DeliveryPolicy\"",\""AttributeValue\"":\""{\\\""healthyRetryPolicy\\\"":{\\\""numRetries\\\"":6, \\\""minDelayTarget\\\"":40, \\\""maxDelayTarget\\\"":60}}\"",\""Subscript
ionArn\"":\""uGrS1lHCcU\""}"",""planner"":""go-planner"",""region"":"""",""request_type"":""aws.sns.set_subscription_attributes"",""timestamp"":""2017-04-24T17:49:59.131901""}
{code}
","28/Apr/17 5:20 PM;joshe;The only logical conclusion is that [this line|https://github.com/LuminalHQ/emitInstructions-go/blob/develop/services/sns/subscriptionPlanner/subscriptionPlanner.go#L100] returned an empty string. It doesn't seem possible for it to do so though, unless the reflector missed filling out the region, or if somehow that entry was not found in `psSubscriptions`...",28/Apr/17 5:30 PM;joshe;Ah it very well is possible due to [this|https://github.com/LuminalHQ/emitInstructions-go/blob/develop/services/sns/subscriptionPlanner/subscriptionPlanner.go#L153] as the Ludwig ID may have change from the old nodestream to the new nodestream... need to think about it a little more,"28/Apr/17 6:15 PM;joshe;The only possibility is that the Ludwig ID that was in both the Old nodestream and new nodestream did _not_ appear in the Reflector results. This would result in the enforcement being called on that Ludwig ID but there was no actual result set.

This also makes sense because [the behavior in the Planner|https://github.com/LuminalHQ/emitInstructions-go/blob/develop/services/sns/subscriptionPlanner/requestBuilder.go#L143-L146] when comparing if two subscriptions are equal is to just return {{false}} in the case where they are not found in the reflector results (this should actually be an error).

Thus they were determined to be unequal, and the request was built with empty string for region.","28/Apr/17 6:30 PM;joshe;It appears the resource in question was owned by two FIDs (not sure if simultaneously)... FID {{1deb89ab-5915-424e-acca-672ece2f067b}}

If they were concurrently running and the first was killed, it may have deleted the subscription, and then this update ran before the 10 minute expiration on the resource.

If this is true, after 10 minutes it should actually re-create the Topic and succeed.

So the fix is two part:
1. Fix the SNS subscription planner to actually validate the old nodestream versus reflector results
2. Make SNS Subscriptions idempotent","28/Apr/17 6:33 PM;joshe;Fix 2 should be as simple as adding {{""operation_type"": ""IDEMPOTENT_CREATE""}} to [this stanza|https://github.com/LuminalHQ/strategies/blob/develop/fugue_strategies/coverage/aws_coverage.json#L3235-L3250]","28/Apr/17 6:42 PM;joshe;Ah it appears the [resource ID for subscriptions|https://github.com/LuminalHQ/strategies/blob/develop/fugue_strategies/aws/strategies/sns.py#L225] is quite artisanal... so using the existing solution won't work because this resource ID is a combination of multiple request parameters. 

A new paradigm will be needed to use a dynamic resource ID generation function [here|https://github.com/LuminalHQ/fugue-broker/blob/develop/fugue_broker/broker_worker.py#L402] when checking for idempotency. (Ideally this would be entirely contained in the strategies but the broker does the actual work because the check happens _before_ the strategy is invoked)",01/May/17 9:15 AM;tyler;[~joshe] let's talk out this after planning. I made some changes end of last sprint that may help.,,,,,,,
Data Externalization in Ludwig,FUGUE-5450,33519,,Story,To Do,FUGUE,Fugue Product,software,nate,Fugue Product Project,,Critical,,,stephen,stephen,13/Apr/17 8:11 PM,28/Apr/17 1:08 PM,09/May/17 6:48 PM,,,,Ludwig Language & Compiler,,0,ludwig_language_design,,,,We currently support the ability for compositions to ingest external data via env variables or bindings in external Ludwig files (either imported explicitly or via dynamically modifying import paths). We should investigate supporting this use case as a first class citizen in Ludwig.,,jasper,stephen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,Toolchain Team,,,,2|i00aae:,,,Sprint 29,,,,,13.0,,,Not started,2017-04-28 13:08:16.403,"18/Apr/17 4:30 PM;stephen;We could also pass typed args via the CLI:

{{fugue run -a test FugueDemo.lw String:""foo"" Int:10}}

or named args:

{{fugue run -a test FugueDemo.lw name:""John Doe"" age:42}}

the latter probably makes more sense - we could insert the args into a module (e.g. ARGV.name, ARGV.age) that is automatically inserted into the nodestream.","28/Apr/17 1:08 PM;jasper;Commenting to clarify that this ticket is about making it easier to _use_ external data.

This could happen by allowing to leave holes in expressions that can be filled in later.  E.g.

{noformat}
Int port: {{MONGODB_PORT}}
{noformat}

Or similar.  It's important to note that this is _not_ a string templating feature, we would like to substitute in typed expressions.",,,,,,,,,,,,,,
Flag to disable (or limit) inlining of external files,FUGUE-5453,33778,,Task,In Progress,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,brett,andrew,andrew,26/Apr/17 2:28 PM,08/May/17 11:19 AM,09/May/17 6:48 PM,,,,Ludwig Language & Compiler,,0,,,,,"LWC inlines external files, which in some cases can result in massive outputs (encoded zip files for lambda functions, for example). We'd like a flag which just includes the path for them instead.",,jasper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,GUI-72,,,,,,Toolchain Team,,,,2|i00an0:i,,,Sprint 29,,,,,3.0,,,Not started,2017-04-28 13:10:19.387,"28/Apr/17 1:10 PM;jasper;This should be implement using some sort of limit flag.  We want to trim any _atoms_ (strings, bytes, externals) that are larger than the given limit in bytes.  E.g.:

{noformat}
lwc --limit-atoms=1024
{noformat}",,,,,,,,,,,,,,,
Bugfix: Lambda: Nodejs 0.10 runtime EOL 5/30/2017,FUGUE-5454,33803,,Task,PR Merged,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,chrisg,chrisg,chrisg,27/Apr/17 9:26 AM,09/May/17 2:21 PM,09/May/17 6:48 PM,,,,,,0,sprint29,,,,"{quote}
Hello,

Your AWS Account (ID: 235986320834) currently has one or more Lambda functions using the Node.js v0.10 runtime. As previously communicated, Node v0.10.42 is currently marked as deprecated on AWS Lambda as per our runtime support policy [1].We continue to recommend that all functions that use it must be migrated to a newer Node.js runtime version. Based on customer feedback, we are announcing a *one-time* extension of the date to May 30th 2017 for our community of Node 0.10 developers to aid in the migration of their remaining functions to newer versions.

Why is this happening?
The Node Foundation declared End-of-Life (EOL) of Node.js v0.10 on October 31st 2016[2], which means that it has stopped receiving bug fixes, security updates and performance improvements. Migration onto a newer version of Node.js runtime ensures these upgrades are available to your functions.

What has AWS Lambda done in response?
AWS Lambda announced Node.js v0.10 deprecation on November 2nd 2016 [3] and disabled new function creation using this runtime on January 11th 2017.

Node.js v6.10 support on Lambda was released on March 22nd, 2017, providing an updated target for migration compared to Node.js v4.3.

What do I need to do?
You must migrate all your Node.js v0.10 functions (identified by the “nodejs” runtime) to a newer Node.js version (4.3 or 6.10) before May 30th 2017. For more information on how to migrate, please see the ""Transitioning Lambda Function Code to newer Runtimes"" section of our documentation in the AWS Lambda Developer Guide [4]. This section also describes transitioning function versions using the Node.js v0.10 runtime and aliases associated with such functions/versions.

Which version of Node.js should I migrate to?
While Node.js v4.3 is available on AWS Lambda, we recommend that you migrate directly to Node.js v6.10 which introduces improved ES6 support, along with other language and API improvements.

Where can I get more information?
For information on how to migrate to a newer version of Node.js, please see the ""Transitioning Lambda Function Code to newer Runtimes"" section of our documentation mentioned above and the AWS Lambda Forums [5].

For general information on Node.js and migrating to newer versions from Node.js v0.10, please see the Node community documentation related to this topic [6] [7] [8] [9].

[1]: http://docs.aws.amazon.com/lambda/latest/dg/nodejs-prog-model-using-old-runtime.html#nodejs-prog-model-runtime-support-policy.
[2]: Node Foundation’s announcement of EOL: https://github.com/nodejs/LTS
[3]: AWS Lambda forum post for deprecation: https://forums.aws.amazon.com/ann.jspa?annID=4345
[4]: Transitioning to newer runtimes: http://docs.aws.amazon.com/lambda/latest/dg/nodejs-prog-model-using-old-runtime.html#transition-to-new-nodejs-runtime
[5]: AWS Lambda Forum: https://forums.aws.amazon.com/forum.jspa?forumID=186&start=0
[6]: Upgrading Node.js v0.10 applications: https://www.joyent.com/blog/upgrading-nodejs
[7]: Changes between v0.10 and v4: https://github.com/nodejs/node/wiki/API-changes-between-v0.10-and-v4
[8] Changes between v4 and v5: https://github.com/nodejs/node/blob/v6.x/doc/changelogs/CHANGELOG_V5.md
[9]: Changes between v5 and v6:
https://github.com/nodejs/node/blob/v6.x/doc/changelogs/CHANGELOG_V6.md

Sincerely,
Amazon Web Services
{quote}

We should check our lambda implementation (both fugue-core-aws and the planner) to verify that we don't treat nodejs 0.10 as valid (it would also be nice to sync up the supported runtimes, since several have been added since we initially implemented Lambda support).",,chrisg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,LRT Team,,,,2|i003kq:68j,,,Sprint 29,,,,,2.0,,,Not started,,"02/May/17 4:56 PM;chrisg;The core type changes to remove the {{Nodejs}} constructor and add the {{Python3_6}}, {{Nodejs6_10}}, and {{Dotnet1_0}} runtimes: https://github.com/LuminalHQ/fugue-core-aws/pull/366
",08/May/17 1:26 PM;chrisg;PR has been merged. Just waiting on a release of fugue-core-aws before bumping fuguelib-go in the components.,"09/May/17 2:21 PM;chrisg;emitInstructions-go: https://github.com/LuminalHQ/emitInstructions-go/pull/508
fugue-id-mapper: https://github.com/LuminalHQ/fugue-id-mapper/pull/75",,,,,,,,,,,,,
finish banging out transcriber scaffolding,FUGUE-5458,33810,,Task,To Do,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,robertds,alex,alex,27/Apr/17 2:32 PM,27/Apr/17 2:33 PM,09/May/17 6:48 PM,,,,,,0,,,,,,,alex,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,FUGUE-3486,,,,,,,,,,2|i00bae:,,,Sprint 29,,,,,,,,Not started,,,,,,,,,,,,,,,,,
Use ANTLR parser to generate Ludwig type mappings for transcriber,FUGUE-5459,33811,,Task,To Do,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,robertds,alex,alex,27/Apr/17 2:32 PM,27/Apr/17 2:41 PM,09/May/17 6:48 PM,,,,,,0,,,,,,,alex,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,FUGUE-3486,,,,,,,,,,2|i00bam:,,,Sprint 29,,,,,,,,Not started,,,,,,,,,,,,,,,,,
CLI merge sprint-26.2 hotfixes (update cfn for new vars template) upstream,FUGUE-5467,34089,,Task,Done,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,eric,wayne,wayne,01/May/17 11:56 AM,01/May/17 4:43 PM,09/May/17 6:48 PM,,,,CLI,,0,,,,,,,eric,wayne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,3.0,,,CloudOS Team,,,,2|hzytuu:000000r000000000000i,,,Sprint 29,,,,,3.0,,,Not started,2017-05-01 13:42:29.582,01/May/17 1:42 PM;eric;PR: https://github.com/LuminalHQ/fugue-cli-python/pull/422,01/May/17 2:20 PM;eric;will be released in cli 1.7.0,"01/May/17 3:08 PM;eric;now in release 1.7.0
fugue-client: v0.33.0","01/May/17 3:09 PM;eric;I had a little trouble while smoke testing, but it was due to me messing up my own environment. Once I made the necessary corrections everything worked as expected.",,,,,,,,,,,,
Conductor-image merge sprint-26.2 hotfixes (update cfn for new vars template) upstream,FUGUE-5468,34090,,Task,Done,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,eric,wayne,wayne,01/May/17 11:57 AM,01/May/17 4:42 PM,09/May/17 6:48 PM,,,,,,0,,,,,,,eric,wayne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,2.0,,,CloudOS Team,,,,2|hzytuu:000000r000000000000r,,,Sprint 29,,,,,3.0,,,Not started,2017-05-01 13:42:42.12,01/May/17 1:42 PM;eric;PR: https://github.com/LuminalHQ/conductor-image/pull/328/files,,,,,,,,,,,,,,,
LWC throws internal error for patched Vpc.new,FUGUE-5474,34130,,Task,PR Merged,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,jasper,jasper,jasper,02/May/17 8:49 AM,08/May/17 5:22 AM,09/May/17 6:48 PM,,,,Ludwig Language & Compiler,,0,,,,,"If we patch the {{Vpc.new}} constructor in the AWS libraries to:

{noformat}
fun new {
      cidrBlock: String,
      dhcpOptions: Optional<EC2.DhcpOptions>,
      tags: Optional<List<AWS.Tag>>,
      instanceTenancy: Optional<EC2.Tenancy>,
      region: AWS.Region,
      enableDnsSupport: Optional<Bool>,
      enableDnsHostnames: Optional<Bool>
    } -> EC2.Vpc:
  V.runValidations {
    prefix: ""Invalid Vpc: "",
    validations: [
      validateCidr(cidrBlock),
      validateDnsAttributes(enableDnsSupport, enableDnsHostnames),
    ]
  };
  let defaultDhcpOptions: DhcpOptions.defaultForRegion(region) with { tags: tags }
  EC2.Vpc {
    cidrBlock: cidrBlock,
    tags: tags,
    instanceTenancy: instanceTenancy,
    region: region,
    enableDnsSupport: Optional.unpack(True, enableDnsSupport),
    enableDnsHostnames: Optional.unpack(False, enableDnsHostnames),
    dhcpOptions: case dhcpOptions of
                   | None -> Optional(defaultDhcpOptions)
                   | _    -> dhcpOptions
  }
{noformat}

{{lwc}} crashes with an internal error:

{noformat}
$ lwc Fugue/AWS/EC2/Vpc.lw
lwc: Internal error: non-kind-preserving substitution: ai mapped to DhcpOptions
CallStack (from HasCallStack):
  error, called at lib/Ludwig/TypeCheck/Subst.hs:149:40 in ludwig-0.24.2-65631vG7qwBRyOBFkWUq4:Ludwig.TypeCheck.Subst
{noformat}",,jasper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,Toolchain Team,,,,2|i00c9y:,,,Sprint 29,,,,,3.0,,,Not started,,,,,,,,,,,,,,,,,
Have demarc log the principal that sent a request,FUGUE-5476,34132,,Task,PR Merged,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,matt,alex,alex,02/May/17 9:46 AM,05/May/17 9:48 AM,09/May/17 6:48 PM,,,,,,0,,,,,"When demarc gets a request from a client, it logs like this:

{noformat}
{ ""timestamp"": ""2017-05-02T13:40:41.234"", ""component"": ""demarc.worker"", ""log_level"": ""INFO"", ""message"": ""received Run{'alias': 'lab_asg', 'account': 'lab', 'correlation_id': '9ed4f4034e074678a0f814e282b3bd1d', 'timestamp': 1493732441, 'hash': None, 'dry_run': False, 'key': 'compositions/4c751cb3de64d9524328b66749d5c5a286374529', 'force': False, 'bucket': 'fugue-113450714763-us-east-1'}"" }
{noformat}

We should also be logging the principal that made the request. ",,alex,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,CloudOS Team,,,,2|hzytuu:000000r0000000000009,,,Sprint 29,,,,,2.0,,,Not started,,,,,,,,,,,,,,,,,
Update the migration script for large values,FUGUE-5479,34145,33310,Sub-task,To Do,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,,achintya,achintya,03/May/17 1:04 AM,03/May/17 1:04 AM,09/May/17 6:48 PM,,,,,,0,,,,,"As noted by [~wayne] there's already a PR out for migrating old large value scheme to the new one:
https://github.com/LuminalHQ/fugue-vars-cleaner/pull/5
We need to update this to migrate to the new large value scheme of splitting the S3 blob into chunks and moving it to the new large value bucket.",,achintya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,,,,,,,,2|i00ccu:,,,Sprint 28,Sprint 29,,,,,,,Not started,,,,,,,,,,,,,,,,,
Index out-of-range error from fugue init,FUGUE-5483,34160,,Bug,PR Merged,FUGUE,Fugue Product,software,nate,Fugue Product Project,,High,,wayne,scott,scott,03/May/17 5:23 PM,05/May/17 4:53 PM,04/May/17 11:06 AM,,,,CLI,,0,,,,,"[~carrie] reported this and I can reproduce it with the current (?) RC:

{noformat}
Fugue CLI Version: 1.7.0-2521-d32beac4c028281ae83c465f0e40dd048083652d
VARs Version: 2.0.5-2425-0115a8ea3db23c4ae326ecaef73ceff79b0c1def
LWDoc Version: 0.24.2
LWC Version: 0.24.2
{noformat}

When using an AMI from the wrong region,

{{fugue init --ami ami-69aa2e08 us-east-1}}

this error appears in fuguecli.log:

{noformat}
Traceback (most recent call last):
  File ""site-packages/fugue_cli/cli.py"", line 158, in invoke
  File ""site-packages/click/core.py"", line 1060, in invoke
  File ""site-packages/click/core.py"", line 889, in invoke
  File ""site-packages/click/core.py"", line 534, in invoke
  File ""site-packages/click/decorators.py"", line 27, in new_func
  File ""site-packages/fugue_cli/commands/init.py"", line 96, in init
  File ""site-packages/fugue_cli/amis.py"", line 100, in validate_ami
  File ""site-packages/fugue_cli/amis.py"", line 38, in __init__
IndexError: tuple index out of range
{noformat}",,scott,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,{},,,,,1.0,,,CloudOS Team,,,,2|i00an1:i,,,Sprint 29,,,,,,,,Not started,,,,,,,,,,,,,,,,,
Error with EC2 composition,FUGUE-5484,34166,,Bug,Cannot Reproduce,FUGUE,Fugue Product,software,nate,Fugue Product Project,,Critical,,tyler,ben,ben,04/May/17 10:52 AM,04/May/17 12:52 PM,04/May/17 11:08 AM,,r26,,,,0,NEEDS_INFO,,,,"AMI: ami-8bc7a09d
CLI: 0.30.9
FID: 1728635c-871e-4b4f-8668-48484d7197cc

I ran the EC2 composition provided below, which succeeded.  However when I went to delete I got an error, because there was still an EC2 instance in the subnet in eu-central-1.  That's because a second EC2 instance was created in the subnet, but was not labeled, even though only one should have been.  It was clearly created by Fugue.
The instance_id of the instance that was created correctly was i-07c91e60a315a5935
The second instance_id was i-023d81f97bff12409


{noformat}
Jobs History Report for e2e_test/841117627206 - Thu May 4 2017 10:36am
ec2 (1728635c-871e-4b4f-8668-48484d7197cc)

Row Id      Started    Last Updated    Job Type    Jobs Ran    Job Status    Description
----------  ---------  --------------  ----------  ----------  ------------  -------------------
1493908519  10:35am                    KILL        1
1493908009  10:26am    10:35am         KILL        1           FAILED        DependencyViolation
1493907438  10:17am    10:26am         KILL        1           FAILED        DependencyViolation
1493907407  10:03am    10:16am         SYSTEM      26          SUCCEEDED     JobSucceeded
1493906386  9:59am     10:03am         RUN         1           SUCCEEDED     JobSucceeded

(fdiag) Fugue-107:luminal-testing-1 ben$ fugue status --json
[
    {
        ""account_id"": ""fugue-1493905979482"",
        ""alias"": ""ec2"",
        ""created"": 1493906364,
        ""current_job_status"": {
            ""current_time"": 1493908569,
            ""dry_run"": false,
            ""job_id"": 1493908519,
            ""start_time"": 1493908519
        },
        ""fid"": ""1728635c-871e-4b4f-8668-48484d7197cc"",
        ""last_job_id"": 1493908009,
        ""last_job_status"": {
            ""description"": ""DependencyViolation"",
            ""dry_run"": false,
            ""finish_time"": 1493908515,
            ""job_id"": 1493908009,
            ""job_status"": ""FAILED"",
            ""message"": ""Retries Exhausted issuing instruction [aws.ec2.delete_subnet] with [{'SubnetId': 'subnet-a45804cc'}]: [An error occurred (DependencyViolation) when calling the DeleteSubnet operation: The subnet 'subnet-a45804cc' has dependencies and cannot be deleted.]"",
            ""start_time"": 1493908009,
            ""status_code"": 400
        },
        ""last_message"": ""Retries Exhausted issuing instruction [aws.ec2.delete_subnet] with [{'SubnetId': 'subnet-a45804cc'}]: [An error occurred (DependencyViolation) when calling the DeleteSubnet operation: The subnet 'subnet-a45804cc' has dependencies and cannot be deleted.]"",
        ""resources"": null,
        ""state"": ""KILL"",
        ""updated"": 1493907416
    }
]

{noformat}",,ben,rob,tyler,,,,,,,,,FUGUE-4700,,,04/May/17 10:52 AM;ben;fugue_report-2017-05-04-14-47-50.zip;https://luminal.atlassian.net/secure/attachment/28842/fugue_report-2017-05-04-14-47-50.zip,04/May/17 10:48 AM;ben;simple_ec2.lw;https://luminal.atlassian.net/secure/attachment/28843/simple_ec2.lw,,,,,,,,,,,,,,,{},,,,,1.0,,,LRT Team,,,,2|hzytuu:000000r0000008zzzzzzzzzjv,,,Sprint 29,,,,,,,,Not started,2017-05-04 11:07:41.052,"04/May/17 11:07 AM;rob;Probably linked to 4700. Will confirm. If so, it's fixed in 27, and back porting it would be onerous.  Need a discussion of whether this demands work. ",04/May/17 11:48 AM;tyler;i-07c91e60a315a5935 was found in the first job.,04/May/17 11:49 AM;tyler;i-023d81f97bff12409 was not found in the broker logs :/,04/May/17 11:51 AM;tyler;Last log was from 2017-05-04T14:48:15,"04/May/17 11:55 AM;tyler;... which should be late enough... and the last failed job is in the broker logs. The ""good"" news is this isn't FUGUE-4700.

The reason this is ""good"" news is that FUGUE-4700 is a performance issue that we thought only occurred above a certain load. If we started seeing this at lower loads, we would need to rethink the assumptions and impact of the bug.

As a reminder FUGUE-4700 is an eventual consistency bug cause by storing the last time a resource was seen and the last time the reflector was run in separate Vars keys. This was fixed in the reflector refactor from Sprint 27.","04/May/17 11:57 AM;tyler; i-023d81f97bff12409 does not appear in the reflector descriptors, the mystery thickens (logs are over an appropriate timespan).","04/May/17 12:13 PM;tyler;RCA is weak sauce on this one:
Working theory:
* This bug is **not** related to FUGUE-4700
* AWS accidentally created another instance under our reservation
* Our logic around instances work at the instance level and not reservation level so we are not equipped for this case (think the the instance type that only allows for a single instance vs. {{run_instances}} that requires {{min}} and {{max}} parameters.

References:
* The stray instance appears in the vars dump
* The stray instance does not appear in the broker (it was not created by Fugue)

Missing data:
* Reflector was not logging the values it was picking up so we don't know when the instance appeared or if it appeared under the same reservation number as our original instance or a new one

Actions:
* Improve logging (already done in 1.1.27+ and would be a serious backport to 1.0.26)
* Design a solution to handle multiple instances spawned under a single reservation number. However, we will want to reproduce this with sufficient logs to know what to design for and confirm the fix.

TL;DR
* Likely an AWS bug
* We have improvements in 1.1.27+ that will help us better understand the bug (that are difficult to backport)
* No further realistic work can be done until we have higher fidelity logs around the bug.",,,,,,,,,
